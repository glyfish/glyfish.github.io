---
title: Metropolis Hastings Sampling
key: 20180721
image: /assets/posts/metropolis_hastings_sampling/normal_proposal_acceptance_fit.png
author: Troy Stribling
permlink: /metropolis_hastings_sampling.html
comments: false
---

[Metropolis Hastings Sampling](https://en.wikipedia.org/wiki/Metropolisâ€“Hastings_algorithm) is a method for obtaining samples for known target
probability distribution using samples from some other proposal distribution. It is similar to
[Rejection Sampling]({{ site.baseurl }}{% link _posts/2018-07-29-rejection_sampling.md %}) in providing a
criteria for acceptance of a proposal sample as a target sample but instead of discarding the
samples that do not meet the acceptance criteria the sample
from the previous time step is replicated. Another difference is that Metropolis Hastings samples are modeled as a [Markov Chain](https://en.wikipedia.org/wiki/Markov_chain) where the target distribution
is the [Markov Chain Equilibrium Distribution]({{ site.baseurl }}{% link _posts/2018-08-16-continuous_state_markov_chain_equilibrium.md %}). As a consequence
the previous sample is used as part of the acceptance criteria when generating the next sample.
It will be seen this has the advantage of permitting dynamical adjustment of some proposal distribution
parameters as each sample is generated, which in effect eliminates parameters as model inputs.  
This is is an improvement over Rejection Sampling, where it was
[previously shown]({{ site.baseurl }}{% link _posts/2018-07-29-rejection_sampling.md %}) that
slight variations in proposal distribution parameters can significantly impact performance.
A downside of the Markov Chain representation is that
[autocorrelation]({{ site.baseurl }}{% link _posts/2018-08-25-discrete_cross_correlation_theorem.md %}) can develop in the samples
which is not the case in Rejection Sampling.

<!--more-->

## Algorithm

The [Metropolis Sampling Algorithm](https://www.google.com/search?hl=en&source=hp&ei=O-CGW-qAIMWosgXf9ofQDA&q=Equation+of+state+calculations+by+fast+computing+machines&oq=Equation+of+state+calculations+by+fast+computing+machines&gs_l=psy-ab.3..35i39k1j0i22i30k1l2.1983.1983.0.2488.3.2.0.0.0.0.91.91.1.2.0....0...1.2.64.psy-ab..1.2.180.6...89.Y99UmLWMZD0) was
originally developed by physicist studying simulations of equation of state at the dawn of the computer age.
It is the first a a class of sampling algorithms referred to a [Monte Carlo Markov Chain](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo).
A couple of decades later the algorithm was generalized and given a firmer theoretical foundation becoming the [Metropolis Hastings Sampling Algorithm](https://www.google.com/search?client=safari&rls=en&ei=Xt-GW5GyLOWwtgX38az4Dg&q=Monte+Carlo+Sampling+Methods+Using+Markov+Chains&oq=Monte+Carlo+Sampling+Methods+Using+Markov+Chains&gs_l=psy-ab.3..0j0i22i30k1l2.79402.79402.0.79877.1.1.0.0.0.0.95.95.1.1.0....0...1.2.64.psy-ab..0.1.95....0.99vsUPrnsUQ).
After a few more decades this work became widely used in simulating [Bayesian Posterior Distributions](https://en.wikipedia.org/wiki/Posterior_probability)
in Probabilistic Programming DSLs such as [`pymc`](https://en.wikipedia.org/wiki/PyMC3). This section provides an overview of the algorithm
to motivate details of the theory and examples discussed in following sections.

Let {% katex %}f(y){% endkatex %} denote the target distribution. The algorithm constructs a Markov Chain with
[equilibrium distribution]({{ site.baseurl }}{% link _posts/2018-08-16-continuous_state_markov_chain_equilibrium.md %}),
{% katex %}\pi_{E}{% endkatex %}, such that,

{% katex display %}
f(y) = \pi_{E}(y).
{% endkatex %}

The equilibrium [stochastic kernel]({{ site.baseurl }}{% link _posts/2018-08-16-continuous_state_markov_chain_equilibrium.md %}),
{% katex %}p(x,y){% endkatex %} for the Markov Chain must satisfy,

{% katex display %}
\pi_{E}(y) = \int_{-\infty}^{\infty} p(x, y) \pi_{E}(x) dx\ \ \ \ \ (1).
{% endkatex %}

To generate samples for {% katex %}p(x, y){% endkatex %} samples from a proposal Markov Chain with stochastic kernel {% katex %}q(x, y){% endkatex %}
are produced. Let {% katex %}x{% endkatex %} denote the state of the {% katex %}p(x, y){% endkatex %} Markov Chain at the previous
time step, {% katex %}t-1{% endkatex %} and {% katex %}y{% endkatex %} represent the yet to be determined state at time
{% katex %}t{% endkatex %}. Consider a proposal sample, {% katex %}y^{\ast}{% endkatex %}, generated by {% katex %}q(x, y){% endkatex %}.
Define an acceptance function,

{% katex display %}
0\ \leq\ \alpha(x, y^{\ast})\ \leq 1,
{% endkatex %}

and an *acceptance* random variable, {% katex %}U{% endkatex %}, with distribution {% katex %}\textbf{Uniform}(0,\ 1){% endkatex %}
independent of {% katex %}y^{\ast}{% endkatex %}. Now, if the following inequality is satisfied,

{% katex display %}
U\ \leq\ \alpha(x, y^{\ast}),
{% endkatex %}

*accept* the proposed sample, {% katex %}y=y^{\ast}{% endkatex %}, as a sample of {% katex %}p(x, y){% endkatex %}. If the inequality
is not satisfied then *reject* the sample by replicating the state from the previous time step, {% katex %}y=x{% endkatex %}.

It will be shown that,

{% katex display %}
\alpha(x, y) = min\left\{\frac{f(y)q(y,x)}{f(x)q(x,y)}, 1\right\}\ \ \ \ \ (2).
{% endkatex %}

The algorithm can be summarized by the following steps that are repeated for each sample.

1. Generate samples of {% katex %}y^{\ast}{% endkatex %} conditioned on {% katex %}x{% endkatex %}
and independently generate samples of {% katex %}U{% endkatex %}.
2. If {% katex %}U\ \leq\ \alpha(x, y^{\ast}){% endkatex %} then {% katex %}y = y^{\ast}{% endkatex %}.
3. If {% katex %}U\ >\ \alpha(x, y^{\ast}){% endkatex %} then {% katex %}y = x{% endkatex %}.

This is similar to [Rejection Sampling]({{ site.baseurl }}{% link _posts/2018-07-29-rejection_sampling.md %}) with a more complex acceptance function and different rejection procedure but remains simple.

## Theory

Metropolis Hastings Sampling has a simple implementation but the theory that provides
validation of the algorithm is more complicated than required for both
[Rejection Sampling]({{ site.baseurl }}{% link _posts/2018-07-29-rejection_sampling.md %}) and
[Inverse CDF Sampling]({{ site.baseurl }}{% link _posts/2018-07-21-inverse_cdf_sampling.md %}).
Here the proof that the algorithm produces the desired result is accomplished in four steps.
The first will show that distributions and stochastic kernels that satisfy Time Reversal Symmetry,
also referred to as Detailed Balance, are equilibrium solutions that satisfy equation
{% katex %}(1){% endkatex %}. Next, the stochastic kernel resulting from the algorithm is derived
and followed by a proof that a distribution that satisfies Time Reversal Symmetry with the resulting
stochastic kernel is a solution to equation {% katex %}(1){% endkatex %}. Finally, the
expression for {% katex %}\alpha(x,y){% endkatex %} from equation {% katex %}(2){% endkatex %} will
be derived.

### Time Reversal Symmetry

Consider a stochastic kernel {% katex %}p(x,y){% endkatex %} and distribution {% katex %}\pi_(y){% endkatex %}
where {% katex %}x{% endkatex %} is the state of the Markov Chain generated by the kernel at
{% katex %}t-1{% endkatex %} and {% katex %}y{% endkatex %} the
state at time {% katex %}t{% endkatex %}. Consider a chain of {% katex %}N{% endkatex %} steps.
The expected number of transitions from {% katex %}x{% endkatex %} to {% katex %}y{% endkatex %} will satisfy,

{% katex display %}
N_{y}=N \pi_(y) p(x, y).
{% endkatex %}

It follows that the rate of transition from {% katex %}x{% endkatex %} to {% katex %}y{% endkatex %} is
given by {% katex %}\pi(y) p(x,y){% endkatex %}. The transition rate for the Markov Chain obtained by
reversing time, that is the chain transitions from {% katex %}y{% endkatex %} to {% katex %}x{% endkatex %}
will be {% katex %}\pi(x) p(y, x){% endkatex %}.

Time Reversal Symmetry implies that the transition rate for the time reversed Markov Chain is the same as the
forward time chain, namely,

{% katex display %}
\pi(y) p(x, y) = \pi_(x) p(y,x)\ \ \ \ \ (3).
{% endkatex %}

If Time Reversal Symmetry is assumed it follows that,

{% katex display %}
\begin{aligned}
\int_{-\infty}^{\infty} p(x, y) \pi(x) dx &= \int_{-\infty}^{\infty} p(y, x) \pi(y) dx \\
&= \pi(y) \int_{-\infty}^{\infty} p(y, x) dx \\
&= \pi (y),
\end{aligned}
{% endkatex %}

where the last step is a result of the definition of a stochastic kernel,
{% katex %}\int_{-\infty}^{\infty} p(y, x) dx = 1{% endkatex %}. This any distribution and kernel satisfying
equation {% katex %}(3){% endkatex %} is an equilibrium solution since it will also satisfy
equation {% katex %}(1){% endkatex %}

### Stochastic Kernel

In a [previous post]({{ site.baseurl }}{% link _posts/2018-07-29-rejection_sampling.md %}) it was shown
that a stochastic kernel can be interpreted as probability density conditioned of the state
at the previous time step,

{% katex display %}
p(x,y) = f(y\mid x).
{% endkatex %}

It follows that the [Cumulative Probability Distribution (CDF)]({{ site.baseurl }}{% link _posts/2018-07-21-inverse_cdf_sampling.md %}) of obtaining a sample {% katex %}y{% endkatex %}
for a given {% katex %}x{% endkatex %} is given by,

{% katex display %}
\begin{aligned}
P\left[ Y\ \leq\ y\ \mid\ X=x \right] &= \int_{-\infty}^{y} f(w\mid x) dw \\
&= \int_{-\infty}^{y} p(x,w) dw.
\end{aligned}
{% endkatex %}

Let {% katex %}U{% endkatex %} represent the {% katex %}\textbf{Uniform}(0, 1){% endkatex %} acceptance
random variable. A proposal sample, {% katex %}y^{\ast}{% endkatex %} conditioned
on {% katex %}x{% endkatex %}, is generated by {% katex %}q(x,y){% endkatex %} independent of
{% katex %}U{% endkatex %} and accepted if,

{% katex display %}
U\ \leq\ \alpha(x, y^{\ast}),
{% endkatex %}

then {% katex %}y=y^{\ast}{% endkatex %}. The proposal is rejected if,

{% katex display %}
U\ >\ \alpha(x, y^{\ast}),
{% endkatex %}

then {% katex %}y=x{% endkatex %}. Now, using the
[Law of Total Probability](https://en.wikipedia.org/wiki/Law_of_total_probability) to condition on
{% katex %}U{% endkatex %} the stochastic kernel CDF becomes,

{% katex display %}
\begin{aligned}
P\left[ Y\ \leq\ y\ \mid\ X=x \right]\ &=\ P\left[Y\ \leq\ y\ \mid\ U\ \leq\ \alpha(x, y^{\ast}),\ X=x \right]\ P\left[U\ \leq\ \alpha(x, y^{\ast})\ \mid\ X=x \right] \\
&\ + P\left[Y\ \leq\ y\ \mid\ U\ >\ \alpha(x, y^{\ast}),\ X=x \right]\ P\left[U\ >\ \alpha(x, y^{\ast})\ \mid\ X=x \right].
\end{aligned}\ \ \ \ \ (4)
{% endkatex %}

The first term is the probability that the proposed sample is accepted and the second term the
probability that it is rejected. Consider the first term where
{% katex %}U\ \leq\ \alpha(x, y^{\ast}){% endkatex %} and {% katex %}y=y^{\ast}{% endkatex %},

{% katex display %}
P\left[Y\ \leq\ y,\ U\ \leq\ \alpha(x, y^{\ast})\ \mid\ X=x \right]\ =\ P\left[Y\ \leq\ y\ \mid\ U\ \leq\ \alpha(x, y^{\ast}),\ X=x \right]\ P\left[U\ \leq\ \alpha(x, y^{\ast})\ \mid\ X=x \right]\ \ \ \ \ (5),
{% endkatex %}

which follows from the definition of
[Conditional Probability](https://en.wikipedia.org/wiki/Conditional_probability).

The algorithm assumes that {% katex %}U{% endkatex %} is independent of {% katex %}y^{\ast}{% endkatex %} conditioned on {% katex %}x{% endkatex %}. It follows that the joint density is
given by,

{% katex display %}
\begin{aligned}
h(y^{\ast}, u\mid x) &= g(u)f(y^{\ast}\mid x) \\
&= q(x, y^{\ast}),
\end{aligned}
{% endkatex %}

where the last step follows by noting that the density of {% katex %}U{% endkatex %} is given by
{% katex %}g(u)=1{% endkatex %}, since {% katex %}\textbf{Uniform}(0,1){% endkatex %}, and the density
of {% katex %}y^{\ast}{% endkatex %} conditioned on {% katex %}x{% endkatex %} is given by the
proposal stochastic kernel {% katex %}f(y^{\ast}\mid x)=q(x, y^{\ast}){% endkatex %}. Using this
result to continue with the result from equation {% katex %}(5){% endkatex %} gives the acceptance
probability term for the stochastic kernel CDF from equation {% katex %}(4){% endkatex %},

{% katex display %}
\begin{aligned}
P\left[Y\ \leq\ y,\ U\ \leq\ \alpha(x, y^{\ast})\ \mid\ X=x \right] &= \int_{-\infty}^{y}\int_{0}^{\alpha(x, y^{\ast})} h(y^{\ast}, u\mid x) du dy^{\ast} \\
&= \int_{-\infty}^{y}\int_{0}^{\alpha(x, y^{\ast})} q(x, y^{\ast}) du dy^{\ast} \\
& =  \int_{-\infty}^{y} \alpha(x, y^{\ast}) q(x, y^{\ast}) dy^{\ast}
\end{aligned}\ \ \ \ \ (6).
{% endkatex %}

Next, consider the second term in equation {% katex %}(4){% endkatex %}, which represents the case where
{% katex %}y^{\ast}{% endkatex %} is rejected as a sample of {% katex %}y{% endkatex %}
since {% katex %}y^{\ast}{% endkatex %} is rejected {% katex %}y=x{% endkatex %}.
It follows that any value of {% katex %}y^{\ast}{% endkatex %} is possible so
the integral over {% katex %}y^{\ast}{% endkatex %} needs to be over its entire range. Using the previous
result for the joint density {% katex %}h(y^{\ast}, u\mid x){% endkatex %}. gives

{% katex display %}
\begin{aligned}
P\left[Y\ = x,\ U\ >\ \alpha(x, y^{\ast})\ \mid\ X=x \right] &= \int_{-\infty}^{\infty}\int_{\alpha(x, y^{\ast})}^{1} h(y^{\ast}, u\mid x) du dy^{\ast} \\
&= \int_{-\infty}^{\infty}\int_{\alpha(x, y^{\ast})}^{1} q(x, y^{\ast}) du dy^{\ast} \\
&= \int_{-\infty}^{\infty}\left[ 1-\alpha(x, y^{\ast})\right] q(x, y^{\ast}) dy^{\ast} \\
&= \int_{-\infty}^{\infty}\ q(x, y^{\ast}) dy^{\ast} - \int_{-\infty}^{\infty}\alpha(x, y^{\ast}) q(x, y^{\ast}) dy^{\ast} \\
&= 1 - \int_{-\infty}^{\infty}\alpha(x, y^{\ast}) q(x, y^{\ast}) dy^{\ast}
\end{aligned}\ \ \ \ \ (7)
{% endkatex %}

since {% katex %}\int_{-\infty}^{\infty}\ q(x, y^{\ast}) dy^{\ast}=1{% endkatex %}. Simplify
things by defining the rejection probability density by,

{% katex display %}
f_{R}(x) = 1 - \int_{-\infty}^{\infty}\alpha(x, w) q(x, w) dw\ \ \ \ \ (8).
{% endkatex %}

Equation {% katex %}(7){% endkatex %} needs to be written as an integral over
{% katex %}y{% endkatex %} to continue with the evaluation of equation {% katex %}(4){% endkatex %}. This
is accomplished by use of the [Dirac Delta Function](https://en.wikipedia.org/wiki/Dirac_delta_function)
which is defined by,

{% katex display %}
\int_{-\infty}^{\infty} f(y^{\ast})\delta(y^{\ast}-x) dy^{\ast} = f(x).
{% endkatex %}

If use is made of equation {% katex %}(8){% endkatex %} and the Dirac Delta Function it follows that
equation {% katex %}(7){% endkatex %} becomes,

{% katex display %}
P\left[Y\ \leq\ y,\ U\ >\ \alpha(x, y^{\ast})\ \mid\ X=x \right] = \int_{-\infty}^{y} f_{R}(y^{\ast})\delta(y^{\ast}-x) dy^{\ast}\ \ \ \ \ (9).
{% endkatex %}

The upper range of the limit was changed to conform to the limits of the CDF. This is acceptable
since if {% katex %}y\ <\ x{% endkatex %} the integral will evaluate to zero as it should. The Metropolis Hastings stochastic kernel can now by constructed by assembling the results obtained in
equations {% katex %}(6){% endkatex %} and {% katex %}(9){% endkatex %} and revisiting equation
{% katex %}(4){% endkatex %},

{% katex display %}
\begin{aligned}
\int_{-\infty}^{y} p(x,w) dw & = P\left[ Y\ \leq\ y\ \mid\ X=x \right] \\
&= P\left[Y\ \leq\ y,\ U\ \leq\ \alpha(x, y^{\ast})\ \mid\ X=x \right] + P\left[Y\ \leq\ y,\ U\ >\ \alpha(x, y^{\ast})\ \mid\ X=x \right] \\
&= \int_{-\infty}^{y} \alpha(x, y^{\ast}) q(x, y^{\ast}) dy^{\ast} + \int_{-\infty}^{y} f_{R}(y^{\ast})\delta(y^{\ast}-x) dy^{\ast} \\
&= \int_{-\infty}^{y} \left[ \alpha(x, y^{\ast}) q(x, y^{\ast}) + f_{R}(y^{\ast})\delta(y^{\ast}-x)\right] dy^{\ast}
\end{aligned}
{% endkatex %}

Finally, the desired result is obtained by collecting the terms under a single integration variable,

{% katex display %}
\begin{gathered}
p(x,y) = \alpha(x, y) q(x, y) + f_{R}(y)\delta(y-x) \\
f_{R}(x) = 1 - \int_{-\infty}^{\infty}\alpha(x, y) q(x, y) dy
\end{gathered}\ \ \ \ \ (10)
{% endkatex %}

It is also interesting to verify that the kernel satisfies the stochastic property, namely,

{% katex display %}
\begin{aligned}
\int_{-\infty}^{\infty} p(x,y) dy &= \int_{-\infty}^{\infty}\left[ \alpha(x, y) q(x, y) + f_{R}(y)\delta(y-x)\right] dy \\
&= \int_{-\infty}^{\infty}\alpha(x, y) q(x, y) dy - f_{R}(x) \\
&= \int_{-\infty}^{\infty}\alpha(x, y) q(x, y) dy + 1 - \int_{-\infty}^{\infty}\alpha(x, y) q(x, y) dy \\
&= 1
\end{aligned}
{% endkatex %}

which is the desired result.

### Equilibrium

It can now be shown that the Metropolis Hastings stochastic kernel from equation {% katex %}(10){% endkatex %}
is an equilibrium solution by evaluating the integral from equation {% katex %}(1){% endkatex %} which
defines equilibrium. Additionally, time Reversal Symmetry must also be assumed,

{% katex display %}
\pi(x)\alpha(x,y)q(x,y) = \pi(y)\alpha(y,x)q(y,x)\ \ \ \ \ (11),
{% endkatex %}

where {% katex %}\alpha(x,y){% endkatex %} is the acceptance function from equation
{% katex %}(1){% endkatex %}, {% katex %}q(x,y){% endkatex %} is the stochastic kernel for the
proposal Markov Chain, {% katex %}\pi(y){% endkatex %} is the distribution, {% katex %}y{% endkatex %} is
the state of the target Markov Chain at time step {% katex %}t{% endkatex %} and {% katex %}x{% endkatex %}
the state at time {% katex %}t-1{% endkatex %}.

Now, starting with equation {% katex %}(1){% endkatex %},

{% katex display %}
\begin{aligned}
\int_{-\infty}^{\infty} p(x, y) \pi(x) dx &= \int_{-\infty}^{\infty}\left[ \alpha(x, y) q(x, y) + f_{R}(y)\delta(y-x)\right] \pi(x) dx \\
&= \int_{-\infty}^{\infty}\alpha(x, y) q(x, y) \pi(x) dx + f_{R}(y)\pi(y) \\
&= \int_{-\infty}^{\infty}\alpha(x, y) q(x, y) \pi(x) dx + \left[1 - \int_{-\infty}^{\infty}\alpha(y, w) q(y, w) dw \right]\pi(y) \\
&= \pi(y)+\int_{-\infty}^{\infty}\alpha(x, y) q(x, y) \pi(x) dx - \int_{-\infty}^{\infty}\pi(y)\alpha(y, w) q(y, w) dw \\
&= \pi(y)+\int_{-\infty}^{\infty}\alpha(x, y) q(x, y) \pi(x) dx - \int_{-\infty}^{\infty}\pi(w)\alpha(w, y) q(w, y) dw \\
&= \pi(y),
\end{aligned}
{% endkatex %}

where the last steps follow from substituting equation {% katex %}(11){% endkatex %} into the last term
leading to the desired result.

### Derivation of {% katex %}\alpha(x,y){% endkatex %}



## Example

### Parameterization

### Implementation

```python
def metropolis_hastings(f, q, qsample, stepsize, nsample, x0):
    x = x0
    samples = numpy.zeros(nsample)
    for i in range(nsample):
        accept = numpy.random.rand()
        y_star = qsample(x, stepsize)
        fy_star = f(y_star)
        fx = p(x)
        Î± = (fy_star*q(y_star, x, stepsize)) / (fx*q(x, y_star, stepsize))
        if accept < Î±:
            x = y_star
        samples[i] = x
    return samples
```

### Convergence To Equilibrium

### Burn In

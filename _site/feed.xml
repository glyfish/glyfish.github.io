<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-08-15T08:43:15-07:00</updated><id>http://localhost:4000/</id><title type="html">gly.fish</title><subtitle></subtitle><entry><title type="html">Discrete State Markov Chain Equilibrium</title><link href="http://localhost:4000/2018/08/08/discrete_state_markov_chain_equilibrium.html" rel="alternate" type="text/html" title="Discrete State Markov Chain Equilibrium" /><published>2018-08-08T00:00:00-07:00</published><updated>2018-08-08T00:00:00-07:00</updated><id>http://localhost:4000/2018/08/08/discrete_state_markov_chain_equilibrium</id><content type="html" xml:base="http://localhost:4000/2018/08/08/discrete_state_markov_chain_equilibrium.html">A [Markov Chain](https://en.wikipedia.org/wiki/Markov_chain) is a sequence of states
where transitions between states occur ordered in time with
the probability of transition depending only on the previous state. Here the
states will be assumed a discrete finite set and time a discrete unbounded set. If the
set of states is given by {% katex %}\{x_1,\ x_2,\ldots,\ x_N\}{% endkatex %} the probability
that the process will be in state {% katex %}x_i{% endkatex %} at time {% katex %}t{% endkatex %}
is denoted by {% katex %}P(X_t=x_i){% endkatex %}, referred to as the distribution.
Markov Chain equilibrium is defined by {% katex %}\lim_{t\to\infty}P(X_t=x_i)\ &lt;\ \infty{% endkatex %},
that is, as time advances  
{% katex %}P(X_t=x_i){% endkatex %} becomes independent of time. Here a solution
for this limit is discussed and illustrated with examples.

&lt;!--more--&gt;

## Model

The Markov Chain model is constructed from the set of states
{% katex %}\{x_1,\ x_2,\ldots,\ x_N\}{% endkatex %} ordered in time.
The process starts at time {% katex %}t=0{% endkatex %} with state {% katex %}X_0=x_i{% endkatex %}.
At the next step, {% katex %}t=1{% endkatex %}, the process will assume a state
{% katex %}X_1=x_j{% endkatex %} with probability {% katex %}P(X_1=x_j|X_0=x_i){% endkatex %} since
the it will depend on the state at {% katex %}t=0{% endkatex %} by the definition of a Markov Process.
At the next time step {% katex %}t=2{% endkatex %} the process state will be
{% katex %}X_2=x_k{% endkatex %} with probability,
{% katex display %}
P(X_2=x_k|X_0=x_i, X_1=x_j)=P(X_2=x_k|X_1=x_j),
{% endkatex %}
since by definition the probability of state transition depends only upon the state at the previous time step.
For an arbitrary time the transition from a state {% katex %}X_{t}=x_j{% endkatex %} to a state
{% katex %}X_{t+1}=x_j{% endkatex %} will occur with probability, {% katex %}P(X_{t+1}=x_j|X_t=x_i){% endkatex %}
that is independent of {% katex %}t{% endkatex %}.
Transition probabilities have the form of a matrix,

{% katex display %}
P_{ij} = P(X_{t+1}=x_j|X_t=x_i).
{% endkatex %}

{% katex %}P{% endkatex %} will be an {% katex %}N\times N{% endkatex %} matrix
where {% katex %}N{% endkatex %} is determined by the number of possible states. Each
row represents the Markov Chain transition probability from that state at
time {% katex %}t{% endkatex %} and the columns the values at {% katex %}t+1{% endkatex %}.
It follows that,
{% katex display %}
\begin{gathered}
\sum_{j=1}^{N}P_{ij} = 1\\
P_{ij}\ \geq\ 0
\end{gathered} \ \ \ \ \ (1).
{% endkatex %}

Equation {% katex %}(1){% endkatex %} is the definition of a [Stochastic Matrix](https://en.wikipedia.org/wiki/Stochastic_matrix).

The transition probability for a single step in the Markov Process is defined by {% katex %}P{% endkatex %}.
The transition probability across two time steps can be obtained with use of the
[Law of Total Probability](https://en.wikipedia.org/wiki/Law_of_total_probability),
{% katex display %}
\begin{aligned}
P(X_{t+2}=x_j|X_t=x_i) &amp;= \sum_{k=1}^{N} P(X_{t+2}=x_j | X_{t}=x_i, X_{t+1}=x_k)P(X_{t+1}=x_k | X_{t}=x_i) \\
&amp;= \sum_{k=1}^{N} P(X_{t+2}=x_j | X_{t+1}=x_k)P(X_{t+1}=x_k | X_{t}=x_i) \\
&amp;= \sum_{k=1}^{N} P_{kj}P_{ik} \\
&amp;= \sum_{k=1}^{N} P_{ik}P_{kj} \\
&amp;= {(P^2)}_{ij},
\end{aligned}
{% endkatex %}

where the last step follows from the definition of matrix multiplication. It is straight forward but
tedious to use [Mathematical Induction](https://en.wikipedia.org/wiki/Mathematical_induction) to extend the previous result to the case of an arbitrary time difference, {% katex %}\tau{% endkatex %},
{% katex display %}
P(X_{t+\tau}=x_j|X_t=x_i) = {(P^{\tau})}_{ij}\ \ \ \ \ (2).
{% endkatex %}

It should be noted that since {% katex %}{(P^{\tau})}_{ij}{% endkatex %} is a transition probability it must
satisfy,

{% katex display %}
\begin{gathered}
\sum_{j=1}^{N} {(P^{\tau})}_{ij}\ =\ 1 \\
{(P^{\tau})}_{ij}\ \geq\ 0
\end{gathered} \ \ \ \ \ (3).
{% endkatex %}

To determine the equilibrium solution of the distribution of Markov Chain states,
{% katex %}\{x_1,\ x_2,\ldots,\ x_N\}{% endkatex %}, its time variability must be determined.
Begin by considering an arbitrary distribution at {% katex %}t=0{% endkatex %}, which can be written as a
column vector,
{% katex display %}
\pi =
\begin{pmatrix}
\pi_1 \\
\pi_2 \\
\vdots \\
\pi_N
\end{pmatrix} =
\begin{pmatrix}
P(X_0=x_1) \\
P(X_0=x_2) \\
\vdots \\
P(X_0=x_N)
\end{pmatrix},
{% endkatex %}

since it is a probability distribution {% katex %}\pi_i{% endkatex %} must satisfy,
{% katex display %}
\begin{gathered}
\sum_{i=1}^{N} \pi_i\ =\ 1\\
\pi_i\ \geq \ 0
\end{gathered} \ \ \ \ \ (4).
{% endkatex %}

The distribution after the first step is given by,
{% katex display %}
\begin{aligned}
P(X_1=x_j) &amp;= \sum_{i=1}^{N} P(X_1=x_j|X_0=x_i)P(X_0=x_i) \\
&amp;= \sum_{i=1}^{N} P_{ij}\pi_{i} \\
&amp;= \sum_{i=1}^{N} \pi_{i}P_{ij} \\
&amp;= {(\pi^{T}P)}_{j},
\end{aligned}
{% endkatex %}

where {% katex %}\pi^T{% endkatex %} is the transpose of {% katex %}\pi{% endkatex %}. Similarly, the distribution after the second step is,
{% katex display %}
\begin{aligned}
P(X_2=x_j) &amp;= \sum_{i=1}^{N} P(X_2=x_j|X_1=x_i)P(X_1=x_i) \\
&amp;= \sum_{i=1}^{N} P_{ij}{(\pi^{T}P)}_{i} \\
&amp;= \sum_{i=1}^{N} P_{ij}\sum_{k=1}^{N} \pi_{k}P_{ki} \\
&amp;= \sum_{k=1}^{N} \pi_{k} \sum_{i=1}^{N} P_{ij}P_{ki} \\
&amp;= \sum_{k=1}^{N} \pi_{k} \sum_{i=1}^{N} P_{ki}P_{ij} \\
&amp;= \sum_{k=1}^{N} \pi_{k} {(P^2)}_{kj} \\
&amp;= {(\pi^{T}P^2)}_{j},
\end{aligned}
{% endkatex %}

A pattern is clearly developing. Mathematical Induction can be used to prove the distribution
at an arbitrary time {% katex %}t{% endkatex %} is given by,
{% katex display %}
P(X_t=x_j) = {(\pi^{T}P^t)}_{j}
{% endkatex %}

or as a column vector,

{% katex display %}
\pi_{t}^{T} = \pi^{T}P^t\ \ \ \ \ (5).
{% endkatex %}

Where {% katex %}\pi{% endkatex %} and {% katex %}\pi_t{% endkatex %} are the initial distribution
and the distribution after {% katex %}t{% endkatex %} steps respectively.

## Equilibrium Transition Matrix

The probability of transitioning between two states {% katex %}x_i{% endkatex %} and
{% katex %}x_j{% endkatex %} in {% katex %}t{% endkatex %} time steps was previously shown to be
stochastic matrix {% katex %}P^t{% endkatex %} constrained by equation {% katex %}(3){% endkatex %}.
The equilibrium transition matrix is defined by,
{% katex display %}
P^{E} = \lim_{t\to\infty}P^{t}.
{% endkatex %}
This limit can be determined using
[Matrix Diagonalization](https://en.wikipedia.org/wiki/Diagonalizable_matrix).
The following sections will use diagonalization to construct a form of
{% katex %}P^{t}{% endkatex %} that will easily allow evaluation equilibrium limit.

### Eigenvectors and Eigenvalues of the Transition Matrix

Matrix Diagonalization requires evaluation of eigenvalues and eigenvectors, which are defined
by the solutions to the equation,
{% katex display %}
Pv = \lambda v\ \ \ \ \ (6),
{% endkatex %}
where {% katex %}v{% endkatex %} is the eigenvector and {% katex %}\lambda{% endkatex %} eigenvalue.
From equation {% katex %}(6){% endkatex %} it follows,
{% katex display %}
\begin{aligned}
P^{t}v &amp;= P^{t-1}(Pv)\\
&amp;=P^{t-1}\lambda v\\
&amp;=P^{t-2}(Pv)\lambda\\
&amp;=P^{t-2}\lambda^{2}v \\
&amp;\vdots\\
&amp;=(Pv)\lambda^{t-1}\\
&amp;=\lambda^{t}v.
\end{aligned}
{% endkatex %}

Since {% katex %}P^t{% endkatex %} is a stochastic matrix it satisfies equation {% katex %}(3){% endkatex %}.
As a result of these constraints the limit {% katex %}t\to\infty{% endkatex %} requires,
{% katex display %}
\lim_{t\to\infty}P^{t}=\lim_{t\to\infty}\lambda^{t}v\ \leq\ \infty.
{% endkatex %}

To satisfy this constraint it must be that {% katex %}\lambda\ \leq\ 1 {% endkatex %}. Next,
it will be shown that {% katex %}\lambda_1=1{% endkatex %} and that a column vector of
{% katex %}1's{% endkatex %} with {% katex %}N{% endkatex %} rows,
{% katex display %}
V_1 =
\begin{pmatrix}
1 \\
1 \\
\vdots \\
1
\end{pmatrix}\ \ \ \ \ (7),
{% endkatex %}
are eigenvalue and eigenvector solutions of equation {% katex %}(6).{% endkatex %}
{% katex display %}
\begin{pmatrix}
P_{11} &amp; P_{12} &amp; \cdots &amp; P_{1N} \\
P_{21} &amp; P_{22} &amp; \cdots &amp; P_{2N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
P_{N1} &amp; P_{N2} &amp; \cdots &amp; P_{NN}
\end{pmatrix}
\begin{pmatrix}
1 \\
1 \\
\vdots \\
1
\end{pmatrix}
=
\begin{pmatrix}
\sum_{j=1}^{N}P_{1j} \\
\sum_{j=1}^{N}P_{2j} \\
\vdots \\
\sum_{j=1}^{N}P_{Nj}
\end{pmatrix}
=
\begin{pmatrix}
1 \\
1 \\
\vdots \\
1
\end{pmatrix}
=\lambda_1 V_1,
{% endkatex %}

where use was made of the stochastic matrix condition from equation {% katex %}(1){% endkatex %}, namely,
{% katex %}\sum_{j=1}^{N}P_{ij}=1{% endkatex %}.

To go further a result from the [Perron-Frobenius Theorem](https://en.wikipedia.org/wiki/Perronâ€“Frobenius_theorem) is needed.
This theorem states that a stochastic matrix will have a largest eigenvalue with multiplicity 1. Here all eigenvalues will satisfy
{% katex %}\lambda_1=1 \ &gt;\ \mid{\lambda_i}\mid,\ \forall\ 1\ &lt;\ i\ \leq\ N{% endkatex %}.

Denote the eigenvector {% katex %}V_j{% endkatex %} by the column vector,
{% katex display %}
V_j =
\begin{pmatrix}
v_{1j} \\
v_{2j} \\
\vdots \\
v_{Nj}
\end{pmatrix},
{% endkatex %}

and let {% katex %}V{% endkatex %} be the matrix with columns that are the eigenvectors of
{% katex %}P{% endkatex %} with {% katex %}V_1{% endkatex %} from equation
{% katex %}(7){% endkatex %} in the first column,
{% katex display %}
V=
\begin{pmatrix}
1 &amp; v_{12} &amp; \cdots &amp; v_{1N} \\
1 &amp; v_{22} &amp; \cdots &amp; v_{2N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; v_{N2} &amp; \cdots &amp; v_{NN}
\end{pmatrix}.
{% endkatex %}

Assume that {% katex %}V{% endkatex %} is invertible and denote the inverse by,

{% katex display %}
V^{-1}=
\begin{pmatrix}
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N} \\
v^{-1}_{21} &amp; v^{-1}_{22} &amp; \cdots &amp; v^{-1}_{2N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
v^{-1}_{N1} &amp; v^{-1}_{N2} &amp; \cdots &amp; v^{-1}_{NN}
\end{pmatrix}\ \ \ \ \ (8).
{% endkatex %}

If the identity matrix is represented by {% katex %}I{% endkatex %} then {% katex %}VV^{-1} = I{% endkatex %}.
Let {% katex %}\Lambda{% endkatex %} be a diagonal matrix constructed from the eigenvalues of
{% katex %}P{% endkatex %} using {% katex %}\lambda_1=1{% endkatex %},

{% katex display %}
\Lambda =
\begin{pmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_N
\end{pmatrix}.
{% endkatex %}

Sufficient information about the eigenvalues and eigenvectors has obtained to prove some very
general results for Markov Chains.
The following section will work through the equilibrium limit using the using these results
to construct a diagonalized representation of the matrix.

### Diagonalization of Transition Matrix

Using the results obtained in the previous section {% katex %}P{% endkatex %} the diagonalized
form of the transition matrix is given by,

{% katex display %}
P = V\Lambda V^{-1},
{% endkatex %}

Using this representation of {% katex %}P{% endkatex %} an expression for {% katex %}P^t{% endkatex %}
is obtained,
{% katex display %}
\begin{aligned}
P^{t} &amp;= P^{t-1}V\Lambda V^{-1} \\
&amp;= P^{t-2}V\Lambda V^{-1}V\Lambda V^{-1} \\
&amp;= P^{t-2}V\Lambda^2 V^{-1}\\
&amp;\vdots \\
&amp;= PV\Lambda^{t-1} V^{-1} \\
&amp;= V\Lambda^{t}V^{-1}
\end{aligned}
{% endkatex %}

Evaluation of {% katex %}\Lambda^{t}{% endkatex %} is straight forward,

{% katex display %}
\Lambda^t =
\begin{pmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2^t &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_N^t
\end{pmatrix}.
{% endkatex %}

Since {% katex %}\mid{\lambda_i}\mid\ &lt;\ 1,\ \forall\ 1\ &lt;\ i\ \leq\ N{% endkatex %} in the limit
{% katex %}t\to\infty{% endkatex %} it is seen that,
{% katex display %}
\Lambda^{E} =
\lim_{t\to\infty} \Lambda^t =
\lim_{t\to\infty}
\begin{pmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2^t &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_N^t
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 0
\end{pmatrix},
{% endkatex %}

so,
{% katex display %}
P^{E} = \lim_{t\to\infty} P^{t} = \lim_{t\to\infty} V\Lambda^{t} V^{-1} = V\Lambda^{E}V^{-1}.
{% endkatex %}

Evaluation of each of the first two terms on the righthand side gives,

{% katex display %}
V\Lambda^{E} =
\begin{pmatrix}
1 &amp; v_{12} &amp; \cdots &amp; v_{1N} \\
1 &amp; v_{22} &amp; \cdots &amp; v_{2N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; v_{N2} &amp; \cdots &amp; v_{NN}
\end{pmatrix}
\begin{pmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 0
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
1 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; 0 &amp; \cdots &amp; 0
\end{pmatrix}.
{% endkatex %}

It follows that,
{% katex display %}
V\Lambda^{E} V^{-1} =
\begin{pmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
1 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; 0 &amp; \cdots &amp; 0
\end{pmatrix}
\begin{pmatrix}
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N} \\
v^{-1}_{21} &amp; v^{-1}_{22} &amp; \cdots &amp; v^{-1}_{2N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
v^{-1}_{N1} &amp; v^{-1}_{N2} &amp; \cdots &amp; v^{-1}_{NN}
\end{pmatrix}
=
\begin{pmatrix}
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N} \\
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N}
\end{pmatrix}.
{% endkatex %}

Finally, the equilibrium transition matrix is given by,
{% katex display %}
P^{E} = V\Lambda^{E} V^{-1} =
\begin{pmatrix}
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N} \\
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N}
\end{pmatrix}\ \ \ \ \ (9).
{% endkatex %}

The rows of {% katex %}P^{E}{% endkatex %} are identical and given by the
first row of the inverse of the matrix of eigenvectors, {% katex %}V^{-1}{% endkatex %},
of equation {% katex %}(8){% endkatex %}. This row is a consequence of the location of the
{% katex %}\lambda=1{% endkatex %} eigenvalue in {% katex %}\Lambda{% endkatex %}.
Since {% katex %}P^{E}{% endkatex %} is a transition
matrix it must be that,

{% katex display %}
\begin{gathered}
\sum_{j=1}^{N} v_{ij}^{-1}\ =\ 1\\
v_{ij}^{-1}\ \geq \ 0.
\end{gathered}
{% endkatex %}

## Equilibrium Distribution

The equilibrium distribution is defined by a solution to equation {% katex %}(5){% endkatex %}
that is independent of time.

{% katex display %}
\pi^{T} = \pi^{T}P^t\ \ \ \ \ (10).
{% endkatex %}

Consider a distribution {% katex %}\pi_{E}{% endkatex %} that satisfies,
{% katex display %}
\pi_{E}^{T} = \pi_{E}^{T}P\ \ \ \ \ (11).
{% endkatex %}

It is easy to show that {% katex %}\pi_{E}{% endkatex %} is the equilibrium solution
by substituting it into equation {% katex %}(10){% endkatex %}.
{% katex display %}
\begin{aligned}
\pi_{E}^{T} &amp;= \pi_{E}^{T}P^t \\
&amp;= (\pi_{E}^{T}P)P^{t-1} \\
&amp;= \pi_{E}^{T}P^{t-1} \\
&amp;= \pi_{E}^{T}P^{t-2} \\
&amp;\vdots \\
&amp;= \pi_{E}^{T}P \\
&amp;= \pi_{E}^{T}.
\end{aligned}
{% endkatex %}

### Relationship Between Equilibrium Distribution and Transition Matrix

To determine the relationship between {% katex %}\pi_E{% endkatex %} and {% katex %}P^{E}{% endkatex %},
begin by considering an arbitrary initial distribution states
with {% katex %}N{% endkatex %} elements,
{% katex display %}
\pi =
\begin{pmatrix}
\pi_{1} \\
\pi_{2} \\
\vdots \\
\pi_{N}
\end{pmatrix},
{% endkatex %}

where,

{% katex display %}
\begin{gathered}
\sum_{j=1}^{N}\pi_{j} = 1\\
\pi_{j}\ \geq\ 0
\end{gathered}.
{% endkatex %}

The distribution when the Markov Chain has had sufficient time to reach equilibrium will be given by,
{% katex display %}
\begin{aligned}
\pi^{T}P^{E} &amp;=
\begin{pmatrix}
\pi_{1} &amp; \pi_{2} &amp; \cdots &amp; \pi_{N}
\end{pmatrix}
\begin{pmatrix}
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N} \\
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N}
\end{pmatrix} \\
&amp;=
\begin{pmatrix}
v^{-1}_{11}\sum_{j=1}^{N} \pi_{j} &amp;
v^{-1}_{12}\sum_{j=1}^{N} \pi_{j} &amp;
\cdots &amp;
v^{-1}_{1N}\sum_{j=1}^{N} \pi_{j}
\end{pmatrix},
\end{aligned}
{% endkatex %}

since, {% katex %}\sum_{j=1}^{N} \pi_{j} = 1{% endkatex %}, so

{% katex display %}
\pi^{T}P^{E} =
\begin{pmatrix}
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N}
\end{pmatrix}.
{% endkatex %}

Thus any initial distribution {% katex %}\pi{% endkatex %} will after
sufficient time approach the distribution above. It follows that it will be the solution
of equation {% katex %}(11){% endkatex %} which defines the equilibrium distribution,

{% katex display %}
\pi_E =
\begin{pmatrix}
v^{-1}_{11} \\
v^{-1}_{12} \\
\vdots \\
v^{-1}_{1N}
\end{pmatrix}.
{% endkatex %}

### Solution of Equilibrium Equation

An equation for the equilibrium distribution, {% katex %}\pi_{E}{% endkatex %}, can
be obtained from equation {% katex %}(11){% endkatex %},
{% katex display %}
\pi^{T}_E\left(P - I\right) = 0\ \ \ \ \ (12),
{% endkatex %}
where {% katex %}I{% endkatex %} is the identity matrix. Equation {% katex %}(12){% endkatex %} alone
is insufficient to obtain a unique solution since the system of linear equations it defines is
[Linearly Dependent](https://en.wikipedia.org/wiki/Linear_independence). In a linearly dependent
system of equations some equations are the result of linear operations on the others.
It is straight forward to show that one of the equations defined by {% katex %}(12){% endkatex %} can
be eliminated by summing the other equations and multiplying by {% katex %}-1{% endkatex %}.
If the equations were
linearly independent the only solution would be the trivial zero solution,
{% katex %}{\left( \pi^{T}_E \right)}_{i}\ =\ 0,\ \forall\ i{% endkatex %}. A unique solution to
{% katex %}(12){% endkatex %} is obtained by including the normalization constraint,
{% katex display %}
\sum_{j=1}^{N} {\left( \pi_{E}^{T} \right)}_{j} = 1.
{% endkatex %}
The resulting system of equations to be solved is given by,
{% katex display %}
\pi_{E}^{T}
\begin{pmatrix}
{P_{11} - 1} &amp; P_{12} &amp; \cdots &amp; P_{1N} &amp; 1 \\
P_{21} &amp; {P_{22} -1} &amp; \cdots &amp; P_{2N} &amp; 1 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots\\
P_{N1} &amp; P_{N2} &amp; \cdots &amp; {P_{NN} - 1} &amp; 1 \\
\end{pmatrix}
=
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}\ \ \ \ \ (13).
{% endkatex %}

## Example

Consider the Markov Chain defined by the transition matrix,
{% katex display %}
P =
\begin{pmatrix}
0.0 &amp; 0.9 &amp; 0.1 &amp; 0.0 \\
0.8 &amp; 0.1 &amp; 0.0 &amp; 0.1 \\
0.0 &amp; 0.5 &amp; 0.3 &amp; 0.2 \\
0.1 &amp; 0.0 &amp; 0.0 &amp; 0.9
\end{pmatrix}\ \ \ \ \ (14).
{% endkatex %}
The state transition diagram below provides a graphical representation of {% katex %}P{% endkatex %}.
&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img class=&quot;post-image&quot;  src=&quot;/assets/posts/discrete_state_markov_chain_equilibrium/transition_diagram.png&quot;&gt;
&lt;/div&gt;

### Convergence to Equilibrium

Relaxation of both the transition matrix and distribution to their equilibrium values
is easily demonstrated with the few lines of Python executed within `ipython`.

```shell
In [1]: import numpy
In [2]: t = [[0.0, 0.9, 0.1, 0.0],
   ...:      [0.8, 0.1, 0.0, 0.1],
   ...:      [0.0, 0.5, 0.3, 0.2],
   ...:      [0.1, 0.0, 0.0, 0.9]]
In [3]: p = numpy.matrix(t)
In [4]: p**100
Out[4]:
matrix([[0.27876106, 0.30088496, 0.03982301, 0.38053097],
        [0.27876106, 0.30088496, 0.03982301, 0.38053097],
        [0.27876106, 0.30088496, 0.03982301, 0.38053097],
        [0.27876106, 0.30088495, 0.03982301, 0.38053098]])
```

Here the transition matrix from the initial state to states {% katex %}100{% endkatex %} time steps
in the future is computed using equation {% katex %}(2){% endkatex %}. The result obtained
has identical rows as obtained in equation {% katex %}(9){% endkatex %}.
{% katex display %}
P^{100} =
\begin{pmatrix}
0.27876106 &amp; 0.30088496 &amp; 0.03982301 &amp; 0.38053097 \\
0.27876106 &amp; 0.30088496 &amp; 0.03982301 &amp; 0.38053097 \\
0.27876106 &amp; 0.30088496 &amp; 0.03982301 &amp; 0.38053097 \\
0.27876106 &amp; 0.30088495 &amp; 0.03982301 &amp; 0.38053098
\end{pmatrix}\ \ \ \ \ (15).
{% endkatex %}

For an initial distribution {% katex %}\pi{% endkatex %} the distribution after {% katex %}100{% endkatex %}
time steps is evaluated using,

```shell
In [5]: c = [[0.1],
   ...:      [0.5],
   ...:      [0.35],
   ...:      [0.05]]
In [6]: Ï€ = numpy.matrix(c)
In [8]: Ï€.T*p**100
Out[8]: matrix([[0.27876106, 0.30088496, 0.03982301, 0.38053097]])
```

Here an initial distribution is constructed that satisfies
{% katex %}\sum_{i=0}^3 \pi_i = 1{% endkatex %}. Then equation {% katex %}(5){% endkatex %} is used
to compute the distribution after {% katex %}100{% endkatex %} time steps.
The result is that expected from the previous analysis.
In the equilibrium limit the distribution is the repeated row of the equilibrium transition matrix,
namely,

{% katex display %}
\pi_{100} =
\begin{pmatrix}
0.27876106 \\
0.30088496 \\
0.03982301 \\
0.38053097
\end{pmatrix}\ \ \ \ \ (16).
{% endkatex %}

The plot below illustrates the convergence of the distribution from the previous example.
In the plot the components of {% katex %}\pi_t{% endkatex %} from equation {% katex %}(5){% endkatex %}
are plotted for each time step. The convergence to the limiting value occurs rapidly. Within only
{% katex %}20{% endkatex %} steps {% katex %}\pi_t{% endkatex %} has reached limiting distribution.

&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img class=&quot;post-image&quot;  src=&quot;/assets/posts/discrete_state_markov_chain_equilibrium/distribution_relaxation_1.png&quot;&gt;
&lt;/div&gt;

### Equilibrium Transition Matrix

In this section the equilibrium limit of the transition matrix is determined for the example Markov Chain
shown in equation {% katex %}(14){% endkatex %}.
It was previously shown that this limit is given by equation {% katex %}(9){% endkatex %}. To evaluate
this equation the example transition matrix must be diagonalized.
First, the transition matrix eigenvalues and eigenvectors are computed using the `numpy` linear
algebra library.

```shell
In [9]: Î», v = numpy.linalg.eig(p)
In [10]: Î»
Out[10]: array([-0.77413013,  0.24223905,  1.        ,  0.83189108])
In [11]: v
Out[11]:
matrix([[-0.70411894,  0.02102317,  0.5       , -0.4978592 ],
        [ 0.63959501,  0.11599428,  0.5       , -0.44431454],
        [-0.30555819, -0.99302222,  0.5       , -0.14281543],
        [ 0.04205879, -0.00319617,  0.5       ,  0.73097508]])
```

It is seen that {% katex %}\lambda\ =\ 1{% endkatex %} is indeed an eigenvalue,
as previously proven and that other eigenvalues have magnitudes less than {% katex %}1{% endkatex %}.
This is in agreement with Perron-Frobenius Theorem. The `numpy` linear algebra library normalizes the
eigenvectors and uses the same order for eigenvalues and eigenvector columns. The eigenvector
corresponding to {% katex %}\lambda\ =\ 1{% endkatex %} is in the third column and has all components equal.
Eigenvectors are only known to an arbitrary scalar, so the vector of {% katex %}1's{% endkatex %} used
in the previous analysis can be obtained by multiplying the third column by {% katex %}2{% endkatex %}.
After obtaining the eigenvalues and eigenvectors equation {% katex %}(9){% endkatex %} is evaluated
{% katex %}100{% endkatex %} after time steps and compared with the equilibrium limit.

```shell
In [12]: Î› = numpy.diag(Î»)
In [13]: V = numpy.matrix(v)
In [14]: Vinv = numpy.linalg.inv(V)
In [15]: Î›_t = Î›**100
In [16]: Î›_t
Out[16]:
array([[7.61022278e-12, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
       [0.00000000e+00, 2.65714622e-62, 0.00000000e+00, 0.00000000e+00],
       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00],
       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.01542303e-08]])
In [17]: V * Î›_t * Vinv
Out[17]:
matrix([[0.27876106, 0.30088496, 0.03982301, 0.38053097],
        [0.27876106, 0.30088496, 0.03982301, 0.38053097],
        [0.27876106, 0.30088496, 0.03982301, 0.38053097],
        [0.27876106, 0.30088495, 0.03982301, 0.38053098]])
```

First, the diagonal matrix of eigenvalues, {% katex %}\Lambda{% endkatex %}, is created maintaining
the order of {% katex %}\lambda{% endkatex %}. Next, the matrix {% katex %}V{% endkatex %} is constructed
with eigenvectors as columns while also maintaining the order of vectors in {% katex %}v{% endkatex %}.
The inverse of {% katex %}V{% endkatex %} is then computed. {% katex %}\Lambda^{t}{% endkatex %} can now be
computed for {% katex %}100{% endkatex %} time steps. The result is in agreement with the past effort where
the limit {% katex %}t\to\infty{% endkatex %} was evaluated giving a matrix that contained a
{% katex %}1{% endkatex %} in the {% katex %}(1,1){% endkatex %} component corresponding to the position
of the {% katex %}\lambda=1{% endkatex %} component and zeros for all others.
Here the eigenvectors are ordered differently but the only nonzero component has the
{% katex %}\lambda=1{% endkatex %} eigenvalue. Finally, equation {% katex %}(9){% endkatex %} is evaluated and
all rows are identical and equal to {% katex %}\pi_t{% endkatex %} evaluated at
{% katex %}t=100{% endkatex %}, in agreement with the equilibrium limit determined previously and calculation
performed in the last section shown in equations {% katex %}(15){% endkatex %} and {% katex %}(16){% endkatex %}.

### Equilibrium Distribution

The equilibrium distribution will now be calculated using the system of linear
equations defined by equation {% katex %}(9){% endkatex %}. Below the resulting system of equations
for the example distribution in equation {% katex %}(14){% endkatex %} is shown.

{% katex display %}
\pi_{E}^{T}
\begin{pmatrix}
-1.0 &amp; 0.9 &amp; 0.1 &amp; 0.0 &amp; 1.0 \\
0.8 &amp; -0.9 &amp; 0.0 &amp; 0.1 &amp; 1.0 \\
0.0 &amp; 0.5 &amp; -0.7 &amp; 0.2 &amp; 1.0 \\
0.1 &amp; 0.0 &amp; 0.0 &amp; -0.1 &amp; 1.0
\end{pmatrix}
=
\begin{pmatrix}
0.0 &amp; 0.0 &amp; 0.0 &amp; 0.0 &amp; 1.0
\end{pmatrix}
{% endkatex %}

This system of equations is solved using the least squares method provided by the `numpy` linear
algebra library. This library requires the use of the transpose of the above equation.
The first line below computes this transpose using equation {% katex %}(14){% endkatex %}.

```shell
In [18]: E = numpy.concatenate((p.T - numpy.eye(4), [numpy.ones(4)]))
In [19]: E
Out[19]:
matrix([[-1. ,  0.8,  0. ,  0.1],
        [ 0.9, -0.9,  0.5,  0. ],
        [ 0.1,  0. , -0.7,  0. ],
        [ 0. ,  0.1,  0.2, -0.1],
        [ 1. ,  1. ,  1. ,  1. ]])
In [20]: Ï€e, _, _, _ = numpy.linalg.lstsq(E, numpy.array([0.0, 0.0, 0.0, 0.0, 1.0]), rcond=None)

In [21]: Ï€e
Out[21]: array([0.27876106, 0.30088496, 0.03982301, 0.38053097])

```

Next, the equilibrium distribution is evaluated using the least squares method. The result obtained is
consistent with previous results shown in equation {% katex %}(16){% endkatex %}.

### Simulation

This section will use a direct simulation of equation {% katex %}(14){% endkatex %} to calculate the
equilibrium distribution and compare the result to those previously obtained. Below a Python
implementation of the calculation is shown.

```python
import numpy

def sample_chain(t, x0, nsample):
    xt = numpy.zeros(nsample, dtype=int)
    xt[0] = x0
    up = numpy.random.rand(nsample)
    cdf = [numpy.cumsum(t[i]) for i in range(4)]
    for t in range(nsample - 1):
        xt[t] = numpy.flatnonzero(cdf[xt[t-1]] &gt;= up[t])[0]
    return xt

# Simulation parameters
Ï€_nsamples = 1000
nsamples = 10000
c = [[0.25],
     [0.25],
     [0.25],
     [0.25]]

# Generate Ï€_nsamples initial state samples
Ï€ = numpy.matrix(c)
Ï€_cdf = numpy.cumsum(c)
Ï€_samples = [numpy.flatnonzero(Ï€_cdf &gt;= u)[0] for u in numpy.random.rand(Ï€_nsamples)]

# Run sample_chain for nsamples for each of the initial state samples
chain_samples = numpy.array([])
for x0 in Ï€_samples:
  chain_samples = numpy.append(chain_samples, sample_chain(t, x0, nsamples))
```

The function `sample_chain` performs the simulation. It uses
[Inverse CDF Sampling]({{ site.baseurl }}{% link _posts/2018-07-21-inverse_cdf_sampling.md %}) on the
discrete distribution obtained from the transition matrix defined by equation {% katex %}(14){% endkatex %}
for the state at step {% katex %}t{% endkatex %} to determine the state for the next time step,
{% katex %}t+1{% endkatex %}. The following code uses `sample_chain` to generate and ensemble of
simulations with the initial state also sampled from an assumed initial distribution.
First, simulation parameters are defined and the initial distribution is assumed to be uniform.
Second, `Ï€_nsamples` of the initial state are generated using Inverse CDF sampling with the
initial distribution. Finally, simulations of length `nsamples` are performed for each initial state.
The ensemble of samples are collected in the variable `chain_samples` and plotted below. A comparison
is made with the two other calculations performed. The first is {% katex %}\pi_t{% endkatex %} for
{% katex %}t=100{% endkatex %} shown in equation {% katex %}(16){% endkatex %} and the second
the previous solution to equation {% katex %}(9){% endkatex %}. The different
calculations are indistinguishable.

&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img  class=&quot;post-image&quot; src=&quot;/assets/posts/discrete_state_markov_chain_equilibrium/distribution_comparison.png&quot;&gt;
&lt;/div&gt;

## Conclusions

An overview of the properties of Markov Chain equilibrium has been given. It is shown that equilibrium
is a consequence of assuming the transition matrix and distribution vector are both stochastic.
The equilibrium solutions for the transition matrix and distribution were obtained analytically by
evaluating the limit {% katex %}t\to\infty{% endkatex %}. These results were compared
favorably to numerical calculations of the dynamical equations and ensemble simulations.</content><author><name>Troy Stribling</name></author><summary type="html">A Markov Chain is a sequence of states where transitions between states occur ordered in time with the probability of transition depending only on the previous state. Here the states will be assumed a discrete finite set and time a discrete unbounded set. If the set of states is given by {x1,Â x2,â€¦,Â xN}\{x_1,\ x_2,\ldots,\ x_N\}{x1â€‹,Â x2â€‹,â€¦,Â xNâ€‹} the probability that the process will be in state xix_ixiâ€‹ at time ttt is denoted by P(Xt=xi)P(X_t=x_i)P(Xtâ€‹=xiâ€‹), referred to as the distribution. Markov Chain equilibrium is defined by limtâ†’âˆžP(Xt=xi)Â &amp;lt;Â âˆž\lim_{t\to\infty}P(X_t=x_i)\ &amp;lt;\ \inftylimtâ†’âˆžâ€‹P(Xtâ€‹=xiâ€‹)Â &amp;lt;Â âˆž, that is, as time advances P(Xt=xi)P(X_t=x_i)P(Xtâ€‹=xiâ€‹) becomes independent of time. Here a solution for this limit is discussed and illustrated with examples.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/posts/discrete_state_markov_chain_equilibrium/distribution_comparison.png" /></entry><entry><title type="html">Rejection Sampling</title><link href="http://localhost:4000/2018/07/29/rejection_sampling.html" rel="alternate" type="text/html" title="Rejection Sampling" /><published>2018-07-29T00:00:00-07:00</published><updated>2018-07-29T00:00:00-07:00</updated><id>http://localhost:4000/2018/07/29/rejection_sampling</id><content type="html" xml:base="http://localhost:4000/2018/07/29/rejection_sampling.html">Rejection Sampling is a method for obtaining samples for a known target probability distribution
with no sampler using samples from some other proposal distribution with a sampler.
It is a more general method than
[Inverse CDF Sampling]({{ site.baseurl }}{% link _posts/2018-07-21-inverse_cdf_sampling.md %}) which requires
distribution to have an invertible CDF. Inverse CDF Sampling transforms a
{% katex %}\textbf{Uniform}(0,\ 1){% endkatex %} random variable into a random variable with
a desired target distribution using the inverted CDF of the target distribution. While, Rejection Sampling
is a method for transformation of random variables from arbitrary proposal distributions into a desired target
distribution.

&lt;!--more--&gt;

The implementation of Rejection Sampling requires the consideration of a target distribution,
{% katex %}f_X(X){% endkatex %}, a proposal distribution, {% katex %}f_Y(Y){% endkatex %}, and a {% katex %}\textbf{Uniform}(0,\ 1){% endkatex %} acceptance probability, {% katex %}U{% endkatex %}, with distribution {% katex %}f_U(U)=1{% endkatex %}. A proposal sample, {% katex %}Y{% endkatex %}, is generated using, {% katex %}f_Y(Y){% endkatex %}, and independently a uniform acceptance sample, {% katex %}U{% endkatex %}, is generated
using {% katex %}f_U(U){% endkatex %}.
A criterion is defined for acceptance of a sample, {% katex %}X{% endkatex %}, to be considered a
sample of {% katex %}f_X(X){% endkatex %},

{% katex display %}
U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)}\ \ \ \ \ (1),
{% endkatex %}

where, {% katex %}c{% endkatex %}, is chosen to satisfy
{% katex %}0\ \leq\ f_X(Y)/cf_Y(Y)\ \leq\ 1, \ \ \forall\ Y{% endkatex %}. If equation
{% katex %}(1){% endkatex %} is satisfied the proposed sample {% katex %}Y{% endkatex %} is
accepted as a sample of {% katex %}f_X(X){% endkatex %} where {% katex %}X=Y{% endkatex %}.
If equation {% katex %}(1){% endkatex %} is not satisfied {% katex %}Y{% endkatex %} is discarded.

The acceptance function is defined by,

{% katex display %}
h(Y) = \frac{f_X(Y)}{f_Y(Y)}\ \ \ \ \ (2).
{% endkatex %}

It can be insightful to compare {% katex %}h(y){% endkatex %} to {% katex %}f_X(y){% endkatex %}
when choosing a proposal distribution. If {% katex %}h(y){% endkatex %} does not have a peak that
can enclose the peak in {% katex %}f_X(Y){% endkatex %} then the choice of
{% katex %}f_Y(Y){% endkatex %} should be reconsidered.

The Rejection Sampling algorithm can be summarized in the following steps that are repeated for the generation of each sample,

1. Generate a sample {% katex %}Y \sim f_Y(Y){% endkatex %}.
2. Generate a sample {% katex %}U\sim\textbf{Uniform}(0,\ 1){% endkatex %} independent of {% katex %}Y{% endkatex %}.
3. If equation {% katex %}(1){% endkatex %} is satisfied then {% katex %}X=Y{% endkatex %} is accepted as a sample
of {% katex %}f_X(X){% endkatex %}. If equation {% katex %}(1){% endkatex %} is not satisfied then {% katex %}Y{% endkatex %}
is discarded.

## Proof

To prove that Rejection Sampling works it must be shown that,
{% katex display %}
P\left[Y\ \leq\ y\ |\ U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)}\right]=F_X(y)\ \ \ \ \ (3),
{% endkatex %}

where {% katex %}F_X(y){% endkatex %} is the CDF for {% katex %}f_X(y){% endkatex %},
{% katex display %}
F_X(y) = \int_{-\infty}^{y} f_X(w)dw.
{% endkatex %}

To prove equation {% katex %}(2){% endkatex %} a couple of intermediate steps are required. First,
The joint distribution of {% katex %}U{% endkatex %} and {% katex %}Y{% endkatex %} containing the
acceptance constraint will be evaluated,

{% katex display %}
P\left[U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)},\ Y\ \leq\ y\right] = \frac{F_X(y)}{c}\ \ \ \ \ (4).
{% endkatex %}

Since the Rejection Sampling algorithm as described in the previous section assumes
that {% katex %}Y{% endkatex %} and {% katex %}U{% endkatex %} are independent random variables,
{% katex%}
f_{YU}(Y, U)\ =\ f_Y(Y)f_U(U)\ = f_Y(Y).
{% endkatex %} It follows that,

{% katex display %}
\begin{aligned}
P\left[U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)},\ Y\ \leq y\right] &amp;= \int_{-\infty}^{y}\int_{0}^{f_X(w)/cf_Y(w)} f_{YU}(w, u) du dw\\
&amp;= \int_{-\infty}^{y}\int_{0}^{f_X(w)/cf_Y(w)} f_Y(w) du dw \\
&amp;= \int_{-\infty}^{y} f_Y(w) \int_{0}^{f_X(w)/cf_Y(w)} du dw \\
&amp;= \int_{-\infty}^{y} f_Y(w) \frac{f_X(w)}{cf_Y(w)} dw \\
&amp;= \frac{1}{c}\int_{-\infty}^y f_X(w) dw \\
&amp;= \frac{F_X(y)}{c}, \\
\end{aligned}
{% endkatex %}

Next, it will be shown that the probability of accepting a sample is given by,
{% katex display %}
P\left[U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)}\right] = \frac{1}{c}\ \ \ \ \ (5).
{% endkatex %}

This result follows from equation {% katex %}(4){% endkatex %} by taking the
limit {% katex %}y\to\infty{% endkatex %}
and noting that, {% katex %}\int_{-\infty}^{\infty} f_X(y) dy = 1{% endkatex %}.

{% katex display %}
\begin{aligned}
P\left[U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)}\right] &amp;= \int_{-\infty}^{\infty}\int_{0}^{f_X(w)/cf_Y(w)} f_{YU}(w, u) du dw\\
&amp;= \frac{1}{c}\int_{-\infty}^{\infty} f_X(w) dw \\
&amp;= \frac{1}{c}
\end{aligned}
{% endkatex %}

Finally, equation {% katex %}(3){% endkatex %} can be proven, using the definition of [Conditional Probability](https://en.wikipedia.org/wiki/Conditional_probability),
equation {% katex %}(4){% endkatex %} and equation {% katex %}(5){% endkatex %},

{% katex display %}
\begin{aligned}
P\left[Y\ \leq\ y\ |\ U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)}\right] &amp;= \frac{P\left[U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)},\ Y \leq y\right]}{P\left[U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)}\right]}\\
&amp;=\frac{F_X(y)}{c}\frac{1}{1/c}\\
&amp;=F_X(y)
\end{aligned}
{% endkatex %}

## Implementation

An implementation in Python of the Rejection Sampling algorithm,

```python
def rejection_sample(h, y_samples, c):
    nsamples = len(y_samples)
    u = numpy.random.rand(nsamples)
    accepted_mask = (u &lt;= h(y_samples) / c)
    return y_samples[accepted_mask]
```

The above function `rejection_sample(h, y_samples, c)` takes three arguments as input which are
described in the table below.

| Arguments  | Description  |
| :-----: | :---- |
| `h` | The acceptance function. Defined by {% katex %}h(Y)=f_X(Y)/f_Y(Y){% endkatex %}.|
| `y_samples` | Array of samples of {% katex %}Y{% endkatex %} generated using {% katex %}f_Y(Y){% endkatex %}.|
|`c`   |A constant chosen so {% katex %}0\ \leq\ h(Y)/c\ \leq\ 1, \ \ \forall\ Y{% endkatex %} |

The execution of `rejection_sample(h, y_samples, c)` begins by generating an appropriate number of acceptance variable samples, {% katex %}U{% endkatex %}, and
then determines which satisfy the acceptance criterion specified by equation (1). The accepted samples are then returned.

## Examples

Consider the [Weibull Distribution](https://en.wikipedia.org/wiki/Weibull_distribution). The PDF is
given by,

{% katex display %}
f_X(x; k, \lambda) =
\begin{cases}
\frac{k}{\lambda}\left(\frac{x}{\lambda} \right)^{k-1} e^{\left(\frac{-x}{\lambda}\right)^k} &amp; x\ \geq\ 0 \\
0 &amp; x &lt; 0
\end{cases} \ \ \ \ \ (4),
{% endkatex %}

where {% katex %}k\ &gt;\ 0{% endkatex %} is the shape parameter and {%katex %}\lambda\ &gt;\ 0{% endkatex %} the scale parameter.
The CDF is given by,

{% katex display %}
F_X(x; k, \lambda) =
\begin{cases}
1-e^{\left(\frac{-x}{\lambda}\right)^k
} &amp; x \geq 0 \\
0 &amp; x &lt; 0.
\end{cases}
{% endkatex %}

The first and second moments are,

{% katex display %}
\begin{aligned}
\mu &amp; = \lambda\Gamma\left(1+\frac{1}{k}\right) \\
\sigma^2 &amp; = \lambda^2\left[\Gamma\left(1+\frac{2}{k}\right)-\left(\Gamma\left(1+\frac{1}{k}\right)\right)^2\right]
\end{aligned} \ \ \ \ \ (5).
{% endkatex %}

In the examples described here it will be assumed that {% katex %}k=5.0{% endkatex %} and
{% katex %}\lambda=1.0{% endkatex %}. The following plot shows the PDF and CDF using these values.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_pdf.png&quot;&gt;

The following sections will compare the performance of generating Weibull samples using {% katex %}\textbf{Uniform}(0,\ m){% endkatex %} and
{% katex %}\textbf{Normal}(\mu,\ \sigma){% endkatex %} proposal distributions.

### Uniform Proposal Distribution

Here a {% katex %}\textbf{Uniform}(0,\ m){% endkatex %} proposal distribution will be used to
generate samples for the Weibull distribution {% katex %}(4){% endkatex %}. It provides a simple and
illustrative example of the algorithm. The following plot shows the
target distribution {% katex %}f_X(y){% endkatex %}, the proposal distribution
{% katex %}f_Y(y){% endkatex %} and the acceptance function
{% katex %}h(y)=f_X(y)/f_Y(y){% endkatex %} used in this example. The
uniform proposal distribution requires that a bound be placed
on the proposal samples, which will be assumed to be {% katex %}m=1.6{% endkatex %}. Since
the proposal distribution is constant the acceptance function, {% katex %}h(y){% endkatex %}, will be
a constant multiple of the target distribution. This is illustrated in the plot below.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_uniform_sampled_functions.png&quot;&gt;

The Python code used to generate the samples using  `rejection_sample(h, y_samples, c)` is listed
below.

```python
weibull_pdf = lambda v: (k/Î»)*(v/Î»)**(k-1)*numpy.exp(-(v/Î»)**k)

k = 5.0
Î» = 1.0

xmax = 1.6
ymax = 2.0
nsamples = 100000

y_samples = numpy.random.rand(nsamples) * xmax
samples = rejection_sample(weibull_pdf, y_samples, ymax)
```

The following plot compares the histogram computed form the generated samples with the target
distribution {% katex %}(4){% endkatex %}. The fit is a little ragged along the peak of the distribution but acceptable.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_uniform_sampled_distribution.png&quot;&gt;

The next two plots illustrate convergence of the sample mean, {% katex %}\mu{% endkatex %},
and standard deviation, {% katex %}\sigma{% endkatex %}, by comparing the cumulative sums
computed from the samples to target distribution values computed
from equation {% katex %}(5){% endkatex %}. The convergence of the sampled
{% katex %}\mu{% endkatex %} is quite rapid. Within only {% katex %}10^2{% endkatex %}
samples {% katex %}\mu{% endkatex %} computed form the samples is very close the the target value
and by {% katex %}10^3{% endkatex %} samples the two values are indistinguishable. The convergence of the sampled {% katex %}\sigma{% endkatex %} to the target value is not as
rapid as the convergence of the sampled {% katex %}\mu{% endkatex %} but is still quick. By
{% katex %}10^3{% endkatex %} samples the two values are near indistinguishable.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_uniform_mean_convergence.png&quot;&gt;

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_uniform_sigma_convergence.png&quot;&gt;

Even though {% katex %}10^5{% endkatex %} proposal samples were generated not all were accepted. The
plot below provides insight into the efficiency of the algorithm. In the plot the generated pairs of
acceptance probability {% katex %}U{% endkatex %} and sample proposal {% katex %}Y{% endkatex %} are
plotted with {% katex %}U{% endkatex %} on the vertical axis and {% katex %}Y{% endkatex %} on
the horizontal axis. Also, shown is the scaled acceptance function {% katex %}h(Y)/c{% endkatex %}.
If {% katex %}U\ &gt;\ h(Y)/c{% endkatex %} the sample is rejected and colored orange in the plot and
if {% katex %}U\ \leq\ h(Y)/c{% endkatex %} the sample is accepted, {% katex %}X=Y{% endkatex %} and colored blue. Only {% katex %}31\%{% endkatex %} of the generated samples were accepted.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_uniform_efficiency.png&quot;&gt;

To improve the acceptance percentage of proposed samples a different proposal distribution must be
considered. In the plot above it is seen that the {% katex %}\textbf{Uniform}(0,\ 1.6){% endkatex %} proposal distribution uniformly samples the space enclosed by the rectangle it defines without consideration
for the shape of
the target distribution. The acceptance percentage will be determined by the ratio of the target
distribution area enclosed by the proposal distribution and the proposal distribution area.
As the target distribution becomes sharp
the acceptance percentage will decrease. A proposal distribution that samples the area under
{% katex %}h(Y)/c{% endkatex %} efficiently will have a higher acceptance percentage. It should be kept in mind that rejection of proposal samples is required for the algorithm to work.
If no proposal samples are rejected the proposal and target distributions will be equivalent.

### Normal Proposal Distribution

In this section a sampler using a {% katex %}\textbf{Normal}(\mu,\ \sigma){% endkatex %} proposal
distribution and target Weibull distribution is discussed. A Normal proposal distribution has advantages over
the {% katex %}\textbf{Uniform}(0,\ m){% endkatex %} distribution discussed in the previous
section. First, it can provide unbounded samples, while a uniform proposal requires specifying bounds
on the samples. Second, it is a closer approximation to the target distribution so it should provide samples
that are accepted with greater frequency. A disadvantage of the Normal proposal distribution is that it requires specification of {% katex %}\mu{% endkatex %} and {% katex %}\sigma{% endkatex %}.
If these parameters are the slightest off the performance of the
sampler will be severely degraded. To learn this lesson the first attempt will assume values for
both the parameters that closely match the target distribution. The following plot compares the {% katex %}f_X(y){% endkatex %}, the proposal distribution {% katex %}f_Y(y){% endkatex %} and the acceptance function
{% katex %}h(y){% endkatex %}. There is a large peak in {% katex %}h(y){% endkatex %} to right caused
by the more rapid increase of the Weibull distribution relative to the Normal distribution with the result
that most of its mass is not aligned with the target distribution.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_normal_1_sampled_functions.png&quot;&gt;

The Python code used to generate the samples using  `rejection_sample(h, y_samples, c)` is listed
below.

```python
weibull_pdf = lambda v: (k/Î»)*(v/Î»)**(k-1)*numpy.exp(-(v/Î»)**k)

def normal(Î¼, Ïƒ):
    def f(x):
        Îµ = (x - Î¼)**2/(2.0*Ïƒ**2)
        return numpy.exp(-Îµ)/numpy.sqrt(2.0*numpy.pi*Ïƒ**2)
    return f

k = 5.0
Î» = 1.0

Ïƒ = 0.2
Î¼ = 0.95

xmax = 1.6
nsamples = 100000

y_samples = numpy.random.normal(Î¼, Ïƒ, nsamples)
ymax = h(x_values).max()
h = lambda x: weibull_pdf(x) / normal(Î¼, Ïƒ)(x)

samples = rejection_sample(h, y_samples, ymax)
```

The first of the following plots compares the histogram computed from the generated samples with the target
distribution {% katex %}(4){% endkatex %} and the second illustrates which proposal samples were accepted.
The histogram fit is good but only {% katex %}23\%{% endkatex %} of the samples were accepted which is worse
than the result obtained with the uniform proposal previously discussed.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_normal_1_sampled_distribution.png&quot;&gt;

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_normal_1_efficiency.png&quot;&gt;

In an attempt to improve the acceptance rate {% katex %}\mu{% endkatex %} of the proposal distribution
is decreased slightly and {% katex %}\sigma{% endkatex %} is increased.
The result is shown in the next plot.
The proposal distribution now covers the target distribution tails. The acceptance function,
{% katex %}h(Y){% endkatex %}, now has its peak inside the target distribution with significant
overlap of mass.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_normal_3_sampled_functions.png&quot;&gt;

The result is much improved. In the plot below it is seen that the percentage of accepted samples has
increased to {% katex %}79\%{% endkatex %}.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_normal_3_efficiency.png&quot;&gt;

The first of the plots below compares the histogram computed from generated samples with the target
distribution {% katex %}(4){% endkatex %} and next two compare the cumulative values of {% katex %}\mu{% endkatex %} and {% katex %}\sigma{% endkatex %} computed from the generated samples with the target distribution values from equation {% katex %}(5){% endkatex %}. The histogram is the best fit of the
examples discussed here and convergence of the sampled {% katex %}\mu{% endkatex %} and
{% katex %}\sigma{% endkatex %} occurs in about {% katex %}10^3{% endkatex %} samples.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_normal_3_sampled_distribution.png&quot;&gt;

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_normal_3_mean_convergence.png&quot;&gt;

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_normal_3_sigma_convergence.png&quot;&gt;

Sampling the Weibull distribution with a Normal proposal distribution can produce a better result than a
uniform distribution but care must be exercised in selecting the Normal distribution parameters.
Some choices can produce inferior results. Analysis of the the acceptance function
{% katex %}(2){% endkatex %} can provide guidance in parameter selectiion.</content><author><name>Troy Stribling</name></author><summary type="html">Rejection Sampling is a method for obtaining samples for a known target probability distribution with no sampler using samples from some other proposal distribution with a sampler. It is a more general method than Inverse CDF Sampling which requires distribution to have an invertible CDF. Inverse CDF Sampling transforms a Uniform(0,Â 1)\textbf{Uniform}(0,\ 1)Uniform(0,Â 1) random variable into a random variable with a desired target distribution using the inverted CDF of the target distribution. While, Rejection Sampling is a method for transformation of random variables from arbitrary proposal distributions into a desired target distribution.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/posts/rejection_sampling/weibull_normal_3_efficiency.png" /></entry><entry><title type="html">Inverse CDF Sampling</title><link href="http://localhost:4000/2018/07/21/inverse_cdf_sampling.html" rel="alternate" type="text/html" title="Inverse CDF Sampling" /><published>2018-07-21T00:00:00-07:00</published><updated>2018-07-21T00:00:00-07:00</updated><id>http://localhost:4000/2018/07/21/inverse_cdf_sampling</id><content type="html" xml:base="http://localhost:4000/2018/07/21/inverse_cdf_sampling.html">Inverse [CDF](https://en.wikipedia.org/wiki/Cumulative_distribution_function) sampling is a method
for obtaining samples from both discrete and continuous probability distributions that requires the
CDF to be invertible.
The method assumes values of the CDF are Uniform random variables on {% katex %}[0, 1]{% endkatex %}.
CDF values are generated and used as input into the inverted CDF to obtain samples with the
distribution defined by the CDF.
&lt;!--more--&gt;

## Discrete Distributions

A discrete probability distribution consisting of a finite set of {% katex %}N{% endkatex %}
probability values is defined by,

{% katex display %}
\{p_1,\ p_2,\ \ldots,\ p_N\}\ \ \ \ \ (1),
{% endkatex %}

with {% katex %}p_i\ \geq\ 0, \  \forall i{% endkatex %} and {% katex %}\sum_{i=1}^N{p_i} = 1{% endkatex %}.
The CDF specifies the probability that {% katex %}i \leq n{% endkatex %} and is given by,

{% katex display %}
P(i\ \leq\ n)=P(n)=\sum_{i=1}^n{p_i},
{% endkatex %}

For a given CDF value, {% katex %}U{% endkatex %}, Equation {% katex %}P(n){% endkatex %} can always
be inverted by evaluating it for each {% katex %}n{% endkatex %} and
searching for the value of {% katex %}n{% endkatex %} that satisfies, {% katex %}P(n)\ \geq\ U{% endkatex %}.
It follows that generated samples of {% katex %}U \sim \textbf{Uniform}(0, 1){% endkatex %} will have distribution
{% katex %}(1){% endkatex %} since the intervals {% katex %}P(n)-P(n-1) = p_n{% endkatex %}
are Uniformly sampled.

Consider the distribution,

{% katex display %}
\left\{\frac{1}{12},\ \frac{1}{12},\ \frac{1}{6},\ \frac{1}{6},\ \frac{1}{12},\ \frac{5}{12} \right\} \ \ \ \ \ (2)
{% endkatex %}
It is shown in the following plot with its CDF. Note that the CDF is a monotonically increasing function.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/inverse_cdf_sampling/discrete_cdf.png&quot;&gt;

A sampler using the Inverse CDF method with target distribution specified in {% katex %}(2){% endkatex %} implemented in
Python is shown below.

```python
import numpy

n = 10000
df = numpy.array([1/12, 1/12, 1/6, 1/6, 1/12, 5/12])
cdf = numpy.cumsum(df)

samples = [numpy.flatnonzero(cdf &gt;= u)[0] for u in numpy.random.rand(n)]
```
The program first stores the CDF computed from each of the sums
{% katex %}P(n){% endkatex %} in an array. Next, CDF samples using
{% katex %}U \sim \textbf{Uniform}(0, 1){% endkatex %} are generated. Finally, for each sampled CDF value,
{% katex %}U{% endkatex %}, the array containing {% katex %}P(n){% endkatex %} is scanned for
the value of {% katex %}n{% endkatex %} where {% katex %}P(n)\ \geq\ U{% endkatex %}. The resulting
values of {% katex %}n{% endkatex %} will have the target distribution {% katex %}(2){% endkatex %}. This is
shown in the figure below.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/inverse_cdf_sampling/discrete_sampled_distribution.png&quot;&gt;

A measure of convergence of the samples to the target distribution can be obtained by comparing the cumulative
moments of the distribution computed from the samples with the target value of the moment computed analytically.
In general for a discrete distribution the first and second moments are given by,

{% katex display %}
\begin{aligned}
\mu &amp; =  \sum_{i=1}^N ip_i\\
\sigma^2 &amp; = \sum_{i=1}^N i^2p_i - \mu^2,
\end{aligned}
{% endkatex %}

In the following two plots the cumulative values of {% katex %}\mu{% endkatex %} and
{% katex %}\sigma{% endkatex %} computed from the samples generated using Inverse CDF Sampling
are compared with the target values using the equations above.
The first shows the convergence of {% katex %}\mu{% endkatex %} and the second the convergence of
{% katex %}\sigma{% endkatex %}. Within only {% katex %}10^3{% endkatex %} samples both
{% katex %}\mu{% endkatex %} and {% katex %}\sigma{% endkatex %} computed from samples is comparable to the
target value and by {% katex %}10^4{% endkatex %} the values are indistinguishable.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/inverse_cdf_sampling/discrete_sampled_mean_convergence.png&quot;&gt;

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/inverse_cdf_sampling/discrete_sampled_sigma_convergence.png&quot;&gt;

The number of operations required for generating samples using Inverse CDF sampling from a discrete
distribution will scale {% katex %}O(N_{samples}N){% endkatex %} where {% katex %}N_{samples}{% endkatex %}
is the desired number of samples and {% katex %}N{% endkatex %} is the number of terms in the discrete distribution.

It is also possible to directly sample distribution {% katex %}(2){% endkatex %} using the `multinomial` sampler from `numpy`,

```python
import numpy

n = 10000
df = numpy.array([1/12, 1/12, 1/6, 1/6, 1/12, 5/12])
samples = numpy.random.multinomial(n, df, size=1)/n
```

## Continuous Distributions

A continuous probability distribution is defined by the [PDF](https://en.wikipedia.org/wiki/Probability_density_function),
{% katex %}f_X(x){% endkatex %}, where {% katex %}f_X(x) \geq 0,\ \forall x{% endkatex %} and
{% katex %}\int_{-\infty}^{\infty} f_X(x) dx = 1{% endkatex %}. The CDF is a monotonically increasing function
that specifies the probability that {% katex %}X\ \leq\ x{% endkatex %}, namely,

{% katex display %}
P(X \leq x) = F_X(x) = \int_{-\infty}^{x} f_X(w) dw.
{% endkatex %}

### Proof

To prove that Inverse CDF sampling works for continuous distributions it must be shown that,

{% katex display %}
P[F_X^{-1}(U)\ \leq\ x] = F_X(x) \ \ \ \ \ (3),
{% endkatex %}

where {% katex %}F_X^{-1}(x){% endkatex %} is the inverse of {% katex %}F_X(x){% endkatex %}
and {% katex %}U \sim \textbf{Uniform}(0, 1){% endkatex %}.

A more general result needed to complete this proof is obtained using a change of variable on a CDF.
If {% katex %}Y=G(X){% endkatex %} is a monotonically increasing invertible function
of {% katex %}X{% endkatex %} then,

{% katex display %}
P(X\ \leq\ x) = P(Y\ \leq\ y) = P[G(X)\ \leq\ G(x)]. \ \ \ \ \ (4)
{% endkatex %}

To prove this note that {% katex %}G(x){% endkatex %} is monotonically increasing so the ordering of values is
preserved for all {% katex %}x{% endkatex %},

{% katex display %}
X\ \leq\ x \implies G(X)\ \leq\ G(x).
{% endkatex %}

Consequently, the order of the integration limits is maintained by the transformation.
Further, since {% katex %}y=G(x){% endkatex %} is invertible,
{% katex %}x = G^{-1}(y){% endkatex %} and {% katex %}dx = (dG^{-1}/dy) dy{% endkatex %}, so

{% katex display %}
\begin{aligned}
P(X\ \leq\ x) &amp; = \int_{-\infty}^{x} f_X(w) dw \\
&amp; = \int_{-\infty}^{y} f_X(G^{-1}(z)) \frac{dG^{-1}}{dz} dz \\
&amp; = \int_{-\infty}^{y} f_Y(z) dz \\
&amp; = P(Y\ \leq\ y) \\
&amp; = P[G(X)\ \leq\ G(x)],
\end{aligned}
{% endkatex %}

where,

{% katex display %}
f_Y(y) = f_X(G^{-1}(y)) \frac{dG^{-1}}{dy}
{% endkatex %}

For completeness consider the case where {% katex %}Y=G(X){% endkatex %} is a monotonically decreasing invertible function
of {% katex %}X{% endkatex %} then,

{% katex display %}
X\ \leq\ x \implies G(X)\ \geq\ G(x),
{% endkatex %}

it follows that,

{% katex display %}
\begin{aligned}
P(X\ \leq x) &amp; = \int_{-\infty}^{x} f_X(w) dw \\
&amp; = \int_{y}^{\infty} f_X(G^{-1}(z)) \frac{dG^{-1}}{dz} dz \\
&amp; = \int_{y}^{\infty} f_Y(z) dz \\
&amp; = P(Y\ \geq\ y) \\
&amp; = P[G(X)\ \geq\ G(x)] \\
&amp; = 1 - P[G(X)\ \leq\ G(x)]
\end{aligned}
{% endkatex %}

The desired proof of equation {% katex %}(3){% endkatex %} follows from equation {% katex %}(4){% endkatex %}
by noting that {% katex %}U \sim \textbf{Uniform}(0, 1){% endkatex %} so {% katex %}f_U(u) = 1{% endkatex %},

{% katex display %}
\begin{aligned}
P[F_X^{-1}(U)\ \leq\ x] &amp; = P[F_X(F_X^{-1}(U))\ \leq\ F_X(x)] \\
&amp; = P[U\ \leq\ F_X(x)] \\
&amp; = \int_{0}^{F_X(x)} f_U(w) dw \\
&amp; = \int_{0}^{F_X(x)} dw \\
&amp; = F_X(x).
\end{aligned}
{% endkatex %}

### Example

Consider the [Weibull Distribution](https://en.wikipedia.org/wiki/Weibull_distribution). The PDF is
given by,

{% katex display %}
f_X(x; k, \lambda) =
\begin{cases}
\frac{k}{\lambda}\left(\frac{x}{\lambda} \right)^{k-1} e^{\left(\frac{-x}{\lambda}\right)^k} &amp; x\ \geq\ 0 \\
0 &amp; x &lt; 0,
\end{cases} \ \ \ \ \ (5)
{% endkatex %}

where {% katex %}k\ &gt;\ 0{% endkatex %} is the shape parameter and {%katex %}\lambda\ &gt;\ 0{% endkatex %} the scale parameter.
The CDF is given by,

{% katex display %}
F_X(x; k, \lambda) =
\begin{cases}
1-e^{\left(\frac{-x}{\lambda}\right)^k} &amp; x \geq 0 \\
0 &amp; x &lt; 0.
\end{cases}
{% endkatex %}

The CDF can be inverted to yield,

{% katex display %}
F_X^{-1}(u; k, \lambda) =
\begin{cases}
\lambda\ln\left(\frac{1}{1-u}\right)^{\frac{1}{k}} &amp; 0 \leq u \leq 1 \\
0 &amp; u &lt; 0 \text{ or } u &gt; 1.
\end{cases}
{% endkatex %}

In the example described here it will be assumed that {% katex %}k=5.0{% endkatex %} and
{% katex %}\lambda=1.0{% endkatex %}. The following plot shows the PDF and CDF using these values.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/inverse_cdf_sampling/weibull_cdf.png&quot;&gt;

The sampler implementation for the continuous case is simpler than for the discrete case.
Just as in the discrete case CDF samples with distribution {% katex %}U \sim \textbf{Uniform}(0, 1){% endkatex %}
are generated. The desired samples with the target Weibull distribution are then computed using the CDF inverse.
Below an implementation of the sampler in Python is listed.

```python
import numpy

k = 5.0
Î» = 1.0
nsamples = 100000

cdf_inv = lambda u: Î» * (numpy.log(1.0/(1.0 - u)))**(1.0/k)
samples = [cdf_inv(u) for u in numpy.random.rand(nsamples)]
```

The following plot compares a histogram of the samples generated by the sampler above and the target
distribution (5). The fit is quite good. The subtle asymmetry of the Weibull distribution is captured.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/inverse_cdf_sampling/weibull_sampled_distribution.png&quot;&gt;

The first and second moments for the Weibull distribution are given by,

{% katex display %}
\begin{aligned}
\mu &amp; = \lambda\Gamma\left(1+\frac{1}{k}\right) \\
\sigma^2 &amp; = \lambda^2\left[\Gamma\left(1+\frac{2}{k}\right)-\left(\Gamma\left(1+\frac{1}{k}\right)\right)^2\right],
\end{aligned}
{% endkatex %}

where {% katex %}\Gamma(x){% endkatex %} is the [Gamma function](https://en.wikipedia.org/wiki/Gamma_function).

The following two plots compare the cumulative values of {% katex %}\mu{% endkatex %} and {% katex %}\sigma{% endkatex %}
computed from sampling of target distribution {% katex %}(5){% endkatex %} with the values from the equations above.
The first shows the convergence of {% katex %}\mu{% endkatex %} and the second the convergence of {% katex %}\sigma{% endkatex %}.
Within only {% katex %}10^3{% endkatex %} samples both {% katex %}\mu{% endkatex %} and {% katex %}\sigma{% endkatex %} computed from
samples are comparable to the target values and by {% katex %}10^4{% endkatex %} the values are indistinguishable.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/inverse_cdf_sampling/weibull_sampled_mean_convergence.png&quot;&gt;

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/inverse_cdf_sampling/weibull_sampled_std_convergence.png&quot;&gt;

### Performance

Any continuous distribution can be sampled by assuming that, {% katex %}f_X(x){% endkatex %}, can be approximated
by the discrete distribution, {% katex %}\left\{f_X(x_i)\Delta x_i \right\}_N{% endkatex %} for
{% katex %}i=1,2,3,\ldots,N{% endkatex %}, where {% katex %}\Delta x_i=(x_{max}-x_{min})/(N-1){% endkatex %} and
{% katex %}x_i = x_{min}+(i-1)\Delta x_i{% endkatex %}. This method has disadvantages compared to using
Inverse CDF sampling on the continuous distribution. First, a bounded range for the samples must
be assumed when in general the range of the samples can be unbounded.
The Inverse CDF method can sample an unbounded range. Second, the performance for
sampling a discrete distribution scales {% katex %}O(N_{samples}N){% endkatex %} while sampling the
continuous distribution is {% katex %}O(N_{samples}){% endkatex %}.</content><author><name>Troy Stribling</name></author><summary type="html">Inverse CDF sampling is a method for obtaining samples from both discrete and continuous probability distributions that requires the CDF to be invertible. The method assumes values of the CDF are Uniform random variables on [0,1][0, 1][0,1]. CDF values are generated and used as input into the inverted CDF to obtain samples with the distribution defined by the CDF.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/posts/inverse_cdf_sampling/weibull_sampled_distribution.png" /></entry></feed>
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-09-06T08:37:14-07:00</updated><id>http://localhost:4000/</id><title type="html">gly.fish</title><subtitle></subtitle><entry><title type="html">Discrete Cross Correlation Theorem</title><link href="http://localhost:4000/2018/08/25/discrete_cross_correlation_theorem.html" rel="alternate" type="text/html" title="Discrete Cross Correlation Theorem" /><published>2018-08-25T00:00:00-07:00</published><updated>2018-08-25T00:00:00-07:00</updated><id>http://localhost:4000/2018/08/25/discrete_cross_correlation_theorem</id><content type="html" xml:base="http://localhost:4000/2018/08/25/discrete_cross_correlation_theorem.html">The [Cross Correlation Theorem](https://en.wikipedia.org/wiki/Cross-correlation) is similar to the more
widely known [Convolution Theorem](https://en.wikipedia.org/wiki/Convolution_theorem). The cross correlation
of two discrete finite time series {% katex %}\{f_0,\ f_1,\ f_2,\ldots\,\ f_{N-1}\}{% endkatex %} and
{% katex %}\{g_0,\ g_1,\ g_2,\ldots\,\ g_{N-1}\}{% endkatex %} is defined by,

{% katex display %}
\psi_t = \sum_{n=0}^{N-1} f_{n} g_{n+t}\ \ \ \ \ (1),
{% endkatex %}

where {% katex %}t{% endkatex %} is called the *time lag*. Cross correlation provides a measure of the similitude
of two time series when shifted by the time lag. A direct calculation of the cross correlation using the
equation above requires {% katex %}O(N^2){% endkatex %} operations. The Cross Correlation Theorem
provides a method for calculating cross correlation in {% katex %}O(NlogN){% endkatex %}
operations by use of the
[Fast Fourier Transform](https://en.wikipedia.org/wiki/Fast_Fourier_transform). Here the theoretical
background required to understand cross correlation calculations using the Cross Correlation Theorem is discussed.
Example calculations are performed and different implementations using the FFT libraries in `numpy` compared.
The important special case of the cross correlation called [Autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation) is addressed in the final section.

&lt;!--more--&gt;

## Cross Correlation

Cross Correlation can be understood by considering the [Covariance](https://en.wikipedia.org/wiki/Covariance) of
two random variables, {% katex %}X{% endkatex %} and {% katex %}Y{% endkatex %}. Covariance is the
[Expectation](https://en.wikipedia.org/wiki/Expected_value) of the product of the deviations of the random variables
from their respective means,

{% katex display %}
\begin{aligned}
Cov(X,Y) &amp;= E\left[(X-E[X])(Y-E[Y])\right] \\
&amp;= E[XY] - E[X]E[Y].
\end{aligned}\ \ \ \ \ (2)
{% endkatex %}

Note that {% katex %}Cov(X,Y)=Cov(Y,X){% endkatex %} and if {% katex %}X{% endkatex %} and
{% katex %}Y{% endkatex %} are
[Independent Random Variables](https://en.wikipedia.org/wiki/Independence_(probability_theory))
{% katex %}E[XY]=E[X]E[Y]\ \implies\ Cov(X,Y)=0{% endkatex %}.
If {% katex %}X=Y{% endkatex %} the Covariance reduces to the [Variance](https://en.wikipedia.org/wiki/Variance),

{% katex display %}
\begin{aligned}
Var(X) &amp;= E\left[(X-E[X])^2\right] \\
&amp;= E[X^2] - \left(E[X]\right)^2.
\end{aligned}
{% endkatex %}

These two results are combined in the definition of the
[Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient),

{% katex display %}
\rho_{XY} = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}.
{% endkatex %}

The correlation coefficient has a geometric interpretation leading to the conclusion that
{% katex %}-1\ \leq \rho_{XY}\ \leq 1{% endkatex %}. At the extreme values
{% katex %}X{% endkatex %} and {% katex %}Y{% endkatex %} are either the same or differ by a
multiple of {% katex %}-1{% endkatex %}. At the midpoint value, {% katex %}\rho_{XY}=0{% endkatex %},
{% katex %}X{% endkatex %} and {% katex %}Y{% endkatex %} are independent random variables. If follows that
{% katex %}\rho_{XY}{% endkatex %} provides a measure of possible *dependence* or *similarity* of two random variables.

Consider the first term in equation {% katex %}(2){% endkatex %} when a time series of samples
of {% katex %}X{% endkatex %} and {% katex %}Y{% endkatex %} of equal length are available,

{% katex display %}
\begin{aligned}
E[XY]&amp; \approx \frac{1}{N^2}\sum_{n=0}^{N-1}x_n y_n \\
&amp;\propto\ \psi_{0},
\end{aligned}
{% endkatex %}

if {% katex %}N{% endkatex %} is sufficiently large. Generalizing this result to arbitrary time shifts leads to
equation {% katex %}(1){% endkatex %}.

An important special case of the cross correlation is the autocorrelation which is defined a the cross
correlation of a time series with itself,

{% katex display %}
r_t = \sum_{n=0}^{N-1} x_{n} x_{n+t}\ \ \ \ \ (3).
{% endkatex %}

Building on the interpretation of cross correlation the autocorrelation is viewed as a measure of *dependence*
or *similarity* of the past and future of a time series. For a time lag of {% katex %}t=0{% endkatex %},

{% katex display %}
r_0\ \propto\ E\left[X^2\right].
{% endkatex %}

## Discrete Fourier Transform

This section will discuss properties of the Discrete Fourier Transform that are used in following sections.
The [Discrete Fourier Transform](https://en.wikipedia.org/wiki/Discrete_Fourier_transform) for a discrete periodic
time series of length {% katex %}N{% endkatex %}, {% katex %}\{f_0,\ f_1,\ f_2,\ldots,\ f_{N-1}\}{% endkatex %},
is defined by,

{% katex display %}
\begin{gathered}
f_{n} = \frac{1}{N}\sum_{k=0}^{N-1}F_{k}e^{2\pi i (k/N)n} \\
F_{k} = \sum_{n=0}^{N-1}f_{n}e^{-2\pi i (n/N)k},
\end{gathered}\ \ \ \ \ (4)
{% endkatex %}

where the expression for {% katex %}f_{n}{% endkatex %} is referred to the inverse transform and the one for
{% katex %}F_{k}{% endkatex %} the forward transform.

### Linearity

Both the forward and inverse transforms are [Linear Operators](http://mathworld.wolfram.com/LinearOperator.html).
An operator, {% katex %}\mathfrak{F}{% endkatex %}, is linear if the operation on a sum is equal the sum of the operations,

{% katex display %}
\mathfrak{F}(a+b) = \mathfrak{F}(a)+\mathfrak{F}(b)\ \ \ \ \ (5)
{% endkatex %}

To show this for the forward transform consider {% katex %}h_{n}=f_{n}+g_{n}{% endkatex %}, then,

{% katex display %}
\begin{aligned}
H_{k} &amp;= \sum_{n=0}^{N-1}h_{n}e^{-2\pi i (n/N)k} \\
&amp;= \sum_{n=0}^{N-1}\left(f_{n}+g_{n}\right)e^{-2\pi i (n/N)k} \\
&amp;= \sum_{n=0}^{N-1}f_{n} e^{-2\pi i (n/N)k} + \sum_{n=0}^{N-1} g_{n}e^{-2\pi i (n/N)k} \\
&amp;= F_{k} + G_{k}.
\end{aligned}
{% endkatex %}

Similarly it can be shown that the inverse transform is linear.

### Periodicity

Periodicity of the forward transform implies that,
{% katex display %}
F_{k+mN}=F_{k}\ \ \ \ \ (6),
{% endkatex %}
where {% katex %}m=\{\ldots,-2,\ -1,\ 0,\ 1,\ 2,\ldots\}{% endkatex %}.
To show that this is true first consider the case
{% katex %}m=1{% endkatex %},

{% katex display %}
\begin{aligned}
F_{k+N} &amp;= \sum_{n=0}^{N} f_{n}e^{-2\pi i (n/N)(k+N)} \\
&amp;= \sum_{n=0}^{N} f_{n} e^{-2\pi i(n/N)k}e^{-2\pi i n} \\
&amp;= \sum_{n=0}^{N} f_{n} e^{-2\pi i(n/N)k} \\
&amp;= F_{k},
\end{aligned}
{% endkatex %}

where the second step follows from, {% katex %}e^{2\pi i n} = 1,\ \forall\ n{% endkatex %}.

For an arbitrary value of {% katex %}m{% endkatex %},
{% katex display %}
\begin{aligned}
F_{k+mN} &amp;= \sum_{n=0}^{N} f_{n}e^{-2\pi i (n/N)(k+mN)} \\
&amp;= \sum_{n=0}^{N} f_{n} e^{-2\pi i(n/N)k}e^{-2\pi i nm} \\
&amp;= \sum_{n=0}^{N} f_{n} e^{-2\pi i(n/N)k} \\
&amp;= F_{k},
\end{aligned}
{% endkatex %}

since, {% katex %}e^{2\pi i mn} = 1,\ \forall\ m,\ n{% endkatex %}.

### Consequence of Real {% katex %}f_{n}{% endkatex %}

If {% katex %}f_{n}{% endkatex %} is real then {% katex %}f_{n}=f_{n}^{\ast}{% endkatex %}, where
{% katex %}\ast{% endkatex %} denotes the [Complex Conjugate](https://en.wikipedia.org/wiki/Complex_conjugate).
It follows that,

{% katex display %}
\begin{aligned}
f_{n} &amp;= f_{n}^{\ast} \\
&amp;= \left\{ \frac{1}{N}\sum_{k=0}^{N-1}F_{k}e^{2\pi i (k/N)n} \right\}^{\ast} \\
&amp;= \frac{1}{N}\sum_{k=0}^{N-1}F_{k}^{\ast}e^{-2\pi i (k/N)n}
\end{aligned}\ \ \ \ \ (7)
{% endkatex %}

Another interesting and related result is,

{% katex display %}
F_{-k} = F_{k}^{\ast}\ \ \ \ \ (8).
{% endkatex %}

which follows from,

{% katex display %}
\begin{aligned}
F_{-k} &amp;= \sum_{n=0}^{N-1} f_{n}e^{2\pi i (n/N)k} \\
&amp;= \sum_{n=0}^{N-1} f_{n}^{\ast}e^{2\pi i (n/N)k} \\
&amp;= \left\{ \sum_{n=0}^{N-1} f_{n}e^{-2\pi i (n/N)k}\right\}^{\ast} \\
&amp;=F_{k}^{\ast}.
\end{aligned}
{% endkatex %}

## Orthogonality of Fourier Basis

The Discrete Fourier Basis is the collection of functions,

{% katex display %}
\left\{e^{2\pi i(k/N)n}\right\},
{% endkatex %}

 where {% katex %}n = 0,\ 1,\ 2,\ldots,N-1{% endkatex %}. It forms an [Orthogonal Basis](https://en.wikipedia.org/wiki/Orthogonal_basis) since,

{% katex display %}
\frac{1}{N} \sum_{n=0}^{N-1} e^{2\pi \left[ (m-k)/N\right] n}\ =\ \delta_{mk} =
\begin{cases}
1 &amp; \text{if}\ m\ =\ k \\
0 &amp; \text{if}\ m\ \ne\  k
\end{cases}\ \ \ \ \ (9),
{% endkatex %}

where {% katex %}\delta_{mk}{% endkatex %} is the [Kronecker Delta](https://en.wikipedia.org/wiki/Kronecker_delta).
This result can be proven for {% katex %}m\ \ne\ k{% endkatex %} by noting that the sum in
equation {% katex %}(9){% endkatex %} is a [Geometric Series](https://en.wikipedia.org/wiki/Geometric_series),

{% katex display %}
\frac{1}{N} \sum_{n=0}^{N-1} e^{2\pi i \left[(m-k)/N \right] n} = \frac{1}{N} \frac{1-e^{2\pi i (m-k)}}{1-e^{2\pi i (m-k)/N}}.
{% endkatex %}

Since {% katex %}2\pi i(m-k){% endkatex %} is
always a multiple of {% katex %}2\pi{% endkatex %} it follows that the numerator is zero,

{% katex display %}
1-e^{2\pi i (m-k)} = 1-1 = 0.
{% endkatex %}

The denominator is zero only if {% katex %}m-k=lN{% endkatex %} where {% katex %}l{% endkatex %} is
an integer. This cannot happen since {% katex %}-(N-1)\ \leq m-k\ \leq N-1{% endkatex %}, so,

{% katex display %}
\sum_{n=0}^{N-1} e^{2\pi \left[ (m-k)/N\right] n}\ =\ 0
{% endkatex %}

If {% katex %}m=k{% endkatex %} then,

{% katex display %}
\sum_{n=0}^{N-1} e^{2\pi i \left[(m-k)/N \right] n} = \sum_{n=0}^{N-1} 1 = N,
{% endkatex %}

this proves that equation {% katex %}(9){% endkatex %}.

## The Cross Correlation Theorem

The Cross Correlation Theorem is a relationship between the Fourier Transform of the cross correlation,
{% katex %}\psi_{t}{% endkatex %} defined by equation {% katex %}(1){% endkatex %} and the
Fourier Transforms of the two time series used in the cross correlation calculation,

{% katex display %}
\Psi_{k} = F_{k}^{\ast}G_{k}\ \ \ \ \ (10),
{% endkatex %}

where,

{% katex display %}
\begin{aligned}
\Psi_{k} &amp;= \sum_{n=0}^{N-1}\psi_{n}e^{-2\pi i (n/N)k} \\
F_{k}^{\ast} &amp;= \sum_{n=0}^{N-1}f_{n}^{\ast}e^{2\pi i (n/N)k} \\
G_{k} &amp;= \sum_{n=0}^{N-1}g_{n}e^{-2\pi i (n/N)k}.
\end{aligned}
{% endkatex %}

To derive equation {% katex %}(10){% endkatex %} consider the Inverse Fourier Transform of the time
series {% katex %}f_{n}{% endkatex %} and {% katex %}g_{n+t}{% endkatex %},

{% katex display %}
\begin{gathered}
f_{n} = \frac{1}{N}\sum_{k=0}^{N-1}F_{k}^{\ast}e^{-2\pi i (k/N)n} \\
g_{n+t} = \frac{1}{N}\sum_{k=0}^{N-1}G_{k}e^{2\pi i (k/N)(n+t)},
\end{gathered}
{% endkatex %}

Substituting these expressions into equation {% katex %}(1){% endkatex %} gives,

{% katex display %}
\begin{aligned}
\psi_t &amp;= \sum_{n=0}^{N-1} f_{n} g_{n+t} \\
&amp;= \sum_{n=0}^{N-1} \left\{   \frac{1}{N}\sum_{k=0}^{N-1}F_{k}^{\ast}e^{-2\pi i (k/N)n} \right\} \left\{ \frac{1}{N}\sum_{m=0}^{N-1}G_{m}e^{2\pi i (m/N)(n+t)} \right\} \\
&amp;= \frac{1}{N}\sum_{k=0}^{N-1}\sum_{m=0}^{N-1} F_{k}^{\ast}G_{m} e^{2\pi i (t/N)m} \frac{1}{N} \sum_{n=0}^{N-1} e^{2\pi i \left[(m-k)/N \right] n} \\
&amp;= \frac{1}{N}\sum_{k=0}^{N-1}\sum_{m=0}^{N-1} F_{k}^{\ast}G_{m} e^{2\pi i (t/N)m} \delta_{mk} \\
&amp;= \frac{1}{N}\sum_{k=0}^{N-1} F_{k}^{\ast}G_{k} e^{2\pi i (t/N)k},
\end{aligned}
{% endkatex %}

where the second step follows from equation {% katex %}(9){% endkatex %}. Equation {% katex %}(10){% endkatex %} follows by taking the Fourier Transform of the previous result,

{% katex display %}
\begin{aligned}
\Psi_{k} &amp;= \sum_{t=0}^{N-1}\psi_{t}e^{-2\pi i (t/N)k} \\
&amp;= \sum_{t=0}^{N-1} \left\{ \frac{1}{N}\sum_{k=0}^{N-1} F_{k}^{\ast}G_{k} e^{2\pi i (t/N)m} \right\}e^{-2\pi i (t/N)k} \\
&amp;= \sum_{m=0}^{N-1} F_{m}^{\ast}G_{m} \frac{1}{N} \sum_{t=0}^{N-1} e^{2\pi i \left[(m-k)/N)\right]t} \\
&amp;= \sum_{m=0}^{N-1} F_{m}^{\ast}G_{m} \delta_{mk} \\
&amp;= F_{k}^{\ast}G_{k},
\end{aligned}
{% endkatex %}

proving the Cross Correlation Theorem defined by equation {% katex %}(10){% endkatex %}.

## Discrete Fourier Transform Example

This section will work through an example calculation of a discrete Fourier Transform that can be worked
out by hand. The manual calculations will be compared with calculations performed using the FFT library
from `numpy`.

The Discrete Fourier Transform of a time series, represented by the column vector {% katex %}f{% endkatex %},
into a column vector of Fourier coefficients, {% katex %}\overline{f}{% endkatex %}, can be represented by the linear
equation,

{% katex display %}
\overline{f} = Tf,
{% endkatex %}

where {% katex %}T{% endkatex %} is the transform matrix computed from the Fourier basis functions.
Each element of the matrix is the value of the basis function used in the calculation.
It is assumed that the time series contains only {% katex %}4{% endkatex %} points so that
{% katex %}T{% endkatex %} will be a {% katex %}4\times 4{% endkatex %} matrix. The transform
matrix only depends on the number of elements in the time series vector and is given by,

{% katex display %}
T=
\begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; 1 \\
1 &amp; e^{-i\pi/2} &amp; e^{-i\pi} &amp; e^{-i3\pi/2} \\
1 &amp; e^{-i\pi} &amp; e^{-i2\pi} &amp; e^{-i3\pi} \\
1 &amp; e^{-i3\pi/2} &amp; e^{-i3\pi} &amp; e^{-i9\pi/2}
\end{pmatrix} =
\begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; 1 \\
1 &amp; -i &amp; -1 &amp; i \\
1 &amp; -1 &amp; 1 &amp; -1 \\
1 &amp; i &amp; -1 &amp; -i
\end{pmatrix} \ \ \ \ \ (11)
{% endkatex %}

Assume an example time series of,

{% katex display %}
f =
\begin{pmatrix}
8 \\
4 \\
8 \\
0
\end{pmatrix}.
{% endkatex %}

It follows that,

{% katex display %}
\begin{aligned}
\begin{pmatrix}
\overline{f_1} \\
\overline{f_2} \\
\overline{f_3} \\
\overline{f_4}
\end{pmatrix}
&amp;=
\begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; 1 \\
1 &amp; -i &amp; -1 &amp; i \\
1 &amp; -1 &amp; 1 &amp; -1 \\
1 &amp; i &amp; -1 &amp; -i
\end{pmatrix}
\begin{pmatrix}
8 \\
4 \\
8 \\
0
\end{pmatrix} \\
&amp;=
\begin{pmatrix}
20 \\
-4i \\
12 \\
4i
\end{pmatrix}
\end{aligned}\ \ \ \ \ (12)
{% endkatex %}

The Python code listing below uses the FFT implementation from `numpy` to confirm the calculation of
equation {% katex %}(12){% endkatex %}. It first defines the time series example data
{% katex %}f{% endkatex %}. The Fourier Transform is then used to compute
{% katex %}\overline{f}{% endkatex %}.

```python
In [1]: import numpy
In [2]: f = numpy.array([8, 4, 8, 0])
In [3]: numpy.fft.fft(f)
Out[3]: array([20.+0.j,  0.-4.j, 12.+0.j,  0.+4.j])
```
## Cross Correlation Theorem Example

This section will work through an example calculation of cross correlation using the Cross Correlation Theorem
with the goal of verifying an implementation of the algorithm in Python. Here use will be made of the
time series vector {% katex %}f{% endkatex %} and the transform matrix {% katex %}T{% endkatex %}
discussed in the previous section. An additional time series vector also needs to be considered, let,

{% katex display %}
g =
\begin{pmatrix}
6 \\
3 \\
9 \\
3
\end{pmatrix}.
{% endkatex %}

First, consider a direct calculation of the cross correlation,
{% katex %}\psi_t = \sum_{n=0}^{N-1} f_{n} g_{n+t}{% endkatex %}. The following python function
`cross_correlate_sum(x, y)` implements the required summation.

```python
In [1]: import numpy
In [2]: def cross_correlate_sum(x, y):
   ...:     n = len(x)
   ...:     correlation = numpy.zeros(len(x))
   ...:     for t in range(n):
   ...:         for k in range(0, n - t):
   ...:             correlation[t] += x[k] * y[k + t]
   ...:     return correlation
   ...:

In [3]: f = numpy.array([8, 4, 8, 0])
In [4]: g = numpy.array([6, 3, 9, 3])
In [5]: cross_correlate_sum(f, g)
Out[5]: array([132.,  84.,  84.,  24.])
```

`cross_correlate_sum(x, y)` takes two vectors, `x` and `y`, as arguments. It assumes that `x` and `y` are equal length and after allocating storage for the result performs the double summation required to compute the cross
correlation for all possible time lags, returning the result. It is also seen that
{% katex %}O(N^2){% endkatex %} operations are required to perform the calculation, where
{% katex %}N{% endkatex %} is the length of the input vectors.

Verification of following results requires a manual calculation. A method of organizing the calculation to facilitate  
this is shown in table below. The table rows are constructed from the elements of
{% katex %}f_{n}{% endkatex %} and the time lagged
elements of {% katex %}g_{n+t}{% endkatex %} for each value {% katex %}t{% endkatex %}.
The column is indexed by the element number
{% katex %}n{% endkatex %}. The time lag shift performed on the vector {% katex %}g_{n+t}{% endkatex %} results
in the translation of the components to the left that increases for each row as the time lag increases. Since the number of elements in
{% katex %}f{% endkatex %} and {% katex %}g{% endkatex %} is finite the time lag shift will lead to some
elements not participating in {% katex %}\psi_{t}{% endkatex %} for some time lag values. If there is no element
in the table at a position the value of {% katex %}f{% endkatex %} or {% katex %}g{% endkatex %} at that position
is assumed to be {% katex %}0{% endkatex %}.

| {% katex %}n{% endkatex %}      |     |     |     |  0  |  1  |  2  |  3 |
| :---: | :---:  | :---:  | :---: | :---:  | :---:  | :---: | :---: |
| {% katex %}f_{n}{% endkatex %}  |     |     |     |  8  |  4  |  8  |  0  |
| {% katex %}g_{n}{% endkatex %}  |     |     |     |  6  |  3  |  9  |  3  |
| {% katex %}g_{n+1}{% endkatex %}|     |     |  6  |  3  |  9  |  3  |     |
| {% katex %}g_{n+2}{% endkatex %}|     |  6  |  3  |  9  |  3  |     |     |
| {% katex %}g_{n+3}{% endkatex %}|  6  |  3  |  9  |  3  |     |     |     |

The cross correlation, {% katex %}\psi_t{% endkatex %}, for a value of the time lag,
{% katex %}t{% endkatex %}, is computed for each {% katex %}n{% endkatex %} by multiplication
of {% katex %}f_{n}{% endkatex %} and {% katex %}g_{n+t}{% endkatex %} and summing the results.
The outcome of this calculation is shown as the column vector
{% katex %}\psi{% endkatex %} below where each row corresponds to a different time lag value.

{% katex display %}
\psi =
\begin{pmatrix}
8\cdot 6 + 4\cdot 3 + 8\cdot 9 + 0\cdot 3 \\
8\cdot 3 + 4\cdot 9 + 8\cdot 3 \\
8\cdot 9 + 4\cdot 3 \\
8\cdot 3
\end{pmatrix}
=
\begin{pmatrix}
132 \\
84 \\
84 \\
24
\end{pmatrix}\ \ \ \ \ \ (13)
{% endkatex %}

The result is the same as determined by `cross_correlate_sum(x,y)`.

Now that an expectation of a result is established it can be compared with the
a calculation using using the Cross Correlation Theorem from equation {% katex %}(10){% endkatex %}
where {% katex %}f{% endkatex %} and {% katex %}g{% endkatex %} are represented by Discrete Fourier
Series. It was previously shown that the Fourier representation is periodic, see
equation {% katex %}(6){% endkatex %}. It follows that the time lag shifts of
{% katex %}g_{n+t}{% endkatex %} will by cyclic as shown in the calculation table below.

| {% katex %}n{% endkatex %}      |  0  |  1  |  2  |  3  |
| :---: | :---:  | :---:  | :---: | :---: |
| {% katex %}f_{n}{% endkatex %}  |  8  |  4  |  8  |  0  |
| {% katex %}g_{n}{% endkatex %}  |  6  |  3  |  9  |  3  |
| {% katex %}g_{n+1}{% endkatex %}|  3  |  9  |  3  |  6  |
| {% katex %}g_{n+2}{% endkatex %}|  9  |  3  |  6  |  3  |
| {% katex %}g_{n+3}{% endkatex %}|  3  |  6  |  3  |  9  |

Performing the calculation following the steps previously described has the following outcome,

{% katex display %}
\psi =
\begin{pmatrix}
8\cdot 6 + 4\cdot 3 + 8\cdot 9 + 0\cdot 3 \\
8\cdot 3 + 4\cdot 9 + 8\cdot 3 + 0\cdot 6 \\
8\cdot 9 + 4\cdot 3 + 8\cdot 6 + 0\cdot 3 \\
8\cdot 3 + 4\cdot 6 + 8\cdot 3 + 0\cdot 9
\end{pmatrix}
=
\begin{pmatrix}
132 \\
84 \\
132 \\
72
\end{pmatrix}.
{% endkatex %}

This is different from what was obtained from a direct evaluation of the cross correlation sum. Even though
the result is different the calculation could be correct since periodicity of {% katex %}f{% endkatex %}
and {% katex %}g{% endkatex %} was not assumed when the sum was evaluated. Below a calculation
using the Cross Correlation Theorem implemented in Python is shown.

```python
In [1]: import numpy
In [2]: f = numpy.array([8, 4, 8, 0])
In [3]: g = numpy.array([6, 3, 9, 3])
In [4]: f_bar = numpy.fft.fft(f)
In [5]: g_bar = numpy.fft.fft(g)
In [6]: numpy.fft.ifft(f_bar * g_bar)
Out[6]: array([132.+0.j,  72.+0.j, 132.+0.j,  84.+0.j])
```

In the calculation {% katex %}f{% endkatex %} and {% katex %}g{% endkatex %} are defined and their Fourier Transforms
are computed. The Cross Correlation Theorem is then used to compute the Fourier Transform of the cross correlation, which is then inverted. The result obtained is the same as obtained in the
manual calculation verifying the results. Since the calculations seem to be correct the problem must be that
periodicity of the Fourier representations {% katex %}f{% endkatex %} and {% katex %}g{% endkatex %} was
not handled properly. This analysis *artifact* is called [aliasing](https://en.wikipedia.org/wiki/Aliasing).
The following example attempts to correct for this problem.

The cross correlation calculation table for periodic {% katex %}f{% endkatex %} and
{% katex %}g{% endkatex %} can be made to resemble the table for the nonperiodic case by
padding the end of the vectors with {% katex %}N-1{% endkatex %} zeros, where {% katex %}N{% endkatex %} is
the vector length, creating a new periodic vector of length {% katex %}2N-1{% endkatex %}. This
new construction is shown below.

| {% katex %}n{% endkatex %}      |     |     |     |  0  |  1  |  2  |  3  |  4  |  5  |  6  |
| :---: | :---:  | :---:  | :---: | :---:  | :---:  | :---: | :---: | :---: | :---: | :---: | :---: |
| {% katex %}f_{n}{% endkatex %}  |     |     |     |  8  |  4  |  8  |  0  |     |     |     |
| {% katex %}g_{n}{% endkatex %}  |     |     |     |  6  |  3  |  9  |  3  |  0  |  0  |  0  |
| {% katex %}g_{n+1}{% endkatex %}|     |     |  6  |  3  |  9  |  3  |  0  |  0  |  0  |  6  |
| {% katex %}g_{n+2}{% endkatex %}|     |  6  |  3  |  9  |  3  |  0  |  0  |  0  |  6  |  3  |
| {% katex %}g_{n+3}{% endkatex %}|  6  |  3  |  9  |  3  |  0  |  0  |  0  |  6  |  3  |  9  |
| {% katex %}g_{n+4}{% endkatex %}|  3  |  9  |  3  |  0  |  0  |  0  |  6  |  3  |  9  |  3  |
| {% katex %}g_{n+5}{% endkatex %}|  9  |  3  |  0  |  0  |  0  |  6  |  3  |  9  |  3  |  0  |
| {% katex %}g_{n+6}{% endkatex %}|  3  |  0  |  0  |  0  |  6  |  3  |  9  |  3  |  0  |  0  |

It follows that,

{% katex display %}
\psi =
\begin{pmatrix}
8\cdot 6 + 4\cdot 3 + 8\cdot 9 + 3\cdot 0 \\
8\cdot 3 + 4\cdot 9 + 8\cdot 3 + 0\cdot 0 \\
8\cdot 9 + 4\cdot 3 + 8\cdot 0 + 0\cdot 0 \\
8\cdot 3 + 4\cdot 0 + 8\cdot 0 + 0\cdot 0 \\
8\cdot 0 + 4\cdot 0 + 8\cdot 0 + 0\cdot 3 \\
8\cdot 0 + 4\cdot 0 + 8\cdot 6 + 0\cdot 3 \\
8\cdot 0 + 4\cdot 6 + 8\cdot 3 + 0\cdot 9
\end{pmatrix}
=
\begin{pmatrix}
132 \\
84 \\
84 \\
24 \\
0 \\
48 \\
48
\end{pmatrix}
{% endkatex %}

The first {% katex %}N{% endkatex %} elements of this result are the same as obtained calculating the sum
directly. The same result is achieved by discarding the last {% katex %}N-1{% endkatex %} elements.
Verification is shown below where the previous example is extended by padding the tail of both
{% katex %}f{% endkatex %} and {% katex %}g{% endkatex %} with three zeros.

```python
In [1]: import numpy
In [2]: f = numpy.array([8, 4, 8, 0])
In [3]: g = numpy.array([6, 3, 9, 3])
In [4]: f_bar = numpy.fft.fft(numpy.concatenate((f, numpy.zeros(len(f)-1))))
In [5]: g_bar = numpy.fft.fft(numpy.concatenate((g, numpy.zeros(len(g)-1))))
In [6]: numpy.fft.ifft(numpy.conj(f_bar) * g_bar)
Out[7]:
array([1.3200000e+02+0.j, 8.4000000e+01+0.j, 8.4000000e+01+0.j,
       2.4000000e+01+0.j, 4.0602442e-15+0.j, 4.8000000e+01+0.j,
       4.8000000e+01+0.j])
```
The following function, `cross_correlate(x, y)`, generalizes the calculation to vectors of arbitrary but equal lengths.

```python
def cross_correlate(x, y):
    n = len(x)
    x_padded = numpy.concatenate((x, numpy.zeros(n-1)))
    y_padded = numpy.concatenate((y, numpy.zeros(n-1)))
    x_fft = numpy.fft.fft(x_padded)
    y_fft = numpy.fft.fft(y_padded)
    h_fft = numpy.conj(x_fft) * y_fft
    cc = numpy.fft.ifft(h_fft)
    return cc[0:n]
```

## Autocorrelation

The autocorrelation, defined by equation {% katex %}(3){% endkatex %}, is the special case of the
cross correlation of a time series with itself. It provides a measure of the *dependence* or
*similarity* of its past and future. The version of the Cross Correlation Theorem for autocorrelation is
given by,

{% katex display %}
R_{k} = F^{\ast}_{k}F_{k} = {\mid F_{k} \mid}^{2}\ \ \ \ \ (14).
{% endkatex %}

{% katex %}R_{k}{% endkatex %} is the *weight* of each of the coefficients in the Fourier Series
representation of the time series and is known as the [Power Spectrum](https://en.wikipedia.org/wiki/Spectral_density).

When discussing the autocorrelation of a time series, {% katex %}f{% endkatex %}, the autocorrelation coefficient is
useful,

{% katex display %}
\gamma_{\tau} = \frac{1}{\sigma_{f}^2}\sum_{n=0}^{t} \left(f_{n} - \mu_f \right) \left(f_{n+\tau} - \mu_f\right)\ \ \ \ \ (15),
{% endkatex %}

where {% katex %}\mu_{f}{% endkatex %} and {% katex %}\sigma_{f}{% endkatex %} are the time series mean and
standard deviation respectively. Below a Python implementation calculating the autocorrelation coefficient
is given.

```python
def autocorrelate(x):
    n = len(x)
    x_shifted = x - x.mean()
    x_padded = numpy.concatenate((x_shifted, numpy.zeros(n-1)))
    x_fft = numpy.fft.fft(x_padded)
    r_fft = numpy.conj(x_fft) * x_fft
    ac = numpy.fft.ifft(r_fft)
    return ac[0:n]/ac[0]
```

`autocorrelate(x)` takes a single argument, `x`, that is the time series used in the calculation.
The function first shifts
`x` by its mean, then adds padding to remove aliasing and computes its FFT. Equation {% katex %}(14){% endkatex %} is
next used to compute the Discrete Fourier Transform of the autocorrelation which is inverted. The final result is
normalized by the zero lag autocorrelation which equals {% katex %}\sigma_f^2{% endkatex %}.

### AR(1) Equilibrium Autocorrelation

The equilibrium properties of the AR(1) random process are discussed in some detail in
[Continuous State Markov Chain Equilibrium]({{ site.baseurl }}{% link _posts/2018-08-16-continuous_state_markov_chain_equilibrium.md %}).
AR(1) is defined by the difference equation,

{% katex display %}
X_{t} = \alpha X_{t-1} + \varepsilon_{t}\ \ \ \ \ (16),
{% endkatex %}

where {% katex %}t=0,\ 1,\ 2,\ldots{% endkatex %} and the {% katex %}\varepsilon_{t}{% endkatex %} are identically
distributed independent {% katex %}\textbf{Normal}(0,\ \sigma^2){% endkatex %} random variables.

The
equilibrium mean and standard deviation, {% katex %}\mu_E{% endkatex %} and {% katex %}\sigma_E{% endkatex %} are given by,

{% katex display %}
\begin{gathered}
\mu_{E} = 0 \\
\sigma_{E} = \frac{\sigma^2}{1 - \alpha^2}.
\end{gathered}\ \ \ \ \ (17)
{% endkatex %}

The equilibrium autocorrelation with time lag {% katex %}\tau{% endkatex %} is defined by,

{% katex display %}
r_{\tau}^{E} = \lim_{t\to\infty} E\left[X_t X_{t+\tau} \right]
{% endkatex %}

If equation {% katex %}(16){% endkatex %} is used to evaluate a few steps beyond an arbitrary time {% katex %}t{% endkatex %}
it is seen that,

{% katex display %}
\begin{aligned}
X_{t+1} &amp;= \alpha X_t + \varepsilon_{t+1} \\
X_{t+2} &amp;= \alpha X_{t+1} + \varepsilon_{t+2} \\
X_{t+3} &amp;= \alpha X_{t+2} + \varepsilon_{t+3}.
\end{aligned}
{% endkatex %}

Substituting the equation for {% katex %}t+1{% endkatex %} into the equation for {% katex %}t+2{% endkatex %} and that
result into the equation for {% katex %}t+3{% endkatex %} gives,

{% katex display %}
X_{t+3} = \alpha^{3} X_t + \sum_{n=1}^{3}\alpha^{n-1} \varepsilon_{t+n}.
{% endkatex %}

If this procedure is continued for {% katex %}\tau{% endkatex %} steps the following is obtained,

{% katex display %}
X_{t+\tau} = \alpha^{\tau} X_t + \sum_{n=1}^{\tau}\alpha^{n-1} \varepsilon_{t+n}.
{% endkatex %}

It follows that the autocorrelation is given by,

{% katex display %}
\begin{aligned}
r_{\tau} &amp;= E\left[X_t X_{t+\tau} \right] \\
&amp;=E\left[ X_{t}\left( \alpha^{\tau} X_t + \sum_{n=1}^{\tau}\alpha^{n-1} \varepsilon_{t+n} \right) \right] \\
&amp;= E\left[ \alpha^{\tau} X_t^2 + \sum_{n=1}^{\tau}\alpha^{n-1} X_t \varepsilon_{t+n}\right] \\
&amp;= \alpha^{\tau} E\left[X_t^2\right] + \sum_{n=1}^{\tau}\alpha^{n-1} E\left[ X_t \varepsilon_{t+n}\right].
\end{aligned}
{% endkatex %}

To go further the summation in the last step needs to be evaluated. In a
[previous post]({{ site.baseurl }}{% link _posts/2018-08-16-continuous_state_markov_chain_equilibrium.md %}) it was
shown that,

{% katex display %}
X_t = \alpha^t X_{0} + \sum_{i=1}^t \alpha^{t-i} \varepsilon_{i}.
{% endkatex %}

Using this result the summation term becomes,

{% katex display %}
\begin{aligned}
E\left[ X_t \varepsilon_{t+n}\right] &amp;= E\left[ \alpha^t X_{0}\varepsilon_{t+n} + \sum_{i=1}^t \alpha^{t-i} \varepsilon_{i}\varepsilon_{t+n} \right] \\
&amp;= \alpha^{t} X_0 E\left[ \varepsilon_{t+n} \right] + \sum_{i=1}^{t} \alpha^{t-i} E\left[ \varepsilon_{i} \varepsilon_{t+n} \right] \\
&amp;= 0,
\end{aligned}
{% endkatex %}

since the {% katex %}\varepsilon_{t}{% endkatex %} are independent and identically distributed random variables. It follows that,

{% katex display %}
\begin{aligned}
r_{\tau} &amp;= E\left[X_t X_{t+\tau} \right] \\
&amp;= \alpha^{\tau} E\left[X_t^2\right]
\end{aligned}
{% endkatex %}

Evaluation of the equilibrium limit gives,

{% katex display %}
\begin{aligned}
r_{\tau}^{E} &amp;= \lim_{t\to\infty} E\left[X_t X_{t+\tau} \right] \\
&amp;= \lim_{t\to\infty} \alpha^{\tau} E\left[X_t^2\right] \\
&amp;= \alpha^{\tau} \sigma_{E}^{2}.
\end{aligned}
{% endkatex %}

The last step follows from {% katex %}(17){% endkatex %} by assuming that
{% katex %}\mid\alpha\mid\ &lt; 1{% endkatex %} so that {% katex %}\mu_{E}{% endkatex %} and
{% katex %}\sigma_{E}{% endkatex %}
are finite. A simple form of the autocorrelation coefficient in the equilibrium limit
follows from equation {% katex %}(15){% endkatex %},

{% katex display %}
\begin{aligned}
\gamma_{\tau}^{E} &amp;= \frac{r_{\tau}}{\sigma^2_E} \\
&amp;= \alpha^{\tau}
\end{aligned}\ \ \ \ \ (18).
{% endkatex %}

{% katex %}\gamma_{\tau}^{E}{% endkatex %}
remains finite for increasing {% katex %}\tau{% endkatex %} only for {% katex %}\mid\alpha\mid\ \leq\ 1{% endkatex %}.

### AR(1) Simulations

In this section the results obtained for AR(1) equilibrium autocorrelation in the previous section will be compared with
simulations. Below an implementation in Python of an AR(1) simulator based on the difference equation representation from
equation {% katex %}(16){% endkatex %} is listed.

```python
def ar_1_series(α, σ, x0=0.0, nsamples=100):
    samples = numpy.zeros(nsamples)
    ε = numpy.random.normal(0.0, σ, nsamples)
    samples[0] = x0
    for i in range(1, nsamples):
        samples[i] = α * samples[i-1] + ε[i]
    return samples
```

The function `ar_1_series(α, σ, x0, nsamples)` takes four arguments: `α` and `σ` from equation {% katex %}(16){% endkatex %},
the initial value of {% katex %}x{% endkatex %}, `x0`, and the number of desired samples, `nsamples`. It begins by allocating storage
for the sample output followed by generation of `nsamples` values of {% katex %}\varepsilon \sim \textbf{Normal}(0,\ \sigma^2){% endkatex %}
with the requested standard deviation, {% katex %}\sigma{% endkatex %}.
The samples are then created using the AR(1 ) difference equation, equation {% katex %}(5){% endkatex %}.

The plots below show examples of simulated time series with {% katex %}\sigma=1{% endkatex %} and values of
{% katex %}\alpha{% endkatex %} satisfying {% katex %}\alpha\ &lt;\ 1{% endkatex %}.
It is seen that for smaller {% katex %}\alpha{% endkatex %} values the series more frequently change direction
and have smaller variance. This is expected from equation {% katex %}(17){% endkatex %}.

&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/discrete_cross_correlation_theorem/ar1_alpha_sample_comparison.png&quot;&gt;
&lt;/div&gt;

The next series of plots compare the autocorrelation coefficient from equation {% katex %}(18){% endkatex %}, obtained in
the equilibrium limit, with an autocorrelation coefficient calculation using the previously discussed `autocorrelate(x)` function on
the generated sample data. Recall that `autocorrelate(x)` performs a calculation using the Cross Correlation Theorem.
Equation {% katex %}(18){% endkatex %} does a good job of capturing the time scale for the series to become uncorrelated.

&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/discrete_cross_correlation_theorem/ar1_alpha_equilibrium_autocorrelation_comparison.png&quot;&gt;
&lt;/div&gt;

## Conclusions

The Discrete Cross Correlation Theorem provides a more efficient method of calculating time series
correlations than directly evaluating the sums. For a time series of length N it decreases the cost
of the calculation from {% katex %}O(N^2){% endkatex %} to {% katex %}O(NlogN){% endkatex %} by
use of the Fast Fourier Transform.
An interpretation of the cross correlation as the time lagged covariance of two random variables was presented
and followed by a discussion of the properties of Discrete Fourier Transforms needed to prove the
Cross Correlation Theorem. After building sufficient tools the theorem was derived by application of the
Discrete Fourier Transform to equation {% katex %}(1){% endkatex %}, which defines cross correlation.
An example manual calculation of a Discrete Fourier Transform was performed and compared with a calculation
using the FFT library form `numpy`. Next, manual calculations of cross correlation using a tabular method
to represent the summations were presented and followed by a calculation using the Discrete
Cross Correlation Theorem which illustrated the problem of aliasing. The next example calculation eliminated
aliasing and recovered a result equal to the direct calculation of the summations.
A *dealiased* implementation using `numpy` FFT libraries was then presented. Finally,
the Discrete Cross Correlation Theorem for the special case of the autocorrelation was
discussed and a `numpy` FFT implementation was provided and followed by an example calculation using the AR(1)
random process. In conclusion the autocorrelation coefficient in the equilibrium limit for AR(1) was evaluated
and shown to be finite only for values of the AR(1) parameter that satisfy
{% katex %}\mid\alpha\mid\ &lt; \ 1{% endkatex %}. This result is compared to direct simulations and shown to
provide a good estimate of the decorrelation time of the process.</content><author><name>Troy Stribling</name></author><summary type="html">The Cross Correlation Theorem is similar to the more widely known Convolution Theorem. The cross correlation of two discrete finite time series {f0, f1, f2,… fN−1}\{f_0,\ f_1,\ f_2,\ldots\,\ f_{N-1}\}{f0​, f1​, f2​,… fN−1​} and {g0, g1, g2,… gN−1}\{g_0,\ g_1,\ g_2,\ldots\,\ g_{N-1}\}{g0​, g1​, g2​,… gN−1​} is defined by, ψt=∑n=0N−1fngn+t     (1), \psi_t = \sum_{n=0}^{N-1} f_{n} g_{n+t}\ \ \ \ \ (1), ψt​=n=0∑N−1​fn​gn+t​     (1), where ttt is called the time lag. Cross correlation provides a measure of the similitude of two time series when shifted by the time lag. A direct calculation of the cross correlation using the equation above requires O(N2)O(N^2)O(N2) operations. The Cross Correlation Theorem provides a method for calculating cross correlation in O(NlogN)O(NlogN)O(NlogN) operations by use of the Fast Fourier Transform. Here the theoretical background required to understand cross correlation calculations using the Cross Correlation Theorem is discussed. Example calculations are performed and different implementations using the FFT libraries in numpy compared. The important special case of the cross correlation called Autocorrelation is addressed in the final section.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/posts/discrete_cross_correlation_theorem/ar1_alpha_equilibrium_autocorrelation_comparison.png" /></entry><entry><title type="html">Continuous State Markov Chain Equilibrium</title><link href="http://localhost:4000/2018/08/16/continuous_state_markov_chain_equilibrium.html" rel="alternate" type="text/html" title="Continuous State Markov Chain Equilibrium" /><published>2018-08-16T00:00:00-07:00</published><updated>2018-08-16T00:00:00-07:00</updated><id>http://localhost:4000/2018/08/16/continuous_state_markov_chain_equilibrium</id><content type="html" xml:base="http://localhost:4000/2018/08/16/continuous_state_markov_chain_equilibrium.html">A [Markov Chain](https://en.wikipedia.org/wiki/Markov_chain) is a sequence of states
where transitions between states occur ordered in time with
the probability of transition depending only on the previous state. Here the
states will be assumed a continuous unbounded set and time a discrete unbounded set. If the
set of states is given by, {% katex %}x\in\mathbb{R}{% endkatex %}, the probability that the process
will be in state {% katex %}x{% endkatex %} at time {% katex %}t{% endkatex %}, denoted by
{% katex %}\pi_t (y){% endkatex %}, is referred to as the distribution. Markov Chain equilibrium is
defined by {% katex %}\lim_{t\to\infty}\pi_t (y)\ &lt;\ \infty{% endkatex %}, that is, as time advances
{% katex %}\pi_t (y){% endkatex %} becomes independent of time. Here a solution
for this limit is discussed and illustrated with examples.

&lt;!--more--&gt;

## Model

The Markov Chain is constructed from the set of states {% katex %}\{x\}{% endkatex %},
ordered in time, where {% katex %}x\in\mathbb{R}{% endkatex %}.
The process starts at time {% katex %}t=0{% endkatex %} with state {% katex %}X_0=x{% endkatex %}.
At the next step, {% katex %}t=1{% endkatex %}, the process will assume a state
{% katex %}X_1=y{% endkatex %}, {% katex %}y\in\{x\}{% endkatex %},
with probability {% katex %}P(X_1=y|X_0=x){% endkatex %}
since it will depend on the state at {% katex %}t=0{% endkatex %} by the definition of a Markov Process.
At the next time step {% katex %}t=2{% endkatex %} the process state will be
{% katex %}X_2=z, \text{where}\ z\in\{x\}{% endkatex %} with probability,
{% katex display %}
P(X_2=z|X_0=x, X_1=y) = P(X_2=z|X_1=y),
{% endkatex %}
since by definition the probability of state transition depends only upon the state at the previous time step.
For an arbitrary time the transition from a state {% katex %}X_{t}=x{% endkatex %} to a state
{% katex %}X_{t+1}=y{% endkatex %} will occur with probability, {% katex %}P(X_{t+1}=y|X_t=x){% endkatex %}
that is independent of {% katex %}t{% endkatex %}.
The [Transition Kernel](https://en.wikipedia.org/wiki/Markov_kernel),
defined by,

{% katex display %}
p(x,y) = P(X_{t+1}=y|X_t=x),
{% endkatex %}

plays the same role as the transition matrix plays in the theory of
[Discrete State Markov Chains]({{ site.baseurl }}{% link _posts/2018-08-08-discrete_state_markov_chain_equilibrium.md %}). In general {% katex %}p(x,y){% endkatex %} cannot always be represented by a probability density function, but
here it is assumed that it has a density function representation.
This leads to the interpretation that for a known value of {% katex %}x{% endkatex %} {% katex %}p(x, y){% endkatex %} can be interpreted as a conditional probability density for a transition to state {% katex %}y{% endkatex %} given that the process is in state {% katex %}x{% endkatex %}, namely,

{% katex display %}
p(x, y) = f(y|x)
{% endkatex %}

Consequently, {% katex %}p(x,y){% endkatex %} is a family of conditional probability distributions each
representing conditioning on a possible state of the chain at time step {% katex %}t{% endkatex %}.
Since {% katex %}p(x, y){% endkatex %} is a conditional probability density for each value of
{% katex %}x{% endkatex %} it follows that,

{% katex display %}
\begin{gathered}
\int^{\infty}_{-\infty} p(x, y) dy = 1\ \forall\ x \\
p(x,y)\ \geq 0\ \forall\ x,\ y
\end{gathered}\ \ \ \ \ (1).
{% endkatex %}

The transition kernel for a single step in the Markov Process is defined by {% katex %}p(x,y){% endkatex %}. The
transition kernel across two time steps is computed as follows. Begin by denoting the process state at time
{% katex %}t{% endkatex %} by {% katex %}X_t=x{% endkatex %}, the state at {% katex %}t+1{% endkatex %} by
{% katex %}X_{t+1}=y{% endkatex %} and the state at {% katex %}t+2{% endkatex %} by
{% katex %}X_{t+2}=z{% endkatex %}. Then the probability of transitioning to a state
{% katex %}X_{t+3}=z{% endkatex %} from {% katex %}X_{t}=x{% endkatex %} in two steps,
{% katex %}p^2(x, z){% endkatex %}, is given by,
{% katex display %}
\begin{aligned}
f(z|x) &amp;= \int_{-\infty}^{\infty} f(z|x, y)f(y|x) dy \\
&amp;= \int_{-\infty}^{\infty} f(z|y)f(y|x) dy \\
&amp;= \int_{-\infty}^{\infty} p(y,z)p(x,y) dy \\
&amp;= \int_{-\infty}^{\infty} p(x,y)p(y,z) dy
\end{aligned}
{% endkatex %}

where use was made of the [Law of Total Probability](https://en.wikipedia.org/wiki/Law_of_total_probability), in the
first step and second step used the Markov Chain property {% katex %}f(z|x, y)=f(z|y){% endkatex %}.
Now, the result obtained for the two step transition kernel is the continuous version of
matrix multiplication of the single step transitions probabilities.
A operator inspired by matrix multiplication would be helpful. Make the definition,
{% katex display %}
P^2(x,z) = \int_{-\infty}^{\infty} p(x, y)p(y, z) dy.
{% endkatex %}

Using this operator, with {% katex %}X_{t+3}=w{% endkatex %}, the three step transition kernel is given by,
{% katex display %}
\begin{aligned}
f(w|x) &amp;= \int_{-\infty}^{\infty} f(w|x,z)f(z|x) dz \\
&amp;= \int_{-\infty}^{\infty} f(w|z) \left[ \int_{-\infty}^{\infty} f(z|y)f(y|x) dy \right] dz \\
&amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(w|z)f(z|y)f(y|x) dy dz \\
&amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p(z,w)p(y,z)p(x,y) dy dz \\
&amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p(x,y)p(y,z)p(z,w) dy dz \\
&amp;= P^3(x,w),
\end{aligned}
{% endkatex %}

where the second step substitutes the two step transition kernel and the remaining steps perform the decomposition to the single step transition kernel, {% katex %}p(x,y){% endkatex %}. Use of
[Mathematical Induction](https://en.wikipedia.org/wiki/Mathematical_induction) is used to show that the transition
kernel between two states in an arbitrary number of steps, {% katex %}t{% endkatex %}, is given by,

{% katex display %}
P^t(x,y) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty} p(x,z_1)p(z_1,z_2)\ldots p(z_{t-1},y) dz_1 dz_2\ldots dz_{t-1}.
{% endkatex %}

The distribution of Markov Chain states {% katex %}\pi (x){% endkatex %} is defined by,

{% katex display %}
\begin{gathered}
\int_{-\infty}^{\infty} \pi (x) dx = 1 \\
\pi (x) \geq 0\ \forall\ x
\end{gathered}
{% endkatex %}

To determine the equilibrium distribution, {% katex %}\pi_E (x){% endkatex %}, the time
variability of {% katex %}\pi_t (x){% endkatex %} must be determined. Begin by considering an arbitrary
distribution at {% katex %}t=0{% endkatex %}, {% katex %}\pi_0 (x){% endkatex %}. The distribution
after the first step will be,

{% katex display %}
\pi_1 (y) = \int_{-\infty}^{\infty} p(x, y)\pi_0 (x) dx.
{% endkatex %}

The distribution after two steps is,

{% katex display %}
\begin{aligned}
\pi_2 (y) &amp;= \int_{-\infty}^{\infty} p(x, y)\pi_1 (x) dx \\
&amp;= \int_{-\infty}^{\infty} p(x, y)\left[ \int_{-\infty}^{\infty} p(z, x)\pi_0 (z) dz \right] dx \\
&amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p(z,x) p(x,y) \pi_0 (z) dx dz
\end{aligned}
{% endkatex %}

The pattern becomes more apparent after the third step,

{% katex display %}
\begin{aligned}
\pi_3 (y) &amp;= \int_{-\infty}^{\infty} p(x, y)\pi_2 (x) dx \\
&amp;= \int_{-\infty}^{\infty} p(x, y)\left[ \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p(w,x) p(z,w)\pi_0 (z) dz dw \right] dx \\
&amp;= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p(x,y) p(w,x) p(z,w)\pi_0 (z) dz dw dx \\
&amp;= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p(z,w) p(w,x) p(x,y) \pi_0 (z) dw dx dz.
\end{aligned}
{% endkatex %}

This looks similar to the previous result obtained for the time dependence of the transition kernel. Making use
of the operator {% katex %}P{% endkatex %} gives,

{% katex display %}
\begin{aligned}
\pi_1 (y) &amp;= P\pi_0(y) = \int_{-\infty}^{\infty} p(x, y)\pi_0 (x) dx \\
\pi_2 (y) &amp;= P^2\pi_0(y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p(z,x) p(x,y) \pi_0 (z) dx dz \\
\pi_3 (y) &amp;= P^3\pi_0(y) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p(z,w) p(w,x) p(x,y) \pi_0 (z) dw dx dz.
\end{aligned}
{% endkatex %}

Mathematical Induction can be used to prove the distribution at an arbitrary time {% katex %}t{% endkatex %}
is given by,

{% katex display %}
\pi_t (y) = P^t\pi_0(y)\ \ \ \ \ (2)
{% endkatex %}

## Equilibrium Distribution

The equilibrium distribution is defined as the invariant solution to equation {% katex %}(2){% endkatex %}
for arbitrary {% katex %}t{% endkatex %}, namely,

{% katex display %}
\pi_E (y) = P^t\pi_E(y)\ \ \ \ \ (3).
{% endkatex %}

Consider,
{% katex display %}
\pi_E (y) = P\pi_E(y)\ \ \ \ \ (4),
{% endkatex %}

substitution into equation {% katex %}(3){% endkatex %} gives,

{% katex display %}
\begin{aligned}
\pi_E (y) &amp;= P^t\pi_E(y) \\
&amp;= P^{t-1}(P\pi_E)(y) \\
&amp;= P^{t-1}\pi_E(y) \\
&amp;\vdots \\
&amp;= P\pi_E(y) \\
&amp;= \pi_E(y).
\end{aligned}
{% endkatex %}

Thus equation {% katex %}(4){% endkatex %} defines the time invariant solution to equation
{% katex %}(3){% endkatex %}.

To go further a particular form for the transition kernel must be specified. Unlike the
[discrete state model]({{ site.baseurl }}{% link _posts/2018-08-08-discrete_state_markov_chain_equilibrium.md %})
a general solution cannot be obtained since convergence of the limit {% katex %}t\to\infty{% endkatex %} will
depend on the assumed transition kernel. The following section will describe a solution to equation
{% katex %}(4){% endkatex %} arising from a simple stochastic processes.

## Example

To evaluate the equilibrium distribution a form for the transition kernel must be assumed. Here the
[AR(1)](https://en.wikipedia.org/wiki/Autoregressive_model) stochastic process is considered.
AR(1) is a simple first order autoregressive model providing an example of a continuous state Markov Chain.
In following sections its equilibrium distribution is determined and the results of simulations are discussed.

AR(1) is defined by the difference equation,

{% katex display %}
X_{t} = \alpha X_{t-1} + \varepsilon_{t}\ \ \ \ \ (5),
{% endkatex %}

where {% katex %}t=0,\ 1,\ 2,\ldots{% endkatex %} and the {% katex %}\varepsilon_{t}{% endkatex %} are identically
distributed independent {% katex %}\textbf{Normal}{% endkatex %}
random variables with zero mean and variance, {% katex %}\sigma^2{% endkatex %}. The {% katex %}t{% endkatex %}
subscript on {% katex %}\varepsilon{% endkatex %} indicates that it is generated at time step
{% katex %}t{% endkatex %}.
The transition kernel for AR(1) can be derived from equation {% katex %}(5){% endkatex %} by using,
{% katex %}\varepsilon_{t} \sim \textbf{Normal}(0,\ \sigma^2){% endkatex %} and letting
{% katex %}x=x_{t-1}{% endkatex %} and {% katex %}y=x_{t}{% endkatex %} so that
{% katex %}\varepsilon_{t} = y - \alpha x{% endkatex %}. The result is,

{% katex display %}
p(x,y) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-(y-\alpha x)^2/2\sigma^2}\ \ \ \ \ \ (6)
{% endkatex %}

Now, consider a few steps,

{% katex display %}
\begin{aligned}
X_1 &amp;= \alpha X_0 + \varepsilon_1 \\
X_2 &amp;= \alpha X_1 + \varepsilon_2 \\
X_3 &amp;= \alpha X_2 + \varepsilon_3,
\end{aligned}
{% endkatex %}

substituting the first equation into the second equation and that result into the third equation gives,

{% katex display %}
X_3 = \alpha^3 X_0 + \alpha^2\varepsilon_1 + \alpha\varepsilon_2 + \varepsilon_3
{% endkatex %}

If this process is continued for {% katex %}t{% endkatex %} steps the following result is
obtained,

{% katex display %}
X_t = \alpha^t X_0 + \sum_{i=1}^{t} \alpha^{t-i} \varepsilon_{i}\ \ \ \ \ (7).
{% endkatex %}

### Equilibrium Solution

In this section equation {% katex %}(7){% endkatex %} is used to evaluate the the mean and variance of
the AR(1) process in the equilibrium limit {% katex %}t\to\infty{% endkatex %}. The mean and variance
obtained are then used to construct {% katex %}\pi_E(x){% endkatex %} that is shown to be a solution
to equation {% katex %}(4){% endkatex %}.

The equilibrium mean is given by,

{% katex display %}
  \mu_{E} = \lim_{t\to\infty} E(X_t).
{% endkatex %}

From equation {% katex %}(7){% endkatex %} it is seen that,

{% katex display %}
\begin{aligned}
E(X_t) &amp;= E\left[ \alpha^t X_0 + \sum_{i=1}^{t} \alpha^{t-i} \varepsilon_{i} \right] \\
&amp;= \alpha^t x_0 + \sum_{i=1}^t E(\varepsilon_i) \\
&amp;= \alpha^t x_0,
\end{aligned}
{% endkatex %}

where the last step follows from {% katex %}E(\varepsilon_i)=0{% endkatex %}. Now,

{% katex display %}
\begin{aligned}
\mu_E &amp;= \lim_{t\to\infty} E(X_t) \\
&amp;= \lim_{t\to\infty} \alpha^t X_0,
\end{aligned}
{% endkatex %}

this limit exists for {% katex %}\mid\alpha\mid\ \leq\ 1{% endkatex %},

{% katex display %}
\mu_E =
\begin{cases}
X_0 &amp; \mid\alpha\mid=1 \\
0 &amp; \mid\alpha\mid\ \leq\ 1
\end{cases}\ \ \ \ \ (8).
{% endkatex %}

The equilibrium variance is given by,

{% katex display %}
\sigma^2_E = \lim_{t\to\infty} E(X^2_t) - \left[E(X_t)\right]^2.
{% endkatex %}

To evaluate this expression and equation for {% katex %}X^2_t{% endkatex %} is needed. From equation
{% katex %}(7){% endkatex %} it follows that,

{% katex display %}
\begin{aligned}
X_t^2 &amp;= \left[ \alpha^t X_0 + \sum_{i=1}^{t} \alpha^{t-i} \varepsilon_i \right]^2 \\
&amp;= \alpha^{2t}X_0^2 + 2\alpha^t X_0\sum_{i=1}^t \alpha^{t-1} \varepsilon_i + \sum_{i=1}^t \sum_{j=1}^t \alpha^{t-i}\alpha^{t-j}\varepsilon_i \varepsilon_j.
\end{aligned}
{% endkatex %}

It follows that,

{% katex display %}
\begin{aligned}
E(X_t^2) &amp;= E\left[\alpha^{2t}X_0^2 + 2\alpha^t X_0\sum_{i=1}^t \alpha^{t-1} \varepsilon_i + \sum_{i=1}^t \sum_{j=1}^t \alpha^{t-i}\alpha^{t-j}\varepsilon_i \varepsilon_j \right] \\
&amp;= \alpha^{2t} X_0^2 +  2\alpha^t X_0\sum_{i=1}^t \alpha^{t-1} E(\varepsilon_i) + \sum_{i=1}^t \sum_{j=1}^t \alpha^{t-i}\alpha^{t-j}E(\varepsilon_i \varepsilon_j) \\
&amp;= \alpha^{2t} X_0^2 + \sum_{i=1}^t \sum_{j=1}^t \alpha^{t-i}\alpha^{t-j}E(\varepsilon_i \varepsilon_j),
\end{aligned}
{% endkatex %}

where the last step follows from {% katex %}E(\varepsilon_i)=0{% endkatex %}. Since the
{% katex %}\varepsilon_t{% endkatex %} are independent,

{% katex display %}
E(\varepsilon_i \varepsilon_j) = \sigma^2 \delta_{ij},
{% endkatex %}

where {% katex %}\delta_{ij}{% endkatex %} is the [Kronecker Delta](https://en.wikipedia.org/wiki/Kronecker_delta),
{% katex display %}
\delta_{ij} =
\begin{cases}
0 &amp; i \neq j \\
1 &amp; i=j
\end{cases}.
{% endkatex %}

Using these results gives,

{% katex display %}
\begin{aligned}
E(X_t^2) &amp;= \alpha^{2t} X_0^2 + \sigma^2\sum_{i=1}^t \sum_{j=1}^t \alpha^{2t-i-j}\delta_{ij} \\
&amp;= \alpha^{2t} X_0^2 + \sigma^2\sum_{i=1}^t \alpha^{2(t-i)} \\
&amp;= \alpha^{2t} X_0^2 + \sigma^2\sum_{i=1}^t \left(\alpha^2\right)^{t-i} \\
&amp;= \alpha^{2t} X_0^2 + \frac{\sigma^2\left[1 - (\alpha^2)^{t-1}\right]}{1 - \alpha^2},
\end{aligned}
{% endkatex %}

The last step follows from summation of a geometric series,

{% katex display %}
\begin{aligned}
\sum_{i=1}^{t} (\alpha^2)^{t-1} &amp;= \sum_{k=0}^{t-1}(\alpha^2)^k \\
&amp;= \frac{1-(\alpha^2)^{t-1}}{1-\alpha^2}.
\end{aligned}
{% endkatex %}

In the limit {% katex %}t\to\infty {% endkatex %} {% katex %}E(X_t^2){% endkatex %} only converges for {% katex %}\mid\alpha\mid\ &lt;\ 1{% endkatex %}. If {% katex %}\alpha=1{% endkatex %} the denominator of the second term in
of {% katex %}E(X_t^2){% endkatex %} is {% katex %}0{% endkatex %} {% katex %}E(X_t^2){% endkatex %} is
undefined. {% katex %}\sigma^2_E{% endkatex %}
is evaluated assuming {% katex %}\mid\alpha\mid\ &lt;\ 1{% endkatex %} if this is the case
{% katex %}(8){% endkatex %} gives {% katex %}\mu_E=0{% endkatex %}, so,

{% katex display %}
\begin{aligned}
\sigma^2_E &amp;= \lim_{t\to\infty} E(X_t^2) - \left(\mu_E\right)^2 \\
&amp;= \lim_{t\to\infty} \alpha^{2t}X_0^2 + \frac{\sigma^2\left[1 - (\alpha^2)^{t-1}\right]}{1 - \alpha^2} \\
&amp;= \frac{\sigma^2}{1 - \alpha^2}.
\end{aligned}\ \ \ \ \ (9)
{% endkatex %}

The equilibrium distribution, {% katex %}\pi_E{% endkatex %}, is found by substituting the results
from equation {% katex %}(8){% endkatex %} and {% katex %}(9){% endkatex %} into a
{% katex %}\textbf{Normal}(\mu_E,\ \sigma^2_E){% endkatex %} distribution to obtain,

{% katex display %}
\pi_E(y) = \frac{1}{\sqrt{2\pi\sigma_E^2}}e^{-y^2/2\sigma_E^2}\ \ \ \ \ (10)
{% endkatex %}

It can be shown that equation {% katex %}(10){% endkatex %} is the equilibrium distribution by verifying that it is a solution to
equation {% katex %}(4){% endkatex %}. Inserting equations {% katex %}(6){% endkatex %} and
{% katex %}(10){% endkatex %} into equation {% katex %}(4){% endkatex %} yields,

{% katex display %}
\begin{aligned}
P\pi_E(y) &amp;= \int_{-\infty}^{\infty} p(x, y) \pi_E(x) dx \\
&amp;= \int_{-\infty}^{\infty} \left[\frac{1}{\sqrt{2\pi\sigma^2}} e^{(y-\alpha x)^2/2\sigma^2}\right]\left[\frac{1}{\sqrt{2\pi\sigma_E^2}}e^{-y^2/2\sigma_E^2}\right] dx \\
&amp;= \frac{1}{\sqrt{2\pi\sigma^2}}\frac{1}{\sqrt{2\pi\sigma_E^2}} \int_{-\infty}^{\infty} e^{-\frac{1}{2}\left[(y-\alpha x)^2/\sigma^2+x^2/\sigma_E^2 \right]} dx \\
&amp;= \frac{1}{\sqrt{2\pi\sigma^2}}\frac{1}{\sqrt{2\pi\sigma_E^2}} \int_{-\infty}^{\infty} e^{-\frac{1}{2}\left[y^2/\sigma_E^2+(x-\alpha y)^2/\sigma^2 \right]} dx \\
&amp;= \frac{1}{\sqrt{2\pi\sigma_E^2}} e^{-y^2/2\sigma_E^2}\frac{1}{\sqrt{2\pi\sigma^2}}\int_{-\infty}^{\infty} e^{-(x-\alpha y)^2/2\sigma^2} dx \\
&amp;= \frac{1}{\sqrt{2\pi\sigma_E^2}} e^{-y^2/2\sigma_E^2} \\
&amp;= \pi_E(y)
\end{aligned}
{% endkatex %}

### Simulation

An AR(1) simulator can be implemented using either the difference equation definition, equation
{% katex %}(5){% endkatex %} or the transition kernel, equation {% katex %}(6){% endkatex %}.
The result produced by either are statistically identical, An example implementation in Python using
equation {% katex %}(5){% endkatex %} is shown below.

```python
def ar_1_difference_series(α, σ, x0, nsample=100):
    samples = numpy.zeros(nsample)
    ε = numpy.random.normal(0.0, σ, nsample)
    samples[0] = x0
    for i in range(1, nsample):
        samples[i] = α * samples[i-1] + ε[i]
    return samples
```

The function `ar_1_difference_series(α, σ, x0, nsamples)` takes four arguments: `α` and `σ`, the initial value
of {% katex %}x{% endkatex %}, `x0` and the number of desired samples `nsample`. It begins by allocating storage
for the sample output followed by generation of `nsample` values of {% katex %}\varepsilon_\sim \textbf{Normal}(0,\ \sigma^2){% endkatex %} with the requested standard deviation, {% katex %}\sigma{% endkatex %}. The samples are then created using the AR(1 ) difference equation, equation {% katex %}(5){% endkatex %}.
A second implementation using the transition kernel, equation {% katex %}(6){% endkatex %} is shown below.

```python
def ar1_kernel_series(α, σ, x0, nsample=100):
    samples = numpy.zeros(nsample)
    samples[0] = x0
    for i in range(1, nsample):
        samples[i] = numpy.random.normal(α * samples[i-1], σ)
    return samples
```

The function `ar1_kernel_series(α, σ, x0, nsamples)` also  takes four arguments: `α` and `σ`
from equation {% katex %}(6){% endkatex %},
the initial value of {% katex %}x{% endkatex %}, `x0` and the number of desired samples, `nsample`.
It begins by allocating storage for the sample output and then generates samples using the transition kernel
with distribution {% katex %}\textbf{Normal}(α * samples[i-1],\ \sigma^2){% endkatex %}.

The plots below show examples of time series generated
using `ar_1_difference_series` with {% katex %}\sigma=1{% endkatex %} and values of {% katex %}\alpha{% endkatex %}
satisfying {% katex %}\alpha\ &lt;\ 1{% endkatex %}. It is seen that for smaller {% katex %}\alpha{% endkatex %} values of
the series more frequently change direction and have smaller variance. This is expected from
equation {% katex %}(9){% endkatex %}, where {% katex %}\sigma_E=1/1-\alpha^2{% endkatex %}.

&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/continuous_state_markov_chain_equilibrium/ar1_alpha_sample_comparison.png&quot;&gt;
&lt;/div&gt;

If {% katex %}\alpha{% endkatex %} is just slightly larger than {% katex %}1{% endkatex %} the time series
can increase rapidly as illustrated in the plot below.

&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/continuous_state_markov_chain_equilibrium/ar1_alpha_larger_than_1.png&quot;&gt;
&lt;/div&gt;

For {% katex %}\alpha\ &lt;\ 1{% endkatex %} {% katex %}\sigma_E{% endkatex %} is bounded and the generated
samples are constrained about the equilibrium mean value, {% katex %}\mu_E=0{% endkatex %}, but for
{% katex %}\alpha\ \geq\ 1{% endkatex %} {% katex %}\sigma_E{% endkatex %} is unbounded
and the samples very quickly develop very large deviations from {% katex %}\mu_E{% endkatex %}.

### Convergence to Equilibrium

For sufficiently large times samples generated by the AR(1) process will approach the equilibrium values
for {% katex %}\alpha\ &lt;\ 1{% endkatex %}. In the plots following the cumulative values of both the
mean and standard deviation computed from simulations, using `ar_1_difference_series` with
{% katex %}\sigma=1{% endkatex %}, are compared with the equilibrium vales {% katex %}\mu_E{% endkatex %}
and {% katex %}\sigma_E{% endkatex %} from equations {% katex %}(8){% endkatex %} and
{% katex %}(9){% endkatex %} respectively. The first plot illustrates the convergence {% katex %}\mu{% endkatex %}
to {% katex %}\mu_E{% endkatex %} for six different simulations with varying initial states,
{% katex %}X_0{% endkatex %}, and {% katex %}\alpha{% endkatex %}. The rate of convergence is seen to
decrease as {% katex %}\alpha{% endkatex %} increases. For smaller {% katex %}\alpha{% endkatex %} the simulation
{% katex %}\mu{% endkatex %} is close to {% katex %}\mu_E{% endkatex %} within {% katex %}10^2{% endkatex %}
samples and indistinguishable from {% katex %}\mu_E{% endkatex %} by {% katex %}10^3{% endkatex %}. For larger
values of {% katex %}\alpha{% endkatex %} the convergence is mush slower. After {% katex %}10^5{% endkatex %}
samples there are still noticeable oscillations of the sampled {% katex %}\mu{% endkatex %} about
{% katex %}\mu_E{% endkatex %}.

&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img class=&quot;post-image&quot;  src=&quot;/assets/posts/continuous_state_markov_chain_equilibrium/mean_convergence.png&quot;&gt;
&lt;/div&gt;

Since {% katex %}\sigma_E{% endkatex %} varies with {% katex %}\alpha{% endkatex %} for clarity only simulations
with varying {% katex %}\alpha{% endkatex %} are shown. The rate of convergence {% katex %}\sigma{% endkatex %} to
{% katex %}\sigma_E{% endkatex %} is slightly slower than the rate seem for {% katex %}\mu{% endkatex %}. For
smaller {% katex %}\alpha{% endkatex %} simulation {% katex %}\sigma{% endkatex %} computations are
indistinguishable form {% katex %}\sigma_E{% endkatex %} by {% katex %}10^3{% endkatex %} samples. For larger
{% katex %}\alpha{% endkatex %} after {% katex %}10^4{% endkatex %} sample deviations about the
{% katex %}\sigma_E{% endkatex %} are still visible.

&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img class=&quot;post-image&quot;  src=&quot;/assets/posts/continuous_state_markov_chain_equilibrium/sigma_convergence.png&quot;&gt;
&lt;/div&gt;

The plot below favorably compares the histogram produced from results of a simulation of {% katex %}10^6{% endkatex %}
samples and the equilibrium distribution, {% katex %}\pi_E(y){% endkatex %}, from equation {% katex %}(10){% endkatex %}.

&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img class=&quot;post-image&quot;  src=&quot;/assets/posts/continuous_state_markov_chain_equilibrium/equilibrium_pdf_comparison_samples.png&quot;&gt;
&lt;/div&gt;

A more efficient method of estimating {% katex %}\pi_E(y){% endkatex %} is obtained from equation
{% katex %}(4){% endkatex %} by noting that for sufficiently large number of samples equation
{% katex %}(4){% endkatex %} can be approximated by the expectation of the transition kernel, namely,

{% katex display %}
\begin{aligned}
\pi_E(y) &amp;= P\pi_E(y) \\
&amp;= \int_{-\infty}^{\infty} p(x, y) \pi_E(x) dx \\
&amp;\approx \frac{1}{N} \sum_{i=0}^{N-1} p(x_i, y).
\end{aligned}\ \ \ \ \ (11)
{% endkatex %}

A Python implementation of the solution to equation {% katex %}(11){% endkatex %} for the average transition kernel
is listed below where two functions are defined.

```python
def ar_1_kernel(α, σ, x, y):
    p = numpy.zeros(len(y))
    for i in range(0, len(y)):
        ε  = ((y[i] -  α * x)**2) / (2.0 * σ**2)
        p[i] = numpy.exp(-ε) / numpy.sqrt(2 * numpy.pi * σ**2)
    return p

def ar_1_equilibrium_distributions(α, σ, x0, y, nsample=100):
    py = [ar_1_kernel(α, σ, x, y) for x in ar_1_difference_series(α, σ, x0, nsample)]
    pavg = [py[0]]
    for i in range(1, len(py)):
        pavg.append((py[i] + i * pavg[i-1]) / (i + 1))
    return pavg
```

The first function `ar_1_kernel(α, σ, x, y)` computes the transition kernel for a range of values and takes four arguments as
input: `α` and `σ` from equation {% katex %}(5){% endkatex %} and the value of `x` and an array of `y` values where
the transition kernel is evaluated. The second function `ar_1_equilibrium_distributions(α, σ, x0, y, nsample)` has five arguments as input:  
`α` and `σ`, the initial value of {% katex %}x{% endkatex %}, `x0`, the array of `y` values used to evaluate the transition kernel and
the number of desired samples `nsample`. `ar_1_equilibrium_distributions` begins by calling `ar_1_difference_series` to generate
`nsample` samples of `x`. These values and the needed inputs are then passed to `ar_1_kernel` providing `nsample` evaluations of
the transition kernel. The cumulative average of the transition kernel is then evaluated and returned.

In practice this method gives reasonable results for as few as {% katex %}10^2{% endkatex %} samples. This is illustrated
in the following plot where the transition kernel mean value computed with just {% katex %}50{% endkatex %} samples
using `ar_1_equilibrium_distributions` is compared {% katex %}\pi_E(y){% endkatex %} from equation
{% katex %}(10){% endkatex %}.

&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img class=&quot;post-image&quot;  src=&quot;/assets/posts/continuous_state_markov_chain_equilibrium/equilibrium_pdf_comparison.png&quot;&gt;
&lt;/div&gt;

The following plot shows intermediate values the calculation in the range of {% katex %}1{% endkatex %} to
{% katex %}50{% endkatex %} samples. This illustrates the changes in the estimated equilibrium distribution
as the calculation progresses. By {% katex %}500{% endkatex %} samples a distribution near the equilibrium distribution
is obtained.

&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/continuous_state_markov_chain_equilibrium/ar1_relaxation_to_equilibrium_2.png&quot;&gt;
&lt;/div&gt;

## Conclusions

Markov Chain equilibrium for continuous state processes provides a general theory of the time evolution
of stochastic kernels and distributions. Unlike the case for the [discrete state model]({{ site.baseurl }}{% link _posts/2018-08-08-discrete_state_markov_chain_equilibrium.md %}) general solutions cannot be obtained
since evaluation of the obtained equations depends of the form of the stochastic kernel. Kernels will
exist which do not have equilibrium solutions. A continuous state process that has an equilibrium
distribution that can be analytically evaluated is AR(1). The stochastic kernel for AR(1) is derived
from its difference equation representation and the first and second moments are evaluated in
the equilibrium limit, {% katex %}{t\to\infty}{% endkatex %}. It is shown that finite values exists only
for values of the AR(1) parameter that satisfy {% katex %}\mid\alpha\mid\ &lt; \ 1{% endkatex %}.
A distribution is then constructed using these moments and shown to be the equilibrium distribution.
Simulations are performed using the difference equation
representation of the process and compared with the equilibrium calculations. The rate of convergence
of simulations to the equilibrium is shown to depend on {% katex %}\alpha{% endkatex %}. For values
not near {% katex %}1{% endkatex %} convergence of the mean occurs with {% katex %}O(10^3){% endkatex %}
time steps and convergence of the standard deviation with {% katex %}O(10^4){% endkatex %} time steps.
For values of {% katex %}\alpha{% endkatex %} closer to {% katex %}1{% endkatex %} convergence has
not occurred by {% katex %}10^4{% endkatex %} time steps.</content><author><name>Troy Stribling</name></author><summary type="html">A Markov Chain is a sequence of states where transitions between states occur ordered in time with the probability of transition depending only on the previous state. Here the states will be assumed a continuous unbounded set and time a discrete unbounded set. If the set of states is given by, x∈Rx\in\mathbb{R}x∈R, the probability that the process will be in state xxx at time ttt, denoted by πt(y)\pi_t (y)πt​(y), is referred to as the distribution. Markov Chain equilibrium is defined by limt→∞πt(y) &amp;lt; ∞\lim_{t\to\infty}\pi_t (y)\ &amp;lt;\ \inftylimt→∞​πt​(y) &amp;lt; ∞, that is, as time advances πt(y)\pi_t (y)πt​(y) becomes independent of time. Here a solution for this limit is discussed and illustrated with examples.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/posts/continuous_state_markov_chain_equilibrium/ar1_relaxation_to_equilibrium_2.png" /></entry><entry><title type="html">Discrete State Markov Chain Equilibrium</title><link href="http://localhost:4000/2018/08/08/discrete_state_markov_chain_equilibrium.html" rel="alternate" type="text/html" title="Discrete State Markov Chain Equilibrium" /><published>2018-08-08T00:00:00-07:00</published><updated>2018-08-08T00:00:00-07:00</updated><id>http://localhost:4000/2018/08/08/discrete_state_markov_chain_equilibrium</id><content type="html" xml:base="http://localhost:4000/2018/08/08/discrete_state_markov_chain_equilibrium.html">A [Markov Chain](https://en.wikipedia.org/wiki/Markov_chain) is a sequence of states
where transitions between states occur ordered in time with
the probability of transition depending only on the previous state. Here the
states will be assumed a discrete finite set and time a discrete unbounded set. If the
set of states is given by {% katex %}\{x_1,\ x_2,\ldots,\ x_N\}{% endkatex %} the probability
that the process will be in state {% katex %}x_i{% endkatex %} at time {% katex %}t{% endkatex %}
is denoted by {% katex %}P(X_t=x_i){% endkatex %}, referred to as the distribution.
Markov Chain equilibrium is defined by {% katex %}\lim_{t\to\infty}P(X_t=x_i)\ &lt;\ \infty{% endkatex %},
that is, as time advances  
{% katex %}P(X_t=x_i){% endkatex %} becomes independent of time. Here a solution
for this limit is discussed and illustrated with examples.

&lt;!--more--&gt;

## Model

The Markov Chain model is constructed from the set of states
{% katex %}\{x_1,\ x_2,\ldots,\ x_N\}{% endkatex %} ordered in time.
The process starts at time {% katex %}t=0{% endkatex %} with state {% katex %}X_0=x_i{% endkatex %}.
At the next step, {% katex %}t=1{% endkatex %}, the process will assume a state
{% katex %}X_1=x_j{% endkatex %} with probability {% katex %}P(X_1=x_j|X_0=x_i){% endkatex %} since
it will depend on the previous state {% katex %}x_0{% endkatex %} as defined for a Markov Process.
At the next time step {% katex %}t=2{% endkatex %} the process state will be
{% katex %}X_2=x_k{% endkatex %} with probability,
{% katex display %}
P(X_2=x_k|X_0=x_i, X_1=x_j)=P(X_2=x_k|X_1=x_j),
{% endkatex %}
since by definition the probability of state transition depends only upon the state at the previous time step.
For an arbitrary time the transition from a state {% katex %}X_{t}=x_j{% endkatex %} to a state
{% katex %}X_{t+1}=x_j{% endkatex %} will occur with probability, {% katex %}P(X_{t+1}=x_j|X_t=x_i){% endkatex %}
that is independent of {% katex %}t{% endkatex %}.
Transition probabilities have the form of a matrix,

{% katex display %}
P_{ij} = P(X_{t+1}=x_j|X_t=x_i).
{% endkatex %}

{% katex %}P{% endkatex %} will be an {% katex %}N\times N{% endkatex %} matrix
where {% katex %}N{% endkatex %} is determined by the number of possible states. Each
row represents the Markov Chain transition probability from that state at
time {% katex %}t{% endkatex %} and the columns the values at {% katex %}t+1{% endkatex %}.
It follows that,
{% katex display %}
\begin{gathered}
\sum_{j=1}^{N}P_{ij} = 1\\
P_{ij}\ \geq\ 0
\end{gathered} \ \ \ \ \ (1).
{% endkatex %}

Equation {% katex %}(1){% endkatex %} is the definition of a [Stochastic Matrix](https://en.wikipedia.org/wiki/Stochastic_matrix).

The transition probability for a single step in the Markov Process is defined by {% katex %}P{% endkatex %}.
The transition probability across two time steps can be obtained with use of the
[Law of Total Probability](https://en.wikipedia.org/wiki/Law_of_total_probability),
{% katex display %}
\begin{aligned}
P(X_{t+2}=x_j|X_t=x_i) &amp;= \sum_{k=1}^{N} P(X_{t+2}=x_j | X_{t}=x_i, X_{t+1}=x_k)P(X_{t+1}=x_k | X_{t}=x_i) \\
&amp;= \sum_{k=1}^{N} P(X_{t+2}=x_j | X_{t+1}=x_k)P(X_{t+1}=x_k | X_{t}=x_i) \\
&amp;= \sum_{k=1}^{N} P_{kj}P_{ik} \\
&amp;= \sum_{k=1}^{N} P_{ik}P_{kj} \\
&amp;= {(P^2)}_{ij},
\end{aligned}
{% endkatex %}

where the last step follows from the definition of matrix multiplication. It is straight forward but
tedious to use [Mathematical Induction](https://en.wikipedia.org/wiki/Mathematical_induction) to extend the previous result to the case of an arbitrary time difference, {% katex %}\tau{% endkatex %},
{% katex display %}
P(X_{t+\tau}=x_j|X_t=x_i) = {(P^{\tau})}_{ij}\ \ \ \ \ (2).
{% endkatex %}

It should be noted that since {% katex %}{(P^{\tau})}_{ij}{% endkatex %} is a transition probability it must
satisfy,

{% katex display %}
\begin{gathered}
\sum_{j=1}^{N} {(P^{\tau})}_{ij}\ =\ 1 \\
{(P^{\tau})}_{ij}\ \geq\ 0
\end{gathered} \ \ \ \ \ (3).
{% endkatex %}

To determine the equilibrium solution of the distribution of states,
{% katex %}\{x_1,\ x_2,\ldots,\ x_N\}{% endkatex %}, the distribution time variability must be determined.
Begin by considering an arbitrary distribution at {% katex %}t=0{% endkatex %}, which can be written as a
column vector,
{% katex display %}
\pi =
\begin{pmatrix}
\pi_1 \\
\pi_2 \\
\vdots \\
\pi_N
\end{pmatrix} =
\begin{pmatrix}
P(X_0=x_1) \\
P(X_0=x_2) \\
\vdots \\
P(X_0=x_N)
\end{pmatrix},
{% endkatex %}

since it is a probability distribution {% katex %}\pi_i{% endkatex %} must satisfy,
{% katex display %}
\begin{gathered}
\sum_{i=1}^{N} \pi_i\ =\ 1\\
\pi_i\ \geq \ 0
\end{gathered} \ \ \ \ \ (4).
{% endkatex %}

The distribution after the first step is given by,
{% katex display %}
\begin{aligned}
P(X_1=x_j) &amp;= \sum_{i=1}^{N} P(X_1=x_j|X_0=x_i)P(X_0=x_i) \\
&amp;= \sum_{i=1}^{N} P_{ij}\pi_{i} \\
&amp;= \sum_{i=1}^{N} \pi_{i}P_{ij} \\
&amp;= {(\pi^{T}P)}_{j},
\end{aligned}
{% endkatex %}

where {% katex %}\pi^T{% endkatex %} is the transpose of {% katex %}\pi{% endkatex %}. Similarly, the distribution after the second step is,
{% katex display %}
\begin{aligned}
P(X_2=x_j) &amp;= \sum_{i=1}^{N} P(X_2=x_j|X_1=x_i)P(X_1=x_i) \\
&amp;= \sum_{i=1}^{N} P_{ij}{(\pi^{T}P)}_{i} \\
&amp;= \sum_{i=1}^{N} P_{ij}\sum_{k=1}^{N} \pi_{k}P_{ki} \\
&amp;= \sum_{k=1}^{N} \pi_{k} \sum_{i=1}^{N} P_{ij}P_{ki} \\
&amp;= \sum_{k=1}^{N} \pi_{k} \sum_{i=1}^{N} P_{ki}P_{ij} \\
&amp;= \sum_{k=1}^{N} \pi_{k} {(P^2)}_{kj} \\
&amp;= {(\pi^{T}P^2)}_{j},
\end{aligned}
{% endkatex %}

A pattern is clearly developing. Mathematical Induction can be used to prove the distribution
at an arbitrary time {% katex %}t{% endkatex %} is given by,
{% katex display %}
P(X_t=x_j) = {(\pi^{T}P^t)}_{j}
{% endkatex %}

or as a column vector,

{% katex display %}
\pi_{t}^{T} = \pi^{T}P^t\ \ \ \ \ (5).
{% endkatex %}

Where {% katex %}\pi{% endkatex %} and {% katex %}\pi_t{% endkatex %} are the initial distribution
and the distribution after {% katex %}t{% endkatex %} steps respectively.

## Equilibrium Transition Matrix

The probability of transitioning between two states {% katex %}x_i{% endkatex %} and
{% katex %}x_j{% endkatex %} in {% katex %}t{% endkatex %} time steps was previously shown to be
stochastic matrix {% katex %}P^t{% endkatex %} constrained by equation {% katex %}(3){% endkatex %}.
The equilibrium transition matrix is defined by,
{% katex display %}
P^{E} = \lim_{t\to\infty}P^{t}.
{% endkatex %}
This limit can be determined using
[Matrix Diagonalization](https://en.wikipedia.org/wiki/Diagonalizable_matrix).
The following sections will use diagonalization to construct a form of
{% katex %}P^{t}{% endkatex %} that will easily allow evaluation equilibrium limit.

### Eigenvectors and Eigenvalues of the Transition Matrix

Matrix Diagonalization requires evaluation of eigenvalues and eigenvectors, which are defined
by the solutions to the equation,
{% katex display %}
Pv = \lambda v\ \ \ \ \ (6),
{% endkatex %}
where {% katex %}v{% endkatex %} is the eigenvector and {% katex %}\lambda{% endkatex %} eigenvalue.
From equation {% katex %}(6){% endkatex %} it follows,
{% katex display %}
\begin{aligned}
P^{t}v &amp;= P^{t-1}(Pv)\\
&amp;=P^{t-1}\lambda v\\
&amp;=P^{t-2}(Pv)\lambda\\
&amp;=P^{t-2}\lambda^{2}v \\
&amp;\vdots\\
&amp;=(Pv)\lambda^{t-1}\\
&amp;=\lambda^{t}v.
\end{aligned}
{% endkatex %}

Since {% katex %}P^t{% endkatex %} is a stochastic matrix it satisfies equation {% katex %}(3){% endkatex %}.
As a result of these constraints the limit {% katex %}t\to\infty{% endkatex %} requires,
{% katex display %}
\lim_{t\to\infty}P^{t}=\lim_{t\to\infty}\lambda^{t}v\ \leq\ \infty.
{% endkatex %}

It follows that {% katex %}\lambda\ \leq\ 1 {% endkatex %}. Next,
it will be shown that {% katex %}\lambda_1=1{% endkatex %} and that a column vector of
{% katex %}1's{% endkatex %} with {% katex %}N{% endkatex %} rows,
{% katex display %}
V_1 =
\begin{pmatrix}
1 \\
1 \\
\vdots \\
1
\end{pmatrix}\ \ \ \ \ (7),
{% endkatex %}
are eigenvalue and eigenvector solutions of equation {% katex %}(6){% endkatex %},
{% katex display %}
\begin{pmatrix}
P_{11} &amp; P_{12} &amp; \cdots &amp; P_{1N} \\
P_{21} &amp; P_{22} &amp; \cdots &amp; P_{2N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
P_{N1} &amp; P_{N2} &amp; \cdots &amp; P_{NN}
\end{pmatrix}
\begin{pmatrix}
1 \\
1 \\
\vdots \\
1
\end{pmatrix}
=
\begin{pmatrix}
\sum_{j=1}^{N}P_{1j} \\
\sum_{j=1}^{N}P_{2j} \\
\vdots \\
\sum_{j=1}^{N}P_{Nj}
\end{pmatrix}
=
\begin{pmatrix}
1 \\
1 \\
\vdots \\
1
\end{pmatrix}
=\lambda_1 V_1,
{% endkatex %}

where use was made of the stochastic matrix condition from equation {% katex %}(1){% endkatex %}, namely,
{% katex %}\sum_{j=1}^{N}P_{ij}=1{% endkatex %}.

To go further a result from the [Perron-Frobenius Theorem](https://en.wikipedia.org/wiki/Perron–Frobenius_theorem) is needed.
This theorem states that a stochastic matrix will have a largest eigenvalue with multiplicity 1. Here all eigenvalues will satisfy
{% katex %}\lambda_1=1 \ &gt;\ \mid{\lambda_i}\mid,\ \forall\ 1\ &lt;\ i\ \leq\ N{% endkatex %}.

Denote the eigenvector {% katex %}V_j{% endkatex %} by the column vector,
{% katex display %}
V_j =
\begin{pmatrix}
v_{1j} \\
v_{2j} \\
\vdots \\
v_{Nj}
\end{pmatrix},
{% endkatex %}

and let {% katex %}V{% endkatex %} be the matrix with columns that are the eigenvectors of
{% katex %}P{% endkatex %} with {% katex %}V_1{% endkatex %} from equation
{% katex %}(7){% endkatex %} in the first column,
{% katex display %}
V=
\begin{pmatrix}
1 &amp; v_{12} &amp; \cdots &amp; v_{1N} \\
1 &amp; v_{22} &amp; \cdots &amp; v_{2N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; v_{N2} &amp; \cdots &amp; v_{NN}
\end{pmatrix}.
{% endkatex %}

Assume that {% katex %}V{% endkatex %} is invertible and denote the inverse by,

{% katex display %}
V^{-1}=
\begin{pmatrix}
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N} \\
v^{-1}_{21} &amp; v^{-1}_{22} &amp; \cdots &amp; v^{-1}_{2N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
v^{-1}_{N1} &amp; v^{-1}_{N2} &amp; \cdots &amp; v^{-1}_{NN}
\end{pmatrix}\ \ \ \ \ (8).
{% endkatex %}

If the identity matrix is represented by {% katex %}I{% endkatex %} then {% katex %}VV^{-1} = I{% endkatex %}.
Let {% katex %}\Lambda{% endkatex %} be a diagonal matrix constructed from the eigenvalues of
{% katex %}P{% endkatex %} using {% katex %}\lambda_1=1{% endkatex %},

{% katex display %}
\Lambda =
\begin{pmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_N
\end{pmatrix}.
{% endkatex %}

Sufficient information about the eigenvalues and eigenvectors has been obtained to prove some very
general results for Markov Chains.
The following section will work through the equilibrium limit using the using these results
to construct a diagonalized representation of the matrix.

### Diagonalization of Transition Matrix

Using the results obtained in the previous section the diagonalized
form of the transition matrix is given by,

{% katex display %}
P = V\Lambda V^{-1},
{% endkatex %}

Using this representation of {% katex %}P{% endkatex %} an expression for {% katex %}P^t{% endkatex %}
is obtained,
{% katex display %}
\begin{aligned}
P^{t} &amp;= P^{t-1}V\Lambda V^{-1} \\
&amp;= P^{t-2}V\Lambda V^{-1}V\Lambda V^{-1} \\
&amp;= P^{t-2}V\Lambda^2 V^{-1}\\
&amp;\vdots \\
&amp;= PV\Lambda^{t-1} V^{-1} \\
&amp;= V\Lambda^{t}V^{-1}
\end{aligned}
{% endkatex %}

Evaluation of {% katex %}\Lambda^{t}{% endkatex %} is straight forward,

{% katex display %}
\Lambda^t =
\begin{pmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2^t &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_N^t
\end{pmatrix}.
{% endkatex %}

Since {% katex %}\mid{\lambda_i}\mid\ &lt;\ 1,\ \forall\ 1\ &lt;\ i\ \leq\ N{% endkatex %} in the limit
{% katex %}t\to\infty{% endkatex %} it is seen that,
{% katex display %}
\Lambda^{E} =
\lim_{t\to\infty} \Lambda^t =
\lim_{t\to\infty}
\begin{pmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2^t &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_N^t
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 0
\end{pmatrix},
{% endkatex %}

so,
{% katex display %}
P^{E} = \lim_{t\to\infty} P^{t} = \lim_{t\to\infty} V\Lambda^{t} V^{-1} = V\Lambda^{E}V^{-1}.
{% endkatex %}

Evaluation of the first two terms on the righthand side gives,

{% katex display %}
V\Lambda^{E} =
\begin{pmatrix}
1 &amp; v_{12} &amp; \cdots &amp; v_{1N} \\
1 &amp; v_{22} &amp; \cdots &amp; v_{2N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; v_{N2} &amp; \cdots &amp; v_{NN}
\end{pmatrix}
\begin{pmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 0
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
1 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; 0 &amp; \cdots &amp; 0
\end{pmatrix}.
{% endkatex %}

It follows that,
{% katex display %}
V\Lambda^{E} V^{-1} =
\begin{pmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
1 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; 0 &amp; \cdots &amp; 0
\end{pmatrix}
\begin{pmatrix}
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N} \\
v^{-1}_{21} &amp; v^{-1}_{22} &amp; \cdots &amp; v^{-1}_{2N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
v^{-1}_{N1} &amp; v^{-1}_{N2} &amp; \cdots &amp; v^{-1}_{NN}
\end{pmatrix}
=
\begin{pmatrix}
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N} \\
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N}
\end{pmatrix}.
{% endkatex %}

Finally, the equilibrium transition matrix is given by,
{% katex display %}
P^{E} = V\Lambda^{E} V^{-1} =
\begin{pmatrix}
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N} \\
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N}
\end{pmatrix}\ \ \ \ \ (9).
{% endkatex %}

The rows of {% katex %}P^{E}{% endkatex %} are identical and given by the
first row of the inverse of the matrix of eigenvectors, {% katex %}V^{-1}{% endkatex %},
from equation {% katex %}(8){% endkatex %}. This row is a consequence of the location of the
{% katex %}\lambda=1{% endkatex %} eigenvalue and eigenvector in {% katex %}\Lambda{% endkatex %} and {% katex %}V{% endkatex %} respectively.
Since {% katex %}P^{E}{% endkatex %} is a transition matrix,

{% katex display %}
\begin{gathered}
\sum_{j=1}^{N} v_{ij}^{-1}\ =\ 1\\
v_{ij}^{-1}\ \geq \ 0.
\end{gathered}
{% endkatex %}

## Equilibrium Distribution

The equilibrium distribution is defined by a solution to equation {% katex %}(5){% endkatex %}
that is independent of time.

{% katex display %}
\pi^{T} = \pi^{T}P^t\ \ \ \ \ (10).
{% endkatex %}

Consider a distribution {% katex %}\pi_{E}{% endkatex %} that satisfies,
{% katex display %}
\pi_{E}^{T} = \pi_{E}^{T}P\ \ \ \ \ (11).
{% endkatex %}

It is easy to show that {% katex %}\pi_{E}{% endkatex %} is an equilibrium solution
by substituting it into equation {% katex %}(10){% endkatex %}.
{% katex display %}
\begin{aligned}
\pi_{E}^{T} &amp;= \pi_{E}^{T}P^t \\
&amp;= (\pi_{E}^{T}P)P^{t-1} \\
&amp;= \pi_{E}^{T}P^{t-1} \\
&amp;= \pi_{E}^{T}P^{t-2} \\
&amp;\vdots \\
&amp;= \pi_{E}^{T}P \\
&amp;= \pi_{E}^{T}.
\end{aligned}
{% endkatex %}

### Relationship Between Equilibrium Distribution and Transition Matrix

To determine the relationship between {% katex %}\pi_E{% endkatex %} and {% katex %}P^{E}{% endkatex %},
begin by considering an arbitrary initial distribution states
with {% katex %}N{% endkatex %} elements,
{% katex display %}
\pi =
\begin{pmatrix}
\pi_{1} \\
\pi_{2} \\
\vdots \\
\pi_{N}
\end{pmatrix},
{% endkatex %}

where,

{% katex display %}
\begin{gathered}
\sum_{j=1}^{N}\pi_{j} = 1\\
\pi_{j}\ \geq\ 0
\end{gathered}.
{% endkatex %}

The distribution when the Markov Chain has had sufficient time to reach equilibrium will be given by,
{% katex display %}
\begin{aligned}
\pi^{T}P^{E} &amp;=
\begin{pmatrix}
\pi_{1} &amp; \pi_{2} &amp; \cdots &amp; \pi_{N}
\end{pmatrix}
\begin{pmatrix}
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N} \\
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N}
\end{pmatrix} \\
&amp;=
\begin{pmatrix}
v^{-1}_{11}\sum_{j=1}^{N} \pi_{j} &amp;
v^{-1}_{12}\sum_{j=1}^{N} \pi_{j} &amp;
\cdots &amp;
v^{-1}_{1N}\sum_{j=1}^{N} \pi_{j}
\end{pmatrix},
\end{aligned}
{% endkatex %}

since, {% katex %}\sum_{j=1}^{N} \pi_{j} = 1{% endkatex %},

{% katex display %}
\pi^{T}P^{E} =
\begin{pmatrix}
v^{-1}_{11} &amp; v^{-1}_{12} &amp; \cdots &amp; v^{-1}_{1N}
\end{pmatrix}.
{% endkatex %}

Thus any initial distribution {% katex %}\pi{% endkatex %} will after
sufficient time approach the distribution above. It follows that it will be the solution
of equation {% katex %}(11){% endkatex %} which defines the equilibrium distribution,

{% katex display %}
\pi_E =
\begin{pmatrix}
v^{-1}_{11} \\
v^{-1}_{12} \\
\vdots \\
v^{-1}_{1N}
\end{pmatrix}.
{% endkatex %}

### Solution of Equilibrium Equation

An equation for the equilibrium distribution, {% katex %}\pi_{E}{% endkatex %}, can
be obtained from equation {% katex %}(11){% endkatex %},
{% katex display %}
\pi^{T}_E\left(P - I\right) = 0\ \ \ \ \ (12),
{% endkatex %}
where {% katex %}I{% endkatex %} is the identity matrix. Equation {% katex %}(12){% endkatex %} alone
is insufficient to obtain a unique solution since the system of linear equations it defines is
[Linearly Dependent](https://en.wikipedia.org/wiki/Linear_independence). In a linearly dependent
system of equations some equations are the result of linear operations on the others.
It is straight forward to show that one of the equations defined by {% katex %}(12){% endkatex %} can
be eliminated by summing the other equations and multiplying by {% katex %}-1{% endkatex %}.
If the equations were
linearly independent the only solution would be the trivial zero solution,
{% katex %}{\left( \pi^{T}_E \right)}_{i}\ =\ 0,\ \forall\ i{% endkatex %}. A unique solution to
{% katex %}(12){% endkatex %} is obtained by including the normalization constraint,
{% katex display %}
\sum_{j=1}^{N} {\left( \pi_{E}^{T} \right)}_{j} = 1.
{% endkatex %}
The resulting system of equations to be solved is given by,
{% katex display %}
\pi_{E}^{T}
\begin{pmatrix}
{P_{11} - 1} &amp; P_{12} &amp; \cdots &amp; P_{1N} &amp; 1 \\
P_{21} &amp; {P_{22} -1} &amp; \cdots &amp; P_{2N} &amp; 1 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots\\
P_{N1} &amp; P_{N2} &amp; \cdots &amp; {P_{NN} - 1} &amp; 1 \\
\end{pmatrix}
=
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}\ \ \ \ \ (13).
{% endkatex %}

## Example

Consider the Markov Chain defined by the transition matrix,
{% katex display %}
P =
\begin{pmatrix}
0.0 &amp; 0.9 &amp; 0.1 &amp; 0.0 \\
0.8 &amp; 0.1 &amp; 0.0 &amp; 0.1 \\
0.0 &amp; 0.5 &amp; 0.3 &amp; 0.2 \\
0.1 &amp; 0.0 &amp; 0.0 &amp; 0.9
\end{pmatrix}\ \ \ \ \ (14).
{% endkatex %}
The state transition diagram below provides a graphical representation of {% katex %}P{% endkatex %}.
&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img class=&quot;post-image&quot;  src=&quot;/assets/posts/discrete_state_markov_chain_equilibrium/transition_diagram.png&quot;&gt;
&lt;/div&gt;

### Convergence to Equilibrium

Relaxation of both the transition matrix and distribution to their equilibrium values
is easily demonstrated with the few lines of Python executed within `ipython`.

```shell
In [1]: import numpy
In [2]: t = [[0.0, 0.9, 0.1, 0.0],
   ...:      [0.8, 0.1, 0.0, 0.1],
   ...:      [0.0, 0.5, 0.3, 0.2],
   ...:      [0.1, 0.0, 0.0, 0.9]]
In [3]: p = numpy.matrix(t)
In [4]: p**100
Out[4]:
matrix([[0.27876106, 0.30088496, 0.03982301, 0.38053097],
        [0.27876106, 0.30088496, 0.03982301, 0.38053097],
        [0.27876106, 0.30088496, 0.03982301, 0.38053097],
        [0.27876106, 0.30088495, 0.03982301, 0.38053098]])
```

Here the transition matrix from the initial state to states {% katex %}100{% endkatex %} time steps
in the future is computed using equation {% katex %}(2){% endkatex %}. The result obtained
has identical rows as obtained in equation {% katex %}(9){% endkatex %}.
{% katex display %}
P^{100} =
\begin{pmatrix}
0.27876106 &amp; 0.30088496 &amp; 0.03982301 &amp; 0.38053097 \\
0.27876106 &amp; 0.30088496 &amp; 0.03982301 &amp; 0.38053097 \\
0.27876106 &amp; 0.30088496 &amp; 0.03982301 &amp; 0.38053097 \\
0.27876106 &amp; 0.30088495 &amp; 0.03982301 &amp; 0.38053098
\end{pmatrix}\ \ \ \ \ (15).
{% endkatex %}

For an initial distribution {% katex %}\pi{% endkatex %} the distribution after {% katex %}100{% endkatex %}
time steps is evaluated using,

```shell
In [5]: c = [[0.1],
   ...:      [0.5],
   ...:      [0.35],
   ...:      [0.05]]
In [6]: π = numpy.matrix(c)
In [8]: π.T*p**100
Out[8]: matrix([[0.27876106, 0.30088496, 0.03982301, 0.38053097]])
```

Here an initial distribution is constructed that satisfies
{% katex %}\sum_{i=0}^3 \pi_i = 1{% endkatex %}. Then equation {% katex %}(5){% endkatex %} is used
to compute the distribution after {% katex %}100{% endkatex %} time steps.
The result is that expected from the previous analysis.
In the equilibrium limit the distribution is the repeated row of the equilibrium transition matrix,
namely,

{% katex display %}
\pi_{100} =
\begin{pmatrix}
0.27876106 \\
0.30088496 \\
0.03982301 \\
0.38053097
\end{pmatrix}\ \ \ \ \ (16).
{% endkatex %}

The plot below illustrates the convergence of the distribution from the previous example.
In the plot the components of {% katex %}\pi_t{% endkatex %} from equation {% katex %}(5){% endkatex %}
are plotted for each time step. The convergence to the limiting value occurs rapidly. Within only
{% katex %}20{% endkatex %} steps {% katex %}\pi_t{% endkatex %} has reached limiting distribution.

&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img class=&quot;post-image&quot;  src=&quot;/assets/posts/discrete_state_markov_chain_equilibrium/distribution_relaxation_1.png&quot;&gt;
&lt;/div&gt;

### Equilibrium Transition Matrix

In this section the equilibrium limit of the transition matrix is determined for the example Markov Chain
shown in equation {% katex %}(14){% endkatex %}.
It was previously shown that this limit is given by equation {% katex %}(9){% endkatex %}. To evaluate
this equation the example transition matrix must be diagonalized.
First, the transition matrix eigenvalues and eigenvectors are computed using the `numpy` linear
algebra library.

```shell
In [9]: λ, v = numpy.linalg.eig(p)
In [10]: λ
Out[10]: array([-0.77413013,  0.24223905,  1.        ,  0.83189108])
In [11]: v
Out[11]:
matrix([[-0.70411894,  0.02102317,  0.5       , -0.4978592 ],
        [ 0.63959501,  0.11599428,  0.5       , -0.44431454],
        [-0.30555819, -0.99302222,  0.5       , -0.14281543],
        [ 0.04205879, -0.00319617,  0.5       ,  0.73097508]])
```

It is seen that {% katex %}\lambda\ =\ 1{% endkatex %} is indeed an eigenvalue,
as previously proven and that other eigenvalues have magnitudes less than {% katex %}1{% endkatex %}.
This is in agreement with Perron-Frobenius Theorem. The `numpy` linear algebra library normalizes the
eigenvectors and uses the same order for eigenvalues and eigenvector columns. The eigenvector
corresponding to {% katex %}\lambda\ =\ 1{% endkatex %} is in the third column and has all components equal.
Eigenvectors are only known to an arbitrary scalar, so the vector of {% katex %}1's{% endkatex %} used
in the previous analysis can be obtained by multiplying the third column by {% katex %}2{% endkatex %}.
After obtaining the eigenvalues and eigenvectors equation {% katex %}(9){% endkatex %} is evaluated
{% katex %}100{% endkatex %} after time steps and compared with the equilibrium limit.

```shell
In [12]: Λ = numpy.diag(λ)
In [13]: V = numpy.matrix(v)
In [14]: Vinv = numpy.linalg.inv(V)
In [15]: Λ_t = Λ**100
In [16]: Λ_t
Out[16]:
array([[7.61022278e-12, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
       [0.00000000e+00, 2.65714622e-62, 0.00000000e+00, 0.00000000e+00],
       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00],
       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.01542303e-08]])
In [17]: V * Λ_t * Vinv
Out[17]:
matrix([[0.27876106, 0.30088496, 0.03982301, 0.38053097],
        [0.27876106, 0.30088496, 0.03982301, 0.38053097],
        [0.27876106, 0.30088496, 0.03982301, 0.38053097],
        [0.27876106, 0.30088495, 0.03982301, 0.38053098]])
```

First, the diagonal matrix of eigenvalues, {% katex %}\Lambda{% endkatex %}, is created maintaining
the order of {% katex %}\lambda{% endkatex %}. Next, the matrix {% katex %}V{% endkatex %} is constructed
with eigenvectors as columns while also maintaining the order of vectors in {% katex %}v{% endkatex %}.
The inverse of {% katex %}V{% endkatex %} is then computed. {% katex %}\Lambda^{t}{% endkatex %} can now be
computed for {% katex %}100{% endkatex %} time steps. The result is in agreement with the past effort where
the limit {% katex %}t\to\infty{% endkatex %} was evaluated giving a matrix that contained a
{% katex %}1{% endkatex %} in the {% katex %}(1,1){% endkatex %} component corresponding to the position
of the {% katex %}\lambda=1{% endkatex %} component and zeros for all others.
Here the eigenvectors are ordered differently but the only nonzero component has the
{% katex %}\lambda=1{% endkatex %} eigenvalue. Finally, equation {% katex %}(9){% endkatex %} is evaluated and
all rows are identical and equal to {% katex %}\pi_t{% endkatex %} evaluated at
{% katex %}t=100{% endkatex %}, in agreement with the equilibrium limit determined previously and calculations
performed in the last section shown in equations {% katex %}(15){% endkatex %} and {% katex %}(16){% endkatex %}.

### Equilibrium Distribution

The equilibrium distribution will now be calculated using the system of linear
equations defined by equation {% katex %}(9){% endkatex %}. Below the resulting system of equations
for the example distribution in equation {% katex %}(14){% endkatex %} is shown.

{% katex display %}
\pi_{E}^{T}
\begin{pmatrix}
-1.0 &amp; 0.9 &amp; 0.1 &amp; 0.0 &amp; 1.0 \\
0.8 &amp; -0.9 &amp; 0.0 &amp; 0.1 &amp; 1.0 \\
0.0 &amp; 0.5 &amp; -0.7 &amp; 0.2 &amp; 1.0 \\
0.1 &amp; 0.0 &amp; 0.0 &amp; -0.1 &amp; 1.0
\end{pmatrix}
=
\begin{pmatrix}
0.0 &amp; 0.0 &amp; 0.0 &amp; 0.0 &amp; 1.0
\end{pmatrix}
{% endkatex %}

This system of equations is solved using the least squares method provided by the `numpy` linear
algebra library, which requires the use of the transpose of the above equation.
The first line below computes it using equation {% katex %}(14){% endkatex %}.

```shell
In [18]: E = numpy.concatenate((p.T - numpy.eye(4), [numpy.ones(4)]))
In [19]: E
Out[19]:
matrix([[-1. ,  0.8,  0. ,  0.1],
        [ 0.9, -0.9,  0.5,  0. ],
        [ 0.1,  0. , -0.7,  0. ],
        [ 0. ,  0.1,  0.2, -0.1],
        [ 1. ,  1. ,  1. ,  1. ]])
In [20]: πe, _, _, _ = numpy.linalg.lstsq(E, numpy.array([0.0, 0.0, 0.0, 0.0, 1.0]), rcond=None)

In [21]: πe
Out[21]: array([0.27876106, 0.30088496, 0.03982301, 0.38053097])

```

Next, the equilibrium distribution is evaluated using the least squares method. The result obtained is
consistent with previous results shown in equation {% katex %}(16){% endkatex %}.

### Simulation

This section will use a direct simulation of equation {% katex %}(14){% endkatex %} to calculate the
equilibrium distribution and compare the result to those previously obtained. Below a Python
implementation of the calculation is shown.

```python
import numpy

def sample_chain(t, x0, nsample):
    xt = numpy.zeros(nsample, dtype=int)
    xt[0] = x0
    up = numpy.random.rand(nsample)
    cdf = [numpy.cumsum(t[i]) for i in range(4)]
    for t in range(nsample - 1):
        xt[t] = numpy.flatnonzero(cdf[xt[t-1]] &gt;= up[t])[0]
    return xt

# Simulation parameters
π_nsamples = 1000
nsamples = 10000
c = [[0.25],
     [0.25],
     [0.25],
     [0.25]]

# Generate π_nsamples initial state samples
π = numpy.matrix(c)
π_cdf = numpy.cumsum(c)
π_samples = [numpy.flatnonzero(π_cdf &gt;= u)[0] for u in numpy.random.rand(π_nsamples)]

# Run sample_chain for nsamples for each of the initial state samples
chain_samples = numpy.array([])
for x0 in π_samples:
  chain_samples = numpy.append(chain_samples, sample_chain(t, x0, nsamples))
```

The function `sample_chain` performs the simulation and uses
[Inverse CDF Sampling]({{ site.baseurl }}{% link _posts/2018-07-21-inverse_cdf_sampling.md %}) on the
discrete distribution obtained from the transition matrix defined by equation {% katex %}(14){% endkatex %}.
The transition matrix determines state at step {% katex %}t+1{% endkatex %} from the state at step
{% katex %}t{% endkatex %}. The following code uses `sample_chain` to generate and ensemble of
simulations with the initial state also sampled from an assumed initial distribution.
First, simulation parameters are defined and the initial distribution is assumed to be uniform.
Second, `π_nsamples` of the initial state are generated using Inverse CDF sampling with the
initial distribution. Finally, simulations of length `nsamples` are performed for each initial state.
The ensemble of samples are collected in the variable `chain_samples` and plotted below. A comparison
is made with the two other calculations performed. The first is {% katex %}\pi_t{% endkatex %} for
{% katex %}t=100{% endkatex %} shown in equation {% katex %}(16){% endkatex %} and the second
the solution to equation {% katex %}(9){% endkatex %}. The different
calculations are indistinguishable.

&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img  class=&quot;post-image&quot; src=&quot;/assets/posts/discrete_state_markov_chain_equilibrium/distribution_comparison.png&quot;&gt;
&lt;/div&gt;

## Conclusions

Markov Chain equilibrium for discrete state processes is a general theory of the time evolution
of transition probabilities and state distributions. It has been shown that equilibrium
is a consequence of assuming the transition matrix and distribution vector are both stochastic.
Expressions were derived for the time evolution of any transition matrix and distribution and
the equilibrium solutions were then obtained analytically by evaluating the limit
{% katex %}t\to\infty{% endkatex %}. A calculation was performed using the obtained
equations for the equilibrium solutions and the `numpy` linear algebra libraries
using an example transition matrix. These results were compared to ensemble simulations.
The time to relax from an arbitrary state to the equilibrium distributions was shown to occur
within {% katex %}O(10){% endkatex %} time steps.</content><author><name>Troy Stribling</name></author><summary type="html">A Markov Chain is a sequence of states where transitions between states occur ordered in time with the probability of transition depending only on the previous state. Here the states will be assumed a discrete finite set and time a discrete unbounded set. If the set of states is given by {x1, x2,…, xN}\{x_1,\ x_2,\ldots,\ x_N\}{x1​, x2​,…, xN​} the probability that the process will be in state xix_ixi​ at time ttt is denoted by P(Xt=xi)P(X_t=x_i)P(Xt​=xi​), referred to as the distribution. Markov Chain equilibrium is defined by limt→∞P(Xt=xi) &amp;lt; ∞\lim_{t\to\infty}P(X_t=x_i)\ &amp;lt;\ \inftylimt→∞​P(Xt​=xi​) &amp;lt; ∞, that is, as time advances P(Xt=xi)P(X_t=x_i)P(Xt​=xi​) becomes independent of time. Here a solution for this limit is discussed and illustrated with examples.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/posts/discrete_state_markov_chain_equilibrium/distribution_comparison.png" /></entry><entry><title type="html">Rejection Sampling</title><link href="http://localhost:4000/2018/07/29/rejection_sampling.html" rel="alternate" type="text/html" title="Rejection Sampling" /><published>2018-07-29T00:00:00-07:00</published><updated>2018-07-29T00:00:00-07:00</updated><id>http://localhost:4000/2018/07/29/rejection_sampling</id><content type="html" xml:base="http://localhost:4000/2018/07/29/rejection_sampling.html">[Rejection Sampling](https://en.wikipedia.org/wiki/Rejection_sampling) is a method for obtaining samples for a known target probability distribution using samples from some other proposal distribution.
It is a more general method than
[Inverse CDF Sampling]({{ site.baseurl }}{% link _posts/2018-07-21-inverse_cdf_sampling.md %}) which requires
distribution to have an invertible CDF. Inverse CDF Sampling transforms a
{% katex %}\textbf{Uniform}(0,\ 1){% endkatex %} random variable into a random variable with
a desired target distribution using the inverted CDF of the target distribution. While, Rejection Sampling
is a method for transformation of random variables from arbitrary proposal distributions into a desired target
distribution.

&lt;!--more--&gt;

The implementation of Rejection Sampling requires the consideration of a target distribution,
{% katex %}f_X(X){% endkatex %}, a proposal distribution, {% katex %}f_Y(Y){% endkatex %}, and a {% katex %}\textbf{Uniform}(0,\ 1){% endkatex %} acceptance probability, {% katex %}U{% endkatex %}, with distribution {% katex %}f_U(U)=1{% endkatex %}. A proposal sample, {% katex %}Y{% endkatex %}, is generated using, {% katex %}f_Y(Y){% endkatex %}, and independently a uniform acceptance sample, {% katex %}U{% endkatex %}, is generated
using {% katex %}f_U(U){% endkatex %}.
A criterion is defined for acceptance of a sample, {% katex %}X{% endkatex %}, to be considered a
sample of {% katex %}f_X(X){% endkatex %},

{% katex display %}
U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)}\ \ \ \ \ (1),
{% endkatex %}

where, {% katex %}c{% endkatex %}, is chosen to satisfy
{% katex %}0\ \leq\ f_X(Y)/cf_Y(Y)\ \leq\ 1, \ \ \forall\ Y{% endkatex %}. If equation
{% katex %}(1){% endkatex %} is satisfied the proposed sample {% katex %}Y{% endkatex %} is
accepted as a sample of {% katex %}f_X(X){% endkatex %} where {% katex %}X=Y{% endkatex %}.
If equation {% katex %}(1){% endkatex %} is not satisfied {% katex %}Y{% endkatex %} is discarded.

The acceptance function is defined by,

{% katex display %}
h(Y) = \frac{f_X(Y)}{f_Y(Y)}\ \ \ \ \ (2).
{% endkatex %}

It can be insightful to compare {% katex %}h(y){% endkatex %} to {% katex %}f_X(y){% endkatex %}
when choosing a proposal distribution. If {% katex %}h(y){% endkatex %} does not share sufficient mass with
{% katex %}f_X(Y){% endkatex %} then the choice of {% katex %}f_Y(Y){% endkatex %} should be reconsidered.

The Rejection Sampling algorithm can be summarized in the following steps that are repeated for the generation of each sample,

1. Generate a sample {% katex %}Y \sim f_Y(Y){% endkatex %}.
2. Generate a sample {% katex %}U\sim\textbf{Uniform}(0,\ 1){% endkatex %} independent of {% katex %}Y{% endkatex %}.
3. If equation {% katex %}(1){% endkatex %} is satisfied then {% katex %}X=Y{% endkatex %} is accepted as a sample
of {% katex %}f_X(X){% endkatex %}. If equation {% katex %}(1){% endkatex %} is not satisfied then {% katex %}Y{% endkatex %}
is discarded.

## Theory

To prove that Rejection Sampling works it must be shown that,
{% katex display %}
P\left[Y\ \leq\ y\ |\ U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)}\right]=F_X(y)\ \ \ \ \ (3),
{% endkatex %}

where {% katex %}F_X(y){% endkatex %} is the CDF for {% katex %}f_X(y){% endkatex %},
{% katex display %}
F_X(y) = \int_{-\infty}^{y} f_X(w)dw.
{% endkatex %}

To prove equation {% katex %}(2){% endkatex %} a couple of intermediate steps are required. First,
The joint distribution of {% katex %}U{% endkatex %} and {% katex %}Y{% endkatex %} containing the
acceptance constraint will be shown to be,

{% katex display %}
P\left[U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)},\ Y\ \leq\ y\right] = \frac{F_X(y)}{c}\ \ \ \ \ (4).
{% endkatex %}

Since the Rejection Sampling algorithm as described in the previous section assumes
that {% katex %}Y{% endkatex %} and {% katex %}U{% endkatex %} are independent random variables,
{% katex%}
f_{YU}(Y, U)\ =\ f_Y(Y)f_U(U)\ = f_Y(Y).
{% endkatex %} It follows that,

{% katex display %}
\begin{aligned}
P\left[U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)},\ Y\ \leq y\right] &amp;= \int_{-\infty}^{y}\int_{0}^{f_X(w)/cf_Y(w)} f_{YU}(w, u) du dw\\
&amp;= \int_{-\infty}^{y}\int_{0}^{f_X(w)/cf_Y(w)} f_Y(w) du dw \\
&amp;= \int_{-\infty}^{y} f_Y(w) \int_{0}^{f_X(w)/cf_Y(w)} du dw \\
&amp;= \int_{-\infty}^{y} f_Y(w) \frac{f_X(w)}{cf_Y(w)} dw \\
&amp;= \frac{1}{c}\int_{-\infty}^y f_X(w) dw \\
&amp;= \frac{F_X(y)}{c}, \\
\end{aligned}
{% endkatex %}

Next, it will be shown that the probability of accepting a sample is given by,
{% katex display %}
P\left[U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)}\right] = \frac{1}{c}\ \ \ \ \ (5).
{% endkatex %}

This result follows from equation {% katex %}(4){% endkatex %} by taking the
limit {% katex %}y\to\infty{% endkatex %}
and noting that, {% katex %}\int_{-\infty}^{\infty} f_X(y) dy = 1{% endkatex %}.

{% katex display %}
\begin{aligned}
P\left[U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)}\right] &amp;= \int_{-\infty}^{\infty}\int_{0}^{f_X(w)/cf_Y(w)} f_{YU}(w, u) du dw\\
&amp;= \frac{1}{c}\int_{-\infty}^{\infty} f_X(w) dw \\
&amp;= \frac{1}{c}
\end{aligned}
{% endkatex %}

Finally, equation {% katex %}(3){% endkatex %} can be proven, using the definition of [Conditional Probability](https://en.wikipedia.org/wiki/Conditional_probability),
equation {% katex %}(4){% endkatex %} and equation {% katex %}(5){% endkatex %},

{% katex display %}
\begin{aligned}
P\left[Y\ \leq\ y\ |\ U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)}\right] &amp;= \frac{P\left[U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)},\ Y \leq y\right]}{P\left[U\ \leq\ \frac{f_X(Y)}{cf_Y(Y)}\right]}\\
&amp;=\frac{F_X(y)}{c}\frac{1}{1/c}\\
&amp;=F_X(y)
\end{aligned}
{% endkatex %}

## Implementation

An implementation in Python of the Rejection Sampling algorithm is listed below,

```python
def rejection_sample(h, y_samples, c):
    nsamples = len(y_samples)
    u = numpy.random.rand(nsamples)
    accepted_mask = (u &lt;= h(y_samples) / c)
    return y_samples[accepted_mask]
```

The above function `rejection_sample(h, y_samples, c)` takes three arguments as input which are
described in the table below.

| Argument  | Description  |
| :-----: | :---- |
| `h` | The acceptance function. Defined by {% katex %}h(Y)=f_X(Y)/f_Y(Y){% endkatex %}.|
| `y_samples` | Array of samples of {% katex %}Y{% endkatex %} generated using {% katex %}f_Y(Y){% endkatex %}.|
|`c` | A constant chosen so {% katex %}0\ \leq\ h(Y)/c\ \leq\ 1, \ \ \forall\ Y{% endkatex %} |

The execution of `rejection_sample(h, y_samples, c)` begins by generating an appropriate number of acceptance variable samples, {% katex %}U{% endkatex %}, and
then determines which satisfy the acceptance criterion specified by equation (1). The accepted samples are then returned.

## Examples

Consider the [{% katex %}\textbf{Weibull}{% endkatex %} Distribution](https://en.wikipedia.org/wiki/Weibull_distribution). The PDF is
given by,

{% katex display %}
f_X(x; k, \lambda) =
\begin{cases}
\frac{k}{\lambda}\left(\frac{x}{\lambda} \right)^{k-1} e^{-\left(x/\lambda\right)^k} &amp; x\ \geq\ 0 \\
0 &amp; x &lt; 0
\end{cases} \ \ \ \ \ (4),
{% endkatex %}

where {% katex %}k\ &gt;\ 0{% endkatex %} is the shape parameter and {%katex %}\lambda\ &gt;\ 0{% endkatex %} the scale parameter.
The CDF is given by,

{% katex display %}
F_X(x; k, \lambda) =
\begin{cases}
1-e^{\left(\frac{-x}{\lambda}\right)^k
} &amp; x \geq 0 \\
0 &amp; x &lt; 0.
\end{cases}
{% endkatex %}

The first and second moments are,

{% katex display %}
\begin{aligned}
\mu &amp; = \lambda\Gamma\left(1+\frac{1}{k}\right) \\
\sigma^2 &amp; = \lambda^2\left[\Gamma\left(1+\frac{2}{k}\right)-\left(\Gamma\left(1+\frac{1}{k}\right)\right)^2\right]
\end{aligned} \ \ \ \ \ (5),
{% endkatex %}

where {% katex %}\Gamma(x){% endkatex %} is the [Gamma function](https://en.wikipedia.org/wiki/Gamma_function).
In the examples described here it will be assumed that {% katex %}k=5.0{% endkatex %} and
{% katex %}\lambda=1.0{% endkatex %}. The plot below shows the PDF and CDF using these values.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_pdf.png&quot;&gt;

The following sections will compare the performance of generating
{% katex %}\textbf{Weibull}{% endkatex %} samples using both
{% katex %}\textbf{Uniform}(0,\ m){% endkatex %} and {% katex %}\textbf{Normal}(\mu,\ \sigma){% endkatex %}
proposal distributions.

### Uniform Proposal Distribution

Here a {% katex %}\textbf{Uniform}(0,\ m){% endkatex %} proposal distribution will be used to
generate samples for the {% katex %}\textbf{Weibull}{% endkatex %} distribution
{% katex %}(4){% endkatex %}. It provides a simple and
illustrative example of the algorithm. The following plot shows the
target distribution {% katex %}f_X(y){% endkatex %}, the proposal distribution
{% katex %}f_Y(y){% endkatex %} and the acceptance function
{% katex %}h(y)=f_X(y)/f_Y(y){% endkatex %} used in this example. The
uniform proposal distribution requires that a bound be placed
on the proposal samples, which will be assumed to be {% katex %}m=1.6{% endkatex %}. Since
the proposal distribution is constant the acceptance function, {% katex %}h(y){% endkatex %}, will be
a constant multiple of the target distribution. This is illustrated in the plot below.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_uniform_sampled_functions.png&quot;&gt;

The Python code used to generate the samples using  `rejection_sample(h, y_samples, c)` is listed
below.

```python
weibull_pdf = lambda v: (k/λ)*(v/λ)**(k-1)*numpy.exp(-(v/λ)**k)

k = 5.0
λ = 1.0

xmax = 1.6
ymax = 2.0
nsamples = 100000

y_samples = numpy.random.rand(nsamples) * xmax
samples = rejection_sample(weibull_pdf, y_samples, ymax)
```

The following plot compares the histogram computed form the generated samples with the target
distribution {% katex %}(4){% endkatex %}. The fit is acceptable.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_uniform_sampled_distribution.png&quot;&gt;

The next two plots illustrate convergence of the sample mean, {% katex %}\mu{% endkatex %},
and standard deviation, {% katex %}\sigma{% endkatex %}, by comparing the cumulative sums
computed from the samples to target distribution values computed
from equation {% katex %}(5){% endkatex %}. The convergence of the sampled
{% katex %}\mu{% endkatex %} is quite rapid. Within only {% katex %}10^2{% endkatex %}
samples {% katex %}\mu{% endkatex %} computed form the samples is very close the the target value
and by {% katex %}10^4{% endkatex %} samples the two values are indistinguishable.
The convergence of the sampled {% katex %}\sigma{% endkatex %} to the target value is not as
rapid as the convergence of the sampled {% katex %}\mu{% endkatex %} but is still quick. By
{% katex %}10^4{% endkatex %} samples the two values are near indistinguishable.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_uniform_mean_convergence.png&quot;&gt;

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_uniform_sigma_convergence.png&quot;&gt;

Even though {% katex %}10^5{% endkatex %} proposal samples were generated not all were accepted. The
plot below provides insight into the efficiency of the algorithm. In the plot the generated pairs of
acceptance probability {% katex %}U{% endkatex %} and sample proposal {% katex %}Y{% endkatex %} are
plotted with {% katex %}U{% endkatex %} on the vertical axis and {% katex %}Y{% endkatex %} on
the horizontal axis. Also, shown is the scaled acceptance function {% katex %}h(Y)/c{% endkatex %}.
If {% katex %}U\ &gt;\ h(Y)/c{% endkatex %} the sample is rejected and colored orange in the plot and
if {% katex %}U\ \leq\ h(Y)/c{% endkatex %} the sample is accepted, {% katex %}X=Y{% endkatex %} and colored blue.
Only {% katex %}31\%{% endkatex %} of the generated samples were accepted.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_uniform_efficiency.png&quot;&gt;

To improve the acceptance percentage of proposed samples a different proposal distribution must be
considered. In the plot above it is seen that the {% katex %}\textbf{Uniform}(0,\ 1.6){% endkatex %}
proposal distribution uniformly samples the space enclosed by the rectangle it defines without consideration for the shape of
the target distribution. The acceptance percentage will be determined by the ratio of the target
distribution area enclosed by the proposal distribution and the proposal distribution area.
As the target distribution becomes sharp the acceptance percentage will decrease. A proposal distribution that samples the area under
{% katex %}h(Y)/c{% endkatex %} efficiently will have a higher acceptance percentage. It should be kept in mind that rejection of proposal samples is required for the algorithm to work.
If no proposal samples are rejected the proposal and target distributions will be equivalent.

### Normal Proposal Distribution

In this section a sampler using a {% katex %}\textbf{Normal}(\mu,\ \sigma){% endkatex %} proposal
distribution and target {% katex %}\textbf{Weibull}{% endkatex %} distribution is discussed.
A {% katex %}\textbf{Normal}{% endkatex %} proposal distribution has advantages over
the {% katex %}\textbf{Uniform}(0,\ m){% endkatex %} distribution discussed in the previous
section. First, it can provide unbounded samples, while a uniform proposal requires specifying bounds
on the samples. Second, it is a closer approximation to the target distribution so it should provide samples
that are accepted with greater frequency. A disadvantage of the {% katex %}\textbf{Normal}{% endkatex %}
proposal distribution is that it requires specification of {% katex %}\mu{% endkatex %} and
{% katex %}\sigma{% endkatex %}.
If these parameters are the slightest off the performance of the
sampler will be severely degraded. To learn this lesson the first attempt will assume values for
both the parameters that closely match the target distribution. The following plot compares the {% katex %}f_X(y){% endkatex %}, the proposal distribution {% katex %}f_Y(y){% endkatex %} and the acceptance function
{% katex %}h(y){% endkatex %}. There is a large peak in {% katex %}h(y){% endkatex %} to right caused
by the more rapid increase of the {% katex %}\textbf{Weibull}{% endkatex %} distribution relative to the
{% katex %}\textbf{Normal}{% endkatex %} distribution with the result
that most of its mass is not aligned with the target distribution.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_normal_1_sampled_functions.png&quot;&gt;

The Python code used to generate the samples using  `rejection_sample(h, y_samples, c)` is listed
below.

```python
weibull_pdf = lambda v: (k/λ)*(v/λ)**(k-1)*numpy.exp(-(v/λ)**k)

def normal(μ, σ):
    def f(x):
        ε = (x - μ)**2/(2.0*σ**2)
        return numpy.exp(-ε)/numpy.sqrt(2.0*numpy.pi*σ**2)
    return f

k = 5.0
λ = 1.0

σ = 0.2
μ = 0.95

xmax = 1.6
nsamples = 100000

y_samples = numpy.random.normal(μ, σ, nsamples)
ymax = h(x_values).max()
h = lambda x: weibull_pdf(x) / normal(μ, σ)(x)

samples = rejection_sample(h, y_samples, ymax)
```

The first of the following plots compares the histogram computed from the generated samples with the target
distribution {% katex %}(4){% endkatex %} and the second illustrates which proposal samples were accepted.
The histogram fit is good but only {% katex %}23\%{% endkatex %} of the samples were accepted which is worse
than the result obtained with the uniform proposal previously discussed.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_normal_1_sampled_distribution.png&quot;&gt;

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_normal_1_efficiency.png&quot;&gt;

In an attempt to improve the acceptance rate {% katex %}\mu{% endkatex %} of the proposal distribution
is decreased slightly and {% katex %}\sigma{% endkatex %} is increased.
The result is shown in the next plot.
The proposal distribution now covers the target distribution tails. The acceptance function,
{% katex %}h(Y){% endkatex %}, now has its peak inside the target distribution with significant
overlap of mass.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_normal_3_sampled_functions.png&quot;&gt;

The result is much improved. In the plot below it is seen that the percentage of accepted samples has
increased to {% katex %}79\%{% endkatex %}.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_normal_3_efficiency.png&quot;&gt;

The first of the plots below compares the histogram computed from generated samples with the target
distribution {% katex %}(4){% endkatex %} and next two compare the cumulative values of {% katex %}\mu{% endkatex %} and {% katex %}\sigma{% endkatex %} computed from the generated samples with the target distribution values from equation {% katex %}(5){% endkatex %}. The histogram is the best fit of the
examples discussed here and convergence of the sampled {% katex %}\mu{% endkatex %} and
{% katex %}\sigma{% endkatex %} occurs in about {% katex %}10^3{% endkatex %} samples.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_normal_3_sampled_distribution.png&quot;&gt;

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_normal_3_mean_convergence.png&quot;&gt;

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/rejection_sampling/weibull_normal_3_sigma_convergence.png&quot;&gt;

Sampling the {% katex %}\textbf{Weibull}{% endkatex %} distribution with a
{% katex %}\textbf{Normal}{% endkatex %} proposal distribution can produce a better result than a
uniform distribution but care must be exercised in selecting the
{% katex %}\textbf{Normal}{% endkatex %} distribution parameters.
Some choices can produce inferior results. Analysis of the the acceptance function
{% katex %}(2){% endkatex %} can provide guidance in parameter selection.

## Conclusions

Rejection Sampling provides a general method for generation of samples for a known
target distribution by rejecting or accepting samples from a known proposal distribution sampler.
It was analytically proven that if proposal samples
are accepted with a probability defined by equation {% katex %}(1){% endkatex %} the accepted samples have
the desired target distribution. An algorithm implementation was discussed and used in examples where
its performance in producing samples with a desired target distribution for several different proposal
distributions was investigated. Mean and standard deviations
computed from generated samples converged to the target distribution values in
{% katex %}O(10^3){% endkatex %} samples.for both the discrete and continuous cases. It was shown that the performance of the algorithm can vary significantly with chosen parameter values for the proposal distribution.
A criteria for evaluating the expected performance of a proposal distribution using the acceptance function,
defined by equation {% katex %}(2){% endkatex %}, was suggested. Performance was shown to improve if the
acceptance function has significant overlap with the proposal distribution.</content><author><name>Troy Stribling</name></author><summary type="html">Rejection Sampling is a method for obtaining samples for a known target probability distribution using samples from some other proposal distribution. It is a more general method than Inverse CDF Sampling which requires distribution to have an invertible CDF. Inverse CDF Sampling transforms a Uniform(0, 1)\textbf{Uniform}(0,\ 1)Uniform(0, 1) random variable into a random variable with a desired target distribution using the inverted CDF of the target distribution. While, Rejection Sampling is a method for transformation of random variables from arbitrary proposal distributions into a desired target distribution.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/posts/rejection_sampling/weibull_normal_3_efficiency.png" /></entry><entry><title type="html">Inverse CDF Sampling</title><link href="http://localhost:4000/2018/07/21/inverse_cdf_sampling.html" rel="alternate" type="text/html" title="Inverse CDF Sampling" /><published>2018-07-21T00:00:00-07:00</published><updated>2018-07-21T00:00:00-07:00</updated><id>http://localhost:4000/2018/07/21/inverse_cdf_sampling</id><content type="html" xml:base="http://localhost:4000/2018/07/21/inverse_cdf_sampling.html">[Inverse CDF](https://en.wikipedia.org/wiki/Inverse_transform_sampling) sampling is a method
for obtaining samples from both discrete and continuous probability distributions that requires the
CDF to be invertible.
The method assumes values of the CDF are Uniform random variables on {% katex %}[0, 1]{% endkatex %}.
CDF values are generated and used as input into the inverted CDF to obtain samples with the
distribution defined by the CDF.
&lt;!--more--&gt;

## Discrete Distributions

A discrete probability distribution consisting of a finite set of {% katex %}N{% endkatex %}
probability values is defined by,

{% katex display %}
\{p_1,\ p_2,\ \ldots,\ p_N\}\ \ \ \ \ (1),
{% endkatex %}

with {% katex %}p_i\ \geq\ 0, \  \forall i{% endkatex %} and {% katex %}\sum_{i=1}^N{p_i} = 1{% endkatex %}.
The CDF specifies the probability that {% katex %}i \leq n{% endkatex %} and is given by,

{% katex display %}
P(i\ \leq\ n)=P(n)=\sum_{i=1}^n{p_i},
{% endkatex %}

For a given CDF value, {% katex %}U{% endkatex %}, The equation for {% katex %}P(n){% endkatex %} can always
be inverted by evaluating it for each {% katex %}n{% endkatex %} and
searching for the smallest value of {% katex %}n{% endkatex %} that satisfies,
{% katex %}P(n)\ \geq\ U{% endkatex %}.
It follows that generated samples determined from {% katex %}U \sim \textbf{Uniform}(0, 1){% endkatex %}
will have distribution {% katex %}(1){% endkatex %} since the intervals {% katex %}P(n)-P(n-1) = p_n{% endkatex %}
are uniformly sampled.

Consider the distribution,

{% katex display %}
\left\{\frac{1}{12},\ \frac{1}{12},\ \frac{1}{6},\ \frac{1}{6},\ \frac{1}{12},\ \frac{5}{12} \right\} \ \ \ \ \ (2)
{% endkatex %}
It is shown in the following plot with its CDF. Note that the CDF is a monotonically increasing function.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/inverse_cdf_sampling/discrete_cdf.png&quot;&gt;

A sampler using the Inverse CDF method with target distribution specified in {% katex %}(2){% endkatex %} implemented in
Python is shown below.

```python
import numpy

n = 10000
df = numpy.array([1/12, 1/12, 1/6, 1/6, 1/12, 5/12])
cdf = numpy.cumsum(df)

samples = [numpy.flatnonzero(cdf &gt;= u)[0] for u in numpy.random.rand(n)]
```
The program first stores the CDF computed from the partial sums
{% katex %}P(n){% endkatex %} in an array. Next, CDF samples using
{% katex %}U \sim \textbf{Uniform}(0, 1){% endkatex %} are generated. Finally, for each sampled CDF value,
{% katex %}U{% endkatex %}, the array containing {% katex %}P(n){% endkatex %} is scanned for
the smallest value of {% katex %}n{% endkatex %} where {% katex %}P(n)\ \geq\ U{% endkatex %}. The resulting
values of {% katex %}n{% endkatex %} will have the target distribution {% katex %}(2){% endkatex %}. This is
shown in the figure below.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/inverse_cdf_sampling/discrete_sampled_distribution.png&quot;&gt;

A measure of convergence of the samples to the target distribution can be obtained by comparing the cumulative
moments of the distribution computed from the samples with the target value of the moment computed analytically.
In general for a discrete distribution the first and second moments are given by,

{% katex display %}
\begin{aligned}
\mu &amp; =  \sum_{i=1}^N ip_i\\
\sigma^2 &amp; = \sum_{i=1}^N i^2p_i - \mu^2,
\end{aligned}
{% endkatex %}

In the following two plots the cumulative values of {% katex %}\mu{% endkatex %} and
{% katex %}\sigma{% endkatex %} computed from the samples generated are compared with the target values
using the equations above.
The first shows the convergence of {% katex %}\mu{% endkatex %} and the second the convergence of
{% katex %}\sigma{% endkatex %}. Within only {% katex %}10^3{% endkatex %} samples both
{% katex %}\mu{% endkatex %} and {% katex %}\sigma{% endkatex %} computed from samples is comparable to the
target value and by {% katex %}10^4{% endkatex %} the values are indistinguishable.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/inverse_cdf_sampling/discrete_sampled_mean_convergence.png&quot;&gt;

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/inverse_cdf_sampling/discrete_sampled_sigma_convergence.png&quot;&gt;

The number of operations required for generating samples using Inverse CDF sampling from a discrete
distribution will scale {% katex %}O(N_{samples}N){% endkatex %} where {% katex %}N_{samples}{% endkatex %}
is the desired number of samples and {% katex %}N{% endkatex %} is the number of terms in the discrete distribution.

It is also possible to directly sample distribution {% katex %}(2){% endkatex %} using the `multinomial` sampler from `numpy`,

```python
import numpy

n = 10000
df = numpy.array([1/12, 1/12, 1/6, 1/6, 1/12, 5/12])
samples = numpy.random.multinomial(n, df, size=1)/n
```

## Continuous Distributions

A continuous probability distribution is defined by the [PDF](https://en.wikipedia.org/wiki/Probability_density_function),
{% katex %}f_X(x){% endkatex %}, where {% katex %}f_X(x) \geq 0,\ \forall x{% endkatex %} and
{% katex %}\int_{-\infty}^{\infty} f_X(x) dx = 1{% endkatex %}. The CDF is a monotonically increasing function
that specifies the probability that {% katex %}X\ \leq\ x{% endkatex %}, namely,

{% katex display %}
P(X \leq x) = F_X(x) = \int_{-\infty}^{x} f_X(w) dw.
{% endkatex %}

### Theory

To prove that Inverse CDF sampling works for continuous distributions it must be shown that,

{% katex display %}
P[F_X^{-1}(U)\ \leq\ x] = F_X(x) \ \ \ \ \ (3),
{% endkatex %}

where {% katex %}F_X^{-1}(x){% endkatex %} is the inverse of {% katex %}F_X(x){% endkatex %}
and {% katex %}U \sim \textbf{Uniform}(0, 1){% endkatex %}.

A more general result needed to complete this proof is obtained using a change of variable on a CDF.
If {% katex %}Y=G(X){% endkatex %} is a monotonically increasing invertible function
of {% katex %}X{% endkatex %} it will be shown that,

{% katex display %}
P(X\ \leq\ x) = P(Y\ \leq\ y) = P[G(X)\ \leq\ G(x)]. \ \ \ \ \ (4)
{% endkatex %}

To prove this note that {% katex %}G(x){% endkatex %} is monotonically increasing so the ordering of values is
preserved for all {% katex %}x{% endkatex %},

{% katex display %}
X\ \leq\ x\ \implies\ G(X)\ \leq\ G(x).
{% endkatex %}

Consequently, the order of the integration limits is maintained by the transformation.
Further, since {% katex %}y=G(x){% endkatex %} is invertible,
{% katex %}x = G^{-1}(y){% endkatex %} and {% katex %}dx = (dG^{-1}/dy) dy{% endkatex %}, so

{% katex display %}
\begin{aligned}
P(X\ \leq\ x) &amp; = \int_{-\infty}^{x} f_X(w) dw \\
&amp; = \int_{-\infty}^{y} f_X(G^{-1}(z)) \frac{dG^{-1}}{dz} dz \\
&amp; = \int_{-\infty}^{y} f_Y(z) dz \\
&amp; = P(Y\ \leq\ y) \\
&amp; = P[G(X)\ \leq\ G(x)],
\end{aligned}
{% endkatex %}

where,

{% katex display %}
f_Y(y) = f_X(G^{-1}(y)) \frac{dG^{-1}}{dy}
{% endkatex %}

For completeness consider the case where {% katex %}Y=G(X){% endkatex %} is a monotonically decreasing invertible function
of {% katex %}X{% endkatex %} then,

{% katex display %}
X\ \leq\ x\ \implies\ G(X)\ \geq\ G(x),
{% endkatex %}

it follows that,

{% katex display %}
\begin{aligned}
P(X\ \leq x) &amp; = \int_{-\infty}^{x} f_X(w) dw \\
&amp; = \int_{y}^{\infty} f_X(G^{-1}(z)) \frac{dG^{-1}}{dz} dz \\
&amp; = \int_{y}^{\infty} f_Y(z) dz \\
&amp; = P(Y\ \geq\ y) \\
&amp; = P[G(X)\ \geq\ G(x)] \\
&amp; = 1 - P[G(X)\ \leq\ G(x)]
\end{aligned}
{% endkatex %}

The desired proof of equation {% katex %}(3){% endkatex %} follows from equation {% katex %}(4){% endkatex %}
by noting that {% katex %}U \sim \textbf{Uniform}(0, 1){% endkatex %} so {% katex %}f_U(u) = 1{% endkatex %},

{% katex display %}
\begin{aligned}
P[F_X^{-1}(U)\ \leq\ x] &amp; = P[F_X(F_X^{-1}(U))\ \leq\ F_X(x)] \\
&amp; = P[U\ \leq\ F_X(x)] \\
&amp; = \int_{0}^{F_X(x)} f_U(w) dw \\
&amp; = \int_{0}^{F_X(x)} dw \\
&amp; = F_X(x).
\end{aligned}
{% endkatex %}

### Example

Consider the [Weibull Distribution](https://en.wikipedia.org/wiki/Weibull_distribution). The PDF is
given by,

{% katex display %}
f_X(x; k, \lambda) =
\begin{cases}
\frac{k}{\lambda}\left(\frac{x}{\lambda} \right)^{k-1} e^{-\left(x/\lambda\right)^k} &amp; x\ \geq\ 0 \\
0 &amp; x &lt; 0
\end{cases} \ \ \ \ \ (5)
{% endkatex %}

where {% katex %}k\ &gt;\ 0{% endkatex %} is the shape parameter and {%katex %}\lambda\ &gt;\ 0{% endkatex %} the scale parameter.
The CDF is given by,

{% katex display %}
F_X(x; k, \lambda) =
\begin{cases}
1-e^{\left(\frac{-x}{\lambda}\right)^k} &amp; x\ \geq\ 0 \\
0 &amp; x\ &lt;\ 0.
\end{cases}
{% endkatex %}

The CDF can be inverted to yield,

{% katex display %}
F_X^{-1}(u; k, \lambda) =
\begin{cases}
\lambda\ln\left(\frac{1}{1-u}\right)^{\frac{1}{k}} &amp; 0\ \leq\ u\ \leq 1 \\
0 &amp; u\ &lt;\ 0 \text{ or } u\ &gt;\ 1.
\end{cases}
{% endkatex %}

In the example described here it will be assumed that {% katex %}k=5.0{% endkatex %} and
{% katex %}\lambda=1.0{% endkatex %}. The following plot shows the PDF and CDF using these values.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/inverse_cdf_sampling/weibull_cdf.png&quot;&gt;

The sampler implementation for the continuous case is simpler than for the discrete case.
Just as in the discrete case CDF samples with distribution {% katex %}U \sim \textbf{Uniform}(0, 1){% endkatex %}
are generated. The desired samples with the target {% katex %}\textbf{Weibull}{% endkatex %}
distribution are then computed using the CDF inverse.
Below an implementation of this procedure in Python is given.

```python
import numpy

k = 5.0
λ = 1.0
nsamples = 100000

cdf_inv = lambda u: λ * (numpy.log(1.0/(1.0 - u)))**(1.0/k)
samples = [cdf_inv(u) for u in numpy.random.rand(nsamples)]
```

The following plot compares a histogram of the samples generated by the sampler above and the target
distribution (5). The fit is quite good. The subtle asymmetry of the
{% katex %}\textbf{Weibull}{% endkatex %} distribution is captured.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/inverse_cdf_sampling/weibull_sampled_distribution.png&quot;&gt;

The first and second moments for the {% katex %}\textbf{Weibull}{% endkatex %} distribution are given by,

{% katex display %}
\begin{aligned}
\mu &amp; = \lambda\Gamma\left(1+\frac{1}{k}\right) \\
\sigma^2 &amp; = \lambda^2\left[\Gamma\left(1+\frac{2}{k}\right)-\left(\Gamma\left(1+\frac{1}{k}\right)\right)^2\right],
\end{aligned}
{% endkatex %}

where {% katex %}\Gamma(x){% endkatex %} is the [Gamma function](https://en.wikipedia.org/wiki/Gamma_function).

The following two plots compare the cumulative values of {% katex %}\mu{% endkatex %} and {% katex %}\sigma{% endkatex %}
computed from sampling of the target distribution {% katex %}(5){% endkatex %} with the values from the equations above.
The first shows the convergence of {% katex %}\mu{% endkatex %} and the second the convergence of {% katex %}\sigma{% endkatex %}.
Within only {% katex %}10^3{% endkatex %} samples both {% katex %}\mu{% endkatex %} and {% katex %}\sigma{% endkatex %} computed from
samples are comparable to the target values and by {% katex %}10^4{% endkatex %} the values are indistinguishable.

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/inverse_cdf_sampling/weibull_sampled_mean_convergence.png&quot;&gt;

&lt;img class=&quot;post-image&quot; src=&quot;/assets/posts/inverse_cdf_sampling/weibull_sampled_std_convergence.png&quot;&gt;

### Performance

Any continuous distribution can be sampled by assuming that {% katex %}f_X(x){% endkatex %} can be approximated
by the discrete distribution, {% katex %}\left\{f_X(x_i)\Delta x_i \right\}_N{% endkatex %} for
{% katex %}i=1,2,3,\ldots,N{% endkatex %}, where {% katex %}\Delta x_i=(x_{max}-x_{min})/(N-1){% endkatex %} and
{% katex %}x_i = x_{min}+(i-1)\Delta x_i{% endkatex %}. This method has disadvantages compared to using
Inverse CDF sampling on the continuous distribution. First, a bounded range for the samples must
be assumed when in general the range of the samples can be unbounded while the Inverse CDF method can sample an unbounded range.
Second, the number of operations required for
sampling a discrete distribution scales {% katex %}O(N_{samples}N){% endkatex %} but sampling the
continuous distribution is {% katex %}O(N_{samples}){% endkatex %}.

## Conclusions

Inverse CDF Sampling provides a method for obtaining samples from a known target distribution using
a sampler with a {% katex %}\textbf{Uniform}(0, 1){% endkatex %} distribution that requires the
target distribution be invertible. Algorithms for both the discrete and continuous cases were
analytically proven to produce samples with distributions defined by the CDF.
Example implementations of the algorithms for both distribution cases were developed.
Samples produced by the algorithms for example target distributions were favorably compared. Mean and standard deviations computed from generated samples converged to the target distribution values
in {% katex %}O(10^3){% endkatex %} samples.
The continuous sampling algorithm was shown to be more performant than the discrete version.
The discrete version required {% katex %}O(N_{samples}N){% endkatex %}
operations while the continuous version required {% katex %}O(N_{samples}){% endkatex %} operations.</content><author><name>Troy Stribling</name></author><summary type="html">Inverse CDF sampling is a method for obtaining samples from both discrete and continuous probability distributions that requires the CDF to be invertible. The method assumes values of the CDF are Uniform random variables on [0,1][0, 1][0,1]. CDF values are generated and used as input into the inverted CDF to obtain samples with the distribution defined by the CDF.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/posts/inverse_cdf_sampling/weibull_sampled_distribution.png" /></entry></feed>